<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- favicon-->
    <link rel="shortcut icon" href="assets/images/favicon.ico">
    <!-- Site Title-->
    <title>Xai - Website</title>
    <meta name="description" content="Science and technology for the eXplanation of AI decision making">
    <!-- Bootstrap CSS file-->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Flickity CSS file-->
    <link href="assets/css/flickity.min.css" rel="stylesheet">
    <!-- Main CSS file-->
    <link href="assets/css/style.css" rel="stylesheet">
    <!-- Fontawesome 5 CSS file-->
    <link href="assets/css/fontawesome-all.min.css" rel="stylesheet">
    <!-- Magnific Popup CSS-->
    <link href="assets/css/magnific-popup.css" rel="stylesheet">
    <!-- Google Fonts-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&amp;display=swap">
    <script type="text/javascript">
      // Matomo Code
      var _paq = window._paq = window._paq || [];
      /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
      _paq.push(['trackPageView']);
      _paq.push(['enableLinkTracking']);
      (function () {
      var u="//piwikdd.isti.cnr.it/";
      _paq.push(['setTrackerUrl', u+'piwik.php']);
      _paq.push(['setSiteId', '12']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
      })
      ();
    </script>
  </head>
  <body>
    <!-- Navbar-->
    <div class="site-header header-bottom-area">
      <nav class="navbar navbar-expand-lg navbar-light sticky">
        <div class="container"><a class="navbar-brand" href="index.html"><img class="site-logo" src="assets/images/logo/xai_logo_color.png" alt="Logo"></a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarNavDropdown">
            <ul class="navbar-nav mx-auto">
              <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
              <li class="nav-item"><a class="nav-link" href="about.html">Project details</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="research-lines.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research Lines</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="line_1.html">1. Local to global</a><a class="dropdown-item" href="line_2.html">2. Casual reasoning</a><a class="dropdown-item" href="line_3.html">3. Platform and XUI</a><a class="dropdown-item" href="line_4.html">4. Case studies</a><a class="dropdown-item" href="line_5.html">5. Ethics and legal</a></div>
              </li>
              <li class="nav-item"><a class="nav-link" href="people.html">People</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="resources.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications and Resources</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="resources.html#publications">Publications</a><a class="dropdown-item" href="reports.html">Reports</a><a class="dropdown-item" href="resources.html#thesis">Thesis</a><a class="dropdown-item" href="dissemination.html">Dissemination tools</a></div>
              </li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">News</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="news.html">All News</a><a class="dropdown-item" href="dist-seminars.html">Distinguished seminars</a><a class="dropdown-item" href="internal-events.html">Internal Events</a></div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
    <!-- End Navbar-->
    <!-- Cookies-->
    <div class="cookie-banner" style="display: none">
      <p class="mx-2">We use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. 
        <u><a href="privacy.html" link’="privacy.html">No, give me more info.</a></u>
      </p>
      <button class="close">&times;</button>
    </div>
    <!-- End Cookies-->
    <article class="entry">
      <div class="entry-content">
        <div class="bg-yellow pt-6 pt-lg-8 pb-4 pb-lg-5">
          <div class="container">
            <div class="row">
              <div class="col-lg-8 offset-lg-2">
                <header class="entry-header text-center">
                  <h2 class="entry-title display-5 font-weight-bold">Local to global</h2>
                </header>
              </div>
            </div>
          </div>
        </div>
        <!-- image-->
        <div class="bg-half">
          <div class="container">
            <div class="row justify-content-lg-center">
              <div class="col-lg-4">
                <div class="bg-black">
                  <div class="py-lg-7 px-lg-6 py-md-6 px-md-5 py-5 px-4">
                    <div class="text-white pr-lg-4 text-center">
                      <h1 class="display-1 mb-1 font-weight-bolder">1</h1>
                      <h5>LINE</h5>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="container mt-6">
          <div class="row justify-content-lg-center mt-6">
            <div class="col-lg-8">
              <!--h4 Abstract-->
              <p class="lead">From the experience of the surveys [ <a href="#GMR2018">GMR2018</a> , <a href="#BGG2021">BGG2021</a> ], we developed and designed various explanation methods with a focus on local rule-based explainers. Also, our goal was to “merge” such local explanations to reach a global consensus on the reasons for the decisions taken by an AI decision support system.</p>
              <h5 class="mt-6">1.1 Local Rule-based Explainer</h5>Our first proposal is the LOcal Rule-based Explainer (LORE) presented in [<a href="#GMG2019">GMG2019</a>]. LORE is a model-agnostic local explanation method that returns as an explanation a “factual rule revealing the reasons for a decision, and a set of counterfactual rules illustrating how to change the classification outcome. The first peculiarity of LORE is that it adopts a synthetic generator based on a genetic algorithm. The second peculiarity of LORE is that it adopts a decision tree as a local surrogate, thus (i) decision rules can naturally be derived from a root-leaf path in a decision tree; and, (ii) counterfactuals can be extracted by symbolic reasoning over the tree.
              <div class="pb-md-2 mt-4"><a class="btn btn-outline-black" href="https://pypi.org/project/LORE-ext/" target="_blank">LINK</a></div>
              <h5 class="mt-6">1.2 Plausible Data-Agnostic Local Explanations</h5>LORE was initially designed to deal with tabular data and binary classification problems. We extended it to work on other data types and for multiclass problems. Also, we are currently working to solve some limitations of LORE related to the stability and actionability of the explanations. In line with LIME [1], our idea was to extend LORE for working on any data type. Indeed, our objective was to define a local model-agnostic and data-agnostic explanation framework to explain the decisions taken by obscure black-box classifiers on specific input instances. Therefore, our proposal would not be tied to a specific type of data or a specific type of classifier. Besides being model-agnostic, LIME is also data-agnostic. However, LIME employs conceptually different neighborhood generation strategies for tabular data, images, and texts. For images, LIME randomly replaces actual super-pixels with super-pixels containing a fixed color. For texts, it randomly removes words. Thus, both for images and text LIME “suppresses” parts of the actual information in the data. On the other hand, for tabular data, LIME assumes uniform distributions for categorical attributes and normal distributions for the continuous ones. Such limitations prevent LIME from basing the local regressor used to extract the explanation on meaningful synthetic instances. Our proposal allows overcoming these limitations by guaranteeing comparable synthetic data generation among all the different data types, ensuring meaningful synthetic instances to learn interpretable local surrogate models.
              
              Our idea was to extend LORE [<a href="#GMG2019">GMG2019</a>] to overcome the limitations of existing approaches by exploiting the latent feature space learned through different types of autoencoders [2] to generate plausible synthetic instances during the neighborhood generation process. Given an instance of any type classified by a black-box, the Latent-LORE (LLORE) allows instantiating a data-specific explainer following the explanation framework structure. The explainer will be able to return a meaningful explanation for the classification reasons. LLORE-based approaches work as follows. First, they generate synthetic instances in the latent feature space using a pre-trained autoencoder (GAM, AAE, VAE, etc.). Then, they learn a latent decision tree classifier. After that, they select and decode the synthetic instances respecting the latent local decision rules observed on the decision tree. Finally, independently from the data type, they return an explanation that always consists of a set of synthetic exemplars and counter-exemplars instances illustrating, respectively, instances classified with the same label and with a different label than the instance to explain, which may be visually analyzed to understand the reasons for the classification. Additionally, a data-specific explanation can be built on the exemplars and counter-exemplars.
              
              We instantiated LLORE for images [<a href="#GMG2019">GMG2019</a>, <a href="#GMM2020">GMM2020</a>], time series [<a href="#GMS2020">GMS2020</a>] and text [<a href="#LGR2020">LGR2020</a>] realizing ad-hoc logic-based explanations. A wide experimentation on datasets of different types and explaining different black-box classifiers empirically demonstrate that LLORE-based explainers overtakes existing explanation methods providing meaningful, stable, useful, and really understandable explanations. In [<a href="#MGY2021">MGY2021</a>, <a href="#MBG2021">MBG2021</a>] we employed ABELE in a case study for skin lesion diagnosis, illustrating how it is possible to provide the practitioner with explanations on the decisions of a Deep Neural Network (DNN). We have proved that after being customized and carefully trained, ABELE can produce meaningful explanations that really help practitioners. The latent space analysis suggests an interesting partitioning of images over the latent space. Still in [<a href="#MGY2021">MGY2021</a>, <a href="#MBG2021">MBG2021</a>] is reported a survey involving real experts in the health domain and common people that supports the hypothesis that explanation methods without a consistent validation are not useful.
              
              As highlighted by these works, the context of synthetic data generation for local explanation methods it is important to generate data samples located within “local” areas surrounding specific instances. The problem with generative adversarial networks and autoencoders is that they require a large quantity of data, and a not negligible training time. In addition, such generative approaches are suited only for particular types of data. In [<a href="#GM2020">GM2020</a>] we overcome these drawbacks proposing DAG, a Data-Agnostic neighborhood Generation approach that, given an input instance and a (small) support set, returns a set of local realistic synthetic instances. DAG applies a data transformation that enables the generation for any type of input data. It is based on a set of generative operators inspired to genetic programming. Such operators work by applying specific vector perturbations by following a fast procedure that only requires a small set of instances to support the data generation.  A wide experimentation on different types of data (tabular data, images, time series, and texts) and against state-of-the-art local neighborhood generators shows the effectiveness of DAG in producing realistic instances independently from the nature of the data.
              
              <h5 class="mt-6">1.3 Local Explanation 4 Health</h5>In order to enable explainable AI systems to support medical decision-making, it is necessary to enable XAI techniques to deal with typical healthcare data characteristics. We incrementally addressed such a problem with the contributions presented in [<a href="#PGM2019">PGM2019</a>] and [<a href="#PPP2020">PPP2020</a>]. [<a href="#PGM2019">PGM2019</a>] presents MARLENA (Multi-lAbel RuLe-based ExplaNAtions), a model-agnostic XAI methodology to address the outcome explanation problem in the context of multi-label black box outcomes. Building on the insights we gained from the experiments carried out in [<a href="#PGM2019">PGM2019</a>], we developed Doctor XAI [<a href="#PPP2020">PPP2020</a>], a model-agnostic technique that is suitable for multi-label black box outcomes and it is also able to deal with ontologically-linked and sequential data. Two key aspects of the presented approach are that it exploited the ontology in creating the synthetic neighborhood and employed a novel encoder/decoder scheme for sequential data that preserves the interpretability of the features. The ontological perturbation allows us to create synthetic instances that consider local features interactions by perturbing the set of neighbors available in the dataset masking semantically similar features. We tested Doctor XAI in two scenarios. First, we tested the ability of Doctor XAI combined with a local-to-global approach to audit a fictional commercial black box. This resulted in a framework for auditing clinical decision support systems called FairLens [<a href="#PPB2021">PPB2021</a>]. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by exploiting DoctorXAI to explain which elements of the affected patients' clinical history drive the model error in the problematic group. We validate FairLens' ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics. Finally in [<a href="#PBF2022">PBF2022</a>], we presented the collective effort of our interdisciplinary team of data scientists, human-computer interaction experts and designers to develop a human-centered, explainable AI system for clinical decision support. Using an iterative design approach that involves healthcare providers as end-users, we present the first cycle of the prototyping-testing-redesigning of DoctorXAI and its explanation-user interface. We first present the DoctorXAI concept that stems from patients data and healthcare application requirements. Then we develop the initial prototype of the explanation user interface, and perform a user study to test its perceived trustworthiness and collect healthcare providers' feedback. We finally exploit the users' feedback to co-design a more human-centered XAI user interface taking into account design principles such as progressive disclosure of information.
              
              
              
              
              <h5 class="mt-6">1.4 Local to Global Approaches</h5>Local explanations enjoy several properties: they are relatively fast and easy to extract, precise, and possibly diverse. Conversely, global explanations are more cumbersome to extract, and, having a larger scope, more general. Thus, these two families present complementary properties. The Local to Global explanation paradigm [<a href="#PGG2018">PGG2018</a>, <a href="#PGG2019">PGG2019</a>] is a natural extension of the Local and Global paradigms, and aims to exploit the fidelity and ease of extraction of Local explanations to generate faithful, general, and simple Global explanations. In our work, we have focused on explanations in the form of axis-parallel decision rules, and have proposed two algorithms to tackle them, namely Rule Relevance Score (RRS) [<a href="#SGM2019">SGM2019</a>] and GLocalX [<a href="#SGM2021">SGM2021</a>]. Rule Relevance Score (RRS) [<a href="#SGM2019">SGM2019</a>] is a simple scoring framework in which we try to select, rather than edit, the local explanations. In other words, with RRS we construct global explanations by selecting local ones. RRS uses a multi-faceted scoring formula in which explanations are ranked according to their fidelity, coverage and outlier coverage, which rewards rules explaining seldomly explained records. GLocalX [<a href="#SGM2021">SGM2021</a>] relies on three assumptions: (i) logical explainability, that is, explanations are best provided in a logical form that can be reasoned upon; (ii) local explainability, that is, regardless of the complexity of the decision boundary of the black box, locally it can be accurately approximated by an explanation; (iii) composability, that is, we can compose local explanations by leveraging their logical form. Starting from a set of local explanations in the form of decision rules constituting the structure's leaves, GLocalX iteratively merges explanations in a bottom-up fashion to create a hierarchical merge structure that yields global explanations on its top layer. GLocalX shows a unique balance between fidelity and simplicity, having state-of-the-art fidelity and yielding small sets of compact global explanations. When comparing with natively global models, such as Decision Trees and CPAR, who have direct access to the whole training data, rather than the local explanations, GLocalX compares favorably. It’s only slightly less faithful than the most faithful model (~2% less faithful than a Decision Tree) while having a far simpler model (up to 1 order of magnitude smaller set of output rules). When compared with models of similar complexity, such as a Pruned Decision Tree, GLocalX is slightly more faithful and less complex.
              
              <h5 class="mt-6">1.5 Towards Interpretable-by-design Models</h5>In parallel with the activity of designing local and global post-hoc explainers, in line with [3] we also started to explore directions for designing predictive models which are interpretable-by-design, i.e., they return the prediction and allow us to understand the reasons that lead to that prediction. Indeed, if the machine logic is transparent and accessible, as humans, we tend to trust more a decision process using a logic similar to that of a human being, rather than a reasoning that we can understand but that is outside the human way of thinking [4].
              
              In [<a href="#GD2021">GD2021</a>] we present MAPIC a MAtrix Profile-based Interpretable time series Classifier. MAPIC is an interpretable model for time series classification able to guarantee high levels of accuracy and efficiency while maintaining the classification, and the classification model, interpretable. In the design of MAPIC we followed the line of research based on shapelets. However, we replaced the inefficient approaches adopted in the state of the art for the search of the most discriminative subsequences with the patterns that is possible to extract from a model named Matrix Profile [5]. In short, the Matrix Profile (MP) represents the distances between all subsequences and their nearest neighbors. From a MP it is possible to efficiently extract some patterns characterizing a time series such as motifs and discords. Motifs are subsequences of a time series which are very similar to each other, while discords are subsequences of a time series which are very different from any other subsequence. As a classification model, MAPIC adopts a decision tree classifier due to its intrinsic interpretability. We empirically demonstrate that MAPIC overtakes existing approaches having a similar interpretability in terms of both accuracy and running time.
              
              For these two last results and from GLocalX it is clear the importance of relying on sound decision tree models. A weak point of traditional decision trees is that they are not very stable and a common procedure to stabilize them is to merge various trees into an unique tree. In a certain sense, this is a form of explanation of a set of decision trees with a single model. Several proposals are present in the literature for traditional decision trees but there is a lack of merging operations for oblique trees and forests of oblique trees. Thus, in [<a href="#BGM2021">BGM2021</a>] we combine XAI and the merging of decision trees. Given any accurate and complex tree-based classifier, our aim is to approximate it with a single interpretable decision tree that guarantees comparable levels of accuracy and a low complexity that permits us to understand the logic it follows for the classification. We propose a Single-tree Approximation MEthod (SAME) that exploits a procedure for merging decision trees, a post-hoc explanation strategy, and a combination of them to turn any tree-based classifier into a single and interpretable decision tree. Given a certain tree-based classifier, the idea of SAME is to reduce any approximation problem with another one for which a solution is known in a sort of “cascade of approximations” with several available alternatives. This allows SAME to turn Random Forests, Oblique Trees and Oblique Forests into a single decision tree. The implementation of SAME required adapting existing procedures for merging traditional decision trees to oblique trees by moving from an intensional approach to an extensional one for efficiency reasons. An experimentation on eight tabular datasets with different size and dimensionality compares SAME against a baseline approach (PHDT)  that directly approximates any classifier with a decision tree. We show that SAME is efficient and that the retrieved single decision tree is at least as accurate as the original non interpretable tree-based model.
              
            </div>
          </div>
        </div>
      </div>
      <div class="entry-content">
        <div class="bg-yellow pt-6 pt-lg-8 pb-4 pb-lg-5 mt-5">
          <div class="container">
            <div class="row"></div>
          </div>
        </div>
        <!-- image-->
        <div class="bg-half">
          <div class="container">
            <div class="row">
              <div class="col-lg-4">
                <div class="bg-black">
                  <div class="py-lg-5 px-lg-6 py-md-5 px-md-5 py-5 px-4">
                    <div class="text-white pr-lg-4 text-center">
                      <h3 class="mb-1 font-weight-bolder" id="publications">Publications</h3>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="container mb-6">
          <div class="row">
            <div class="row mt-5 justify-content-center">
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GMR2018">1.</h4><small>[GMR2018]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-zero"></div><strong>A Survey of Methods for Explaining Black Box Models</strong><br><em>Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco, Giannotti Fosca, Pedreschi Dino</em> (2022) - ACM Computing Surveys. In ACM computing surveys (CSUR), 51(5), 1-42.
                <div class="collapse" id="collapse-zero" aria-labelledby="heading-zero" data-parent="#accordion-zero">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-zero" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1145/3236009" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GMG2019">2.</h4><small>[GMG2019]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-one" type="button">
                <div class="modal fade" id="modal-one" tabindex="-1" role="dialog" aria-labelledby="#modal-one-Title" aria-hidden="true">
                  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Factual and Counterfactual Explanations for Black Box Decision Making</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-one"></div><strong>Factual and Counterfactual Explanations for Black Box Decision Making</strong><br><em>Guidotti Riccardo, Monreale Anna, Giannotti Fosca, Pedreschi Dino, Ruggieri Salvatore, Turini Franco</em> (2021) - IEEE Intelligent Systems. In IEEE Intelligent Systems
                <div class="collapse" id="collapse-one" aria-labelledby="heading-one" data-parent="#accordion-one">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/mis.2019.2957223" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="SGM2021">3.</h4><small>[SGM2021]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/02_glocalX.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-two" type="button">
                <div class="modal fade" id="modal-two" tabindex="-1" role="dialog" aria-labelledby="#modal-two-Title" aria-hidden="true">
                  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">GLocalX - From Local to Global Explanations of Black Box AI Models</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/02_glocalX.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-two"></div><strong>GLocalX - From Local to Global Explanations of Black Box AI Models</strong><br><em>Setzu Mattia, Guidotti Riccardo, Monreale Anna, Turini Franco, Pedreschi Dino, Giannotti Fosca</em> (2021) - Artificial Intelligence. In Artificial Intelligence
                <div class="collapse" id="collapse-two" aria-labelledby="heading-two" data-parent="#accordion-two">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are “black boxes” which we are not able to understand. Relying on these models has a multifaceted impact and raises significant concerns about their transparency. Applications in sensitive and critical domains are a strong motivational factor in trying to understand the behavior of black boxes. We propose to address this issue by providing an interpretable layer on top of black box models by aggregating “local” explanations. We present GLocalX, a “local-first” model agnostic explanation method. Starting from local explanations expressed in form of local decision rules, GLocalX iteratively generalizes them into global explanations by hierarchically aggregating them. Our goal is to learn accurate yet simple interpretable models to emulate the given black box, and, if possible, replace it entirely. We validate GLocalX in a set of experiments in standard and constrained settings with limited or no access to either data or local explanations. Experiments show that GLocalX is able to accurately emulate several models with simple and small models, reaching state-of-the-art performance against natively global solutions. Our findings show how it is often possible to achieve a high level of both accuracy and comprehensibility of classification models, even in complex domains with high-dimensional data, without necessarily trading one property for the other. This is a key requirement for a trustworthy AI, necessary for adoption in high-stakes decision making applications.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1016/j.artint.2021.103457" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GMR2018a">4.</h4><small>[GMR2018a]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-three"></div><strong>Local Rule-Based Explanations of Black Box Decision Systems</strong><br><em>Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore , Pedreschi Dino, Turini Franco , Giannotti Fosca</em> (2018) - Arxive preprint
                <div class="collapse" id="collapse-three" aria-labelledby="heading-three" data-parent="#accordion-three">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of achine learning components in socially sensitive and safety-critical contexts. Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/1805.10820" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="BGG2023">5.</h4><small>[BGG2023]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-four"></div><strong>Benchmarking and survey of explanation methods for black box models</strong><br><em>Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, Salvatore Rinzivillo</em> (2023) - Springer Science+Business Media, LLC, part of Springer Nature. In Data Mining and Knowledge Discovery
                <div class="collapse" id="collapse-four" aria-labelledby="heading-four" data-parent="#accordion-four">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/s10618-023-00933-9" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="MBG2023">6.</h4><small>[MBG2023]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-five"></div><strong>Improving trust and confidence in medical skin lesion diagnosis through explainable deep learning</strong><br><em>Carlo Metta, Andrea Beretta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo, Fosca Giannotti </em> (2023) - Springer Nature. In International Journal of Data Science and Analytics
                <div class="collapse" id="collapse-five" aria-labelledby="heading-five" data-parent="#accordion-five">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">A key issue in critical contexts such as medical diagnosis is the interpretability of the deep learning models adopted in decision-making systems. Research in eXplainable Artificial Intelligence (XAI) is trying to solve this issue. However, often XAI approaches are only tested on generalist classifier and do not represent realistic problems such as those of medical diagnosis. In this paper, we aim at improving the trust and confidence of users towards automatic AI decision systems in the field of medical skin lesion diagnosis by customizing an existing XAI approach for explaining an AI model able to recognize different types of skin lesions. The explanation is generated through the use of synthetic exemplar and counter-exemplar images of skin lesions and our contribution offers the practitioner a way to highlight the crucial traits responsible for the classification decision. A validation survey with domain experts, beginners, and unskilled people shows that the use of explanations improves trust and confidence in the automatic decision system. Also, an analysis of the latent space adopted by the explainer unveils that some of the most frequent skin lesion classes are distinctly separated. This phenomenon may stem from the intrinsic characteristics of each class and may help resolve common misclassifications made by human experts.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/s41060-023-00401-z" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="LSG2023">9.</h4><small>[LSG2023]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-eight"></div><strong>Geolet: An Interpretable Model for Trajectory Classification</strong><br><em>Landi Cristiano,Spinnato Francesco, Guidotti Riccardo, Monreale Anna, Nanni Mirco</em> (2023) - International Symposium on Intelligent Data Analysis. In Proceedings of the 2023 conference Advances in Intelligent Data Analysis XXI
                <div class="collapse" id="collapse-eight" aria-labelledby="heading-eight" data-parent="#accordion-eight">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The large and diverse availability of mobility data enables the development of predictive models capable of recognizing various types of movements. Through a variety of GPS devices, any moving entity, animal, person, or vehicle can generate spatio-temporal trajectories. This data is used to infer migration patterns, manage traffic in large cities, and monitor the spread and impact of diseases, all critical situations that necessitate a thorough understanding of the underlying problem. Researchers, businesses, and governments use mobility data to make decisions that affect people’s lives in many ways, employing accurate but opaque deep learning models that are difficult to interpret from a human standpoint. To address these limitations, we propose Geolet, a human-interpretable machine-learning model for trajectory classification. We use discriminative sub-trajectories extracted from mobility data to turn trajectories into a simplified representation that can be used as input by any machine learning classifier. We test our approach against state-of-the-art competitors on real-world datasets. Geolet outperforms black-box models in terms of accuracy while being orders of magnitude faster than its interpretable competitors.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-eight" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/978-3-031-30047-9_19" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="LCM2023">10.</h4><small>[LCM2023]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-nine"></div><strong>Modeling Events and Interactions through Temporal Processes -- A Survey</strong><br><em>Liguori Angelica, Caroprese Luciano, Minici Marco, Veloso Bruno, Spinnato Francesco, Nanni Mirco, Manco Giuseppe, Gama Joao</em> (2023) - Arxive preprint
                <div class="collapse" id="collapse-nine" aria-labelledby="heading-nine" data-parent="#accordion-nine">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">In real-world scenario, many phenomena produce a collection of events that occur in continuous time. Point Processes provide a natural mathematical framework for modeling these sequences of events. In this survey, we investigate probabilistic models for modeling event sequences through temporal processes. We revise the notion of event modeling and provide the mathematical foundations that characterize the literature on the topic. We define an ontology to categorize the existing approaches in terms of three families: simple, marked, and spatio-temporal point processes. For each family, we systematically review the existing approaches based based on deep learning. Finally, we analyze the scenarios where the proposed techniques can be used for addressing prediction and modeling aspects.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-nine" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.48550/arxiv.2303.06067" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="BGG2023c">12.</h4><small>[BGG2023c]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-eleven"></div><strong>Interpretable Latent Space to Enable Counterfactual Explanations</strong><br><em>Francesco Bodria, Riccardo Guidotti, Fosca Giannotti & Dino Pedreschi </em> (2022) - Proceedings of the 25th international conference on Discovery Science (DS), 2022, Montpellier. In Lecture Notes in Computer Science()
                <div class="collapse" id="collapse-eleven" aria-labelledby="heading-eleven" data-parent="#accordion-eleven">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Many dimensionality reduction methods have been introduced to map a data space into one with fewer features and enhance machine learning models’ capabilities. This reduced space, called latent space, holds properties that allow researchers to understand the data better and produce better models. This work proposes an interpretable latent space that preserves the similarity of data points and supports a new way of learning a classification model that allows prediction and explanation through counterfactual examples. We demonstrate with extensive experiments the effectiveness of the latent space with respect to different metrics in comparison with several competitors, as well as the quality of the achieved counterfactual explanations.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-eleven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/978-3-031-18840-4_37" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="BGG2023b">13.</h4><small>[BGG2023b]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twelve"></div><strong>Transparent Latent Space Counterfactual Explanations for Tabular Data</strong><br><em>Bodria Francesco, Riccardo Guidotti, Fosca Giannotti, Dino Pedreschi</em> (2022) - Proceedings of Data Science and Advanced Analytics (DSAA), 2022 IEEE 9th International Conference. In Proceedings of the 9th IEEE International Conference on Data Science and Advanced, Analytics (DSAA)
                <div class="collapse" id="collapse-twelve" aria-labelledby="heading-twelve" data-parent="#accordion-twelve">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Artificial Intelligence decision-making systems have dramatically increased their predictive performance in recent years, beating humans in many different specific tasks. However, with increased performance has come an increase in the complexity of the black-box models adopted by the AI systems, making them entirely obscure for the decision process adopted. Explainable AI is a field that seeks to make AI decisions more transparent by producing explanations. In this paper, we propose T-LACE, an approach able to retrieve post-hoc counterfactual explanations for a given pre-trained black-box model. T-LACE exploits the similarity and linearity proprieties of a custom-created transparent latent space to build reliable counterfactual explanations. We tested T-LACE on several tabular datasets and provided qualitative evaluations of the generated explanations in terms of similarity, robustness, and diversity. Comparative analysis against various state-of-the-art counterfactual explanation methods shows the higher effectiveness of our approach.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twelve" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://ieeexplore.ieee.org/document/10032407" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="TSS2022">20.</h4><small>[TSS2022]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-nineteen"></div><strong>Explainable AI for Time Series Classification: A Review, Taxonomy and Research Directions</strong><br><em>Andreas Theissler, Francesco Spinnato, Udo Schlegel, Riccardo Guidotti</em> (2022) - IEEE Access. In IEEE Access ( Volume: 10)
                <div class="collapse" id="collapse-nineteen" aria-labelledby="heading-nineteen" data-parent="#accordion-nineteen">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Time series data is increasingly used in a wide range of fields, and it is often relied on in crucial applications and high-stakes decision-making. For instance, sensors generate time series data to recognize different types of anomalies through automatic decision-making systems. Typically, these systems are realized with machine learning models that achieve top-tier performance on time series classification tasks. Unfortunately, the logic behind their prediction is opaque and hard to understand from a human standpoint. Recently, we observed a consistent increase in the development of explanation methods for time series classification justifying the need to structure and review the field. In this work, we (a) present the first extensive literature review on Explainable AI (XAI) for time series classification, (b) categorize the research field through a taxonomy subdividing the methods into time points-based, subsequences-based and instance-based, and (c) identify open research directions regarding the type of explanations and the evaluation of explanations and interpretability.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-nineteen" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1109%2Faccess.2022.3207765" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="G2022">21.</h4><small>[G2022]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty"></div><strong>Counterfactual explanations and how to find them: literature review and benchmarking</strong><br><em>Riccardo Guidotti</em> (2022) - Data Mining and Knowledge Discovery. In Data Mining and Knowledge Discovery
                <div class="collapse" id="collapse-twenty" aria-labelledby="heading-twenty" data-parent="#accordion-twenty">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/s10618-022-00831-6" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="MG2022">22.</h4><small>[MG2022]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-one"></div><strong>Investigating Debiasing Effects on Classification and Explainability</strong><br><em>Marta Marchiori Manerba, Guidotti Riccardo</em> (2022) - Conference on AI, Ethics, and Society (AIES 2022). In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES'22)
                <div class="collapse" id="collapse-twenty-one" aria-labelledby="heading-twenty-one" data-parent="#accordion-twenty-one">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal’s novelty and effectiveness for evaluation and bias detection purposes.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><small>Research Line <strong>1▪5</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="PBF2022">24.</h4><small>[PBF2022]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-three"></div><strong>Co-design of human-centered, explainable AI for clinical decision support</strong><br><em>Panigutti Cecilia, Beretta Andrea, Fadda Daniele , Giannotti Fosca, Pedreschi Dino, Perotti Alan, Rinzivillo Salvatore</em> (2022). In ACM Transactions on Interactive Intelligent Systems
                <div class="collapse" id="collapse-twenty-three" aria-labelledby="heading-twenty-three" data-parent="#accordion-twenty-three">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models, and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique, and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback, with a two-fold outcome: first, we obtain evidence that explanations increase users' trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so that we can re-design a better, more human-centered explanation interface.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><small>Research Line <strong>1▪3▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="SMM2022">26.</h4><small>[SMM2022]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-five"></div><strong>TriplEx: Triple Extraction for Explanation</strong><br><em>Setzu Mattia, Monreale Anna, Minervini Pasquale</em> (2022) - Third Conference on Cognitive Machine Intelligence (COGMI) 2021
                <div class="collapse" id="collapse-twenty-five" aria-labelledby="heading-twenty-five" data-parent="#accordion-twenty-five">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">nan</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="10.1109/CogMI52975.2021.00015" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪2</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="MGY2021">30.</h4><small>[MGY2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-nine"></div><strong>Exemplars and Counterexemplars Explanations for Skin Lesion Classifiers</strong><br><em>Carlo Metta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo</em> (2021) - IOS Press. In HHAI2022: Augmenting Human Intellect, S. Schlobach et al. (Eds.)
                <div class="collapse" id="collapse-twenty-nine" aria-labelledby="heading-twenty-nine" data-parent="#accordion-twenty-nine">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Explainable AI consists in developing models allowing interaction between decision systems and humans by making the decisions understandable. We propose a case study for skin lesion diagnosis showing how it is possible to provide explanations of the decisions of deep neural network trained to label skin lesions.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-nine" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://ebooks.iospress.nl/volumearticle/60877" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="MG2021">31.</h4><small>[MG2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty"></div><strong>FairShades: Fairness Auditing via Explainability in Abusive Language Detection Systems</strong><br><em>Marchiori Manerba Marta, Guidotti Riccardo</em> (2021) - Third Conference on Cognitive Machine Intelligence (COGMI) 2021. In 2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI)
                <div class="collapse" id="collapse-thirty" aria-labelledby="heading-thirty" data-parent="#accordion-thirty">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">At every stage of a supervised learning process, harmful biases can arise and be inadvertently introduced, ultimately leading to marginalization, discrimination, and abuse towards minorities. This phenomenon becomes particularly impactful in the sensitive real-world context of abusive language detection systems, where non-discrimination is difficult to assess. In addition, given the opaqueness of their internal behavior, the dynamics leading a model to a certain decision are often not clear nor accountable, and significant problems of trust could emerge. A robust value-oriented evaluation of models' fairness is therefore necessary. In this paper, we present FairShades, a model-agnostic approach for auditing the outcomes of abusive language detection systems.  Combining explainability and fairness evaluation, FairShades can identify unintended biases and sensitive categories towards which models are most discriminative. This objective is pursued through the auditing of meaningful counterfactuals generated within CheckList framework. We conduct several experiments on BERT-based models to demonstrate our proposal's novelty and effectiveness for unmasking biases. </p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1109/CogMI52975.2021.00014" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪5</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GMP2021">32.</h4><small>[GMP2021]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/springer_book_explain.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-thirty-one" type="button">
                <div class="modal fade" id="modal-thirty-one" tabindex="-1" role="dialog" aria-labelledby="#modal-thirty-one-Title" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Explainable AI Within the Digital Transformation and Cyber Physical Systems: XAI Methods and Applications</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/springer_book_explain.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-one"></div><strong>Explainable AI Within the Digital Transformation and Cyber Physical Systems: XAI Methods and Applications</strong><br><em>Guidotti Riccardo, Monreale Anna, Pedreschi Dino, Giannotti Fosca</em> (2021) - Explainable AI Within the Digital Transformation and Cyber Physical Systems (pp. 9-31)
                <div class="collapse" id="collapse-thirty-one" aria-labelledby="heading-thirty-one" data-parent="#accordion-thirty-one">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">This book presents Explainable Artificial Intelligence (XAI), which aims at producing explainable models that enable human users to understand and appropriately trust the obtained results. The authors discuss the challenges involved in making machine learning-based AI explainable. Firstly, that the explanations must be adapted to different stakeholders (end-users, policy makers, industries, utilities etc.) with different levels of technical knowledge (managers, engineers, technicians, etc.) in different application domains. Secondly, that it is important to develop an evaluation framework and standards in order to measure the effectiveness of the provided explanations at the human and the technical levels. This book gathers research contributions aiming at the development and/or the use of XAI techniques in order to address the aforementioned challenges in different applications such as healthcare, finance, cybersecurity, and document summarization. It allows highlighting the benefits and requirements of using explainable models in different application domains in order to provide guidance to readers to select the most adapted models to their specified problem and conditions. Includes recent developments of the use of Explainable Artificial Intelligence (XAI) in order to address the challenges of digital transition and cyber-physical systems; Provides a textual scientific description of the use of XAI in order to address the challenges of digital transition and cyber-physical systems; Presents examples and case studies in order to increase transparency and understanding of the methodological concepts.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/978-3-030-76409-8" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪5</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GD2021">33.</h4><small>[GD2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-two"></div><strong>Matrix Profile-Based Interpretable Time Series Classifier</strong><br><em>Guidotti Riccardo, D’Onofrio Matteo</em> (2021) - Frontiers in Artificial Intelligence
                <div class="collapse" id="collapse-thirty-two" aria-labelledby="heading-thirty-two" data-parent="#accordion-thirty-two">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Time series classification (TSC) is a pervasive and transversal problem in various fields ranging from disease diagnosis to anomaly detection in finance. Unfortunately, the most effective models used by Artificial Intelligence (AI) systems for TSC are not interpretable and hide the logic of the decision process, making them unusable in sensitive domains. Recent research is focusing on explanation methods to pair with the obscure classifier to recover this weakness. However, a TSC approach that is transparent by design and is simultaneously efficient and effective is even more preferable. To this aim, we propose an interpretable TSC method based on the patterns, which is possible to extract from the Matrix Profile (MP) of the time series in the training set. A smart design of the classification procedure allows obtaining an efficient and effective transparent classifier modeled as a decision tree that expresses the reasons for the classification as the presence of discriminative subsequences. Quantitative and qualitative experimentation shows that the proposed method overcomes the state-of-the-art interpretable approaches.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.3389/frai.2021.699448" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GM2021">35.</h4><small>[GM2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-four"></div><strong>Designing Shapelets for Interpretable Data-Agnostic Classification</strong><br><em>Guidotti Riccardo, Monreale Anna</em> (2021) - Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society
                <div class="collapse" id="collapse-thirty-four" aria-labelledby="heading-thirty-four" data-parent="#accordion-thirty-four">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Time series shapelets are discriminatory subsequences which are representative of a class, and their similarity to a time series can be used for successfully tackling the time series classification problem. The literature shows that Artificial Intelligence (AI) systems adopting classification models based on time series shapelets can be interpretable, more accurate, and significantly fast. Thus, in order to design a data-agnostic and interpretable classification approach, in this paper we first extend the notion of shapelets to different types of data, i.e., images, tabular and textual data. Then, based on this extended notion of shapelets we propose an interpretable data-agnostic classification method. Since the shapelets discovery can be time consuming, especially for data types more complex than time series, we exploit a notion of prototypes for finding candidate shapelets, and reducing both the time required to find a solution and the variance of shapelets. A wide experimentation on datasets of different types shows that the data-agnostic prototype-based shapelets returned by the proposed method empower an interpretable classification which is also fast, accurate, and stable. In addition, we show and we prove that shapelets can be at the basis of explainable AI methods.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1145/3461702.3462553" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="MGY2021">36.</h4><small>[MGY2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-five"></div><strong>Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling</strong><br><em>Metta Carlo, Guidotti Riccardo, Yin Yuan, Gallinari Patrick, Rinzivillo Salvatore</em> (2021) - 2021 IEEE Symposium on Computers and Communications (ISCC). In 2021 IEEE Symposium on Computers and Communications (ISCC)
                <div class="collapse" id="collapse-thirty-five" aria-labelledby="heading-thirty-five" data-parent="#accordion-thirty-five">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Explainable AI consists in developing mechanisms allowing for an interaction between decision systems and humans by making the decisions of the formers understandable. This is particularly important in sensitive contexts like in the medical domain. We propose a use case study, for skin lesion diagnosis, illustrating how it is possible to provide the practitioner with explanations on the decisions of a state of the art deep neural network classifier trained to characterize skin lesions from examples. Our framework consists of a trained classifier onto which an explanation module operates. The latter is able to offer the practitioner exemplars and counterexemplars for the classification diagnosis thus allowing the physician to interact with the automatic diagnosis system. The exemplars are generated via an adversarial autoencoder. We illustrate the behavior of the system on representative examples.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/iscc53001.2021.9631485" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GR2021">37.</h4><small>[GR2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-six"></div><strong>Ensemble of Counterfactual Explainers</strong><br><em>Guidotti Riccardo, Ruggieri Salvatore</em> (2021)
                <div class="collapse" id="collapse-thirty-six" aria-labelledby="heading-thirty-six" data-parent="#accordion-thirty-six">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">In eXplainable Artificial Intelligence (XAI), several counterfactual explainers have been proposed, each focusing on some desirable properties of counterfactual instances: minimality, actionability, stability, diversity, plausibility, discriminative power. We propose an ensemble of counterfactual explainers that boosts weak explainers, which provide only a subset of such properties, to a powerful method covering all of them. The ensemble runs weak explainers on a sample of instances and of features, and it combines their results by exploiting a diversity-driven selection function. The method is model-agnostic and, through a wrapping approach based on autoencoders, it is also data-agnostic</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-six" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://pages.di.unipi.it/ruggieri/Papers/ds2021.pdf" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="MBG2021">38.</h4><small>[MBG2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-seven"></div><strong>Explainable Deep Image Classifiers for Skin Lesion Diagnosis</strong><br><em>Metta Carlo, Beretta Andrea, Guidotti Riccardo, Yin Yuan, Gallinari Patrick, Rinzivillo Salvatore, Giannotti Fosca</em> (2021) - Arxive preprint. In International Journal of Data Science and Analytics
                <div class="collapse" id="collapse-thirty-seven" aria-labelledby="heading-thirty-seven" data-parent="#accordion-thirty-seven">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">A key issue in critical contexts such as medical diagnosis is the interpretability of the deep learning models adopted in decision-making systems. Research in eXplainable Artificial Intelligence (XAI) is trying to solve this issue. However, often XAI approaches are only tested on generalist classifier and do not represent realistic problems such as those of medical diagnosis. In this paper, we analyze a case study on skin lesion images where we customize an existing XAI approach for explaining a deep learning model able to recognize different types of skin lesions. The explanation is formed by synthetic exemplar and counter-exemplar images of skin lesion and offers the practitioner a way to highlight the crucial traits responsible for the classification decision. A survey conducted with domain experts, beginners and unskilled people proof that the usage of explanations increases the trust and confidence in the automatic decision system. Also, an analysis of the latent space adopted by the explainer unveils that some of the most frequent skin lesion classes are distinctly separated. This phenomenon could derive from the intrinsic characteristics of each class and, hopefully, can provide support in the resolution of the most frequent misclassifications by human experts.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-seven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/2111.11863" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪3▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="BGM2021">39.</h4><small>[BGM2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-eight"></div><strong>Deriving a Single Interpretable Model by Merging Tree-Based Classifiers</strong><br><em>Bonsignori Valerio, Guidotti Riccardo, Monreale Anna</em> (2021) - Discovery Science
                <div class="collapse" id="collapse-thirty-eight" aria-labelledby="heading-thirty-eight" data-parent="#accordion-thirty-eight">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Decision tree classifiers have been proved to be among the most interpretable models due to their intuitive structure that illustrates decision processes in form of logical rules. Unfortunately, more complex tree-based classifiers such as oblique trees and random forests overcome the accuracy of decision trees at the cost of becoming non interpretable. In this paper, we propose a method that takes as input any tree-based classifier and returns a single decision tree able to approximate its behavior. Our proposal merges tree-based classifiers by an intensional and extensional approach and applies a post-hoc explanation strategy. Our experiments shows that the retrieved single decision tree is at least as accurate as the original tree-based model, faithful, and more interpretable.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-eight" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-88942-5_27" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪2</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="PPB2021">42.</h4><small>[PPB2021]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/FairLens.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-forty-one" type="button">
                <div class="modal fade" id="modal-forty-one" tabindex="-1" role="dialog" aria-labelledby="#modal-forty-one-Title" aria-hidden="true">
                  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">FairLens: Auditing black-box clinical decision support systems</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/FairLens.png " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-forty-one"></div><strong>FairLens: Auditing black-box clinical decision support systems</strong><br><em>Panigutti Cecilia, Perotti Alan, Panisson André, Bajardi Paolo, Pedreschi Dino</em> (2021) - Information Processing & Management. In Journal of Information Processing and Management
                <div class="collapse" id="collapse-forty-one" aria-labelledby="heading-forty-one" data-parent="#accordion-forty-one">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Highlights: We present a pipeline to detect and explain potential fairness issues in Clinical DSS. We study and compare different multi-label classification disparity measures. We explore ICD9 bias in MIMIC-IV, an openly available ICU benchmark dataset</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1016/j.ipm.2021.102657" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="SGM2019">45.</h4><small>[SGM2019]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-forty-four"></div><strong>Global Explanations with Local Scoring</strong><br><em>Setzu Mattia, Guidotti Riccardo, Monreale Anna, Turini Franco</em> (2021) - Machine Learning and Knowledge Discovery in Databases. In ECML PKDD 2019: Machine Learning and Knowledge Discovery in Databases
                <div class="collapse" id="collapse-forty-four" aria-labelledby="heading-forty-four" data-parent="#accordion-forty-four">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Artificial Intelligence systems often adopt machine learning models encoding complex algorithms with potentially unknown behavior. As the application of these “black box” models grows, it is our responsibility to understand their inner working and formulate them in human-understandable explanations. To this end, we propose a rule-based model-agnostic explanation method that follows a local-to-global schema: it generalizes a global explanation summarizing the decision logic of a black box starting from the local explanations of single predicted instances. We define a scoring system based on a rule relevance score to extract global explanations from a set of local explanations in the form of decision rules. Experiments on several datasets and black boxes show the stability, and low complexity of the global explanations provided by the proposed solution in comparison with baselines and state-of-the-art global explainers.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-43823-4_14" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="PGG2019">48.</h4><small>[PGG2019]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-forty-seven"></div><strong>Meaningful Explanations of Black Box AI Decision Systems</strong><br><em>Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco</em> (2021) - Proceedings of the AAAI Conference on Artificial Intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence
                <div class="collapse" id="collapse-forty-seven" aria-labelledby="heading-forty-seven" data-parent="#accordion-forty-seven">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Black box AI systems for automated decision making, often based on machine learning over (big) data, map a user’s features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases inherited by the algorithms from human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We focus on the urgent open challenge of how to construct meaningful explanations of opaque AI/ML systems, introducing the local-toglobal framework for black box explanation, articulated along three lines: (i) the language for expressing explanations in terms of logic rules, with statistical and causal interpretation; (ii) the inference of local explanations for revealing the decision rationale for a specific case, by auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of many local explanations into simple global ones, with algorithms that optimize for quality and comprehensibility. We argue that the local-first approach opens the door to a wide variety of alternative solutions along different dimensions: a variety of data sources (relational, text, images, etc.), a variety of learning problems (multi-label classification, regression, scoring, ranking), a variety of languages for expressing meaningful explanations, a variety of means to audit a black box.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-seven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1609/aaai.v33i01.33019780" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="G2021">49.</h4><small>[G2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-forty-eight"></div><strong>Evaluating local explanation methods on ground truth</strong><br><em>Guidotti Riccardo</em> (2021) - Artificial Intelligence. In Artificial Intelligence, 103428
                <div class="collapse" id="collapse-forty-eight" aria-labelledby="heading-forty-eight" data-parent="#accordion-forty-eight">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Evaluating local explanation methods is a difficult task due to the lack of a shared and universally accepted definition of explanation. In the literature, one of the most common ways to assess the performance of an explanation method is to measure the fidelity of the explanation with respect to the classification of a black box model adopted by an Artificial Intelligent system for making a decision. However, this kind of evaluation only measures the degree of adherence of the local explainer in reproducing the behavior of the black box classifier with respect to the final decision. Therefore, the explanation provided by the local explainer could be different in the content even though it leads to the same decision of the AI system. In this paper, we propose an approach that allows to measure to which extent the explanations returned by local explanation methods are correct with respect to a synthetic ground truth explanation. Indeed, the proposed methodology enables the generation of synthetic transparent classifiers for which the reason for the decision taken, i.e., a synthetic ground truth explanation, is available by design. Experimental results show how the proposed approach allows to easily evaluate local explanations on the ground truth and to characterize the quality of local explanation methods.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-eight" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1016/j.artint.2020.103428" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="LGR2020">50.</h4><small>[LGR2020]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-forty-nine"></div><strong>Explaining Sentiment Classification with Synthetic Exemplars and Counter-Exemplars</strong><br><em>Lampridis Orestis, Guidotti Riccardo, Ruggieri Salvatore</em> (2021) - Discovery Science. In In International Conference on Discovery Science (pp. 357-373). Springer, Cham.
                <div class="collapse" id="collapse-forty-nine" aria-labelledby="heading-forty-nine" data-parent="#accordion-forty-nine">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">We present xspells, a model-agnostic local approach for explaining the decisions of a black box model for sentiment classification of short texts. The explanations provided consist of a set of exemplar sentences and a set of counter-exemplar sentences. The former are examples classified by the black box with the same label as the text to explain. The latter are examples classified with a different label (a form of counter-factuals). Both are close in meaning to the text to explain, and both are meaningful sentences – albeit they are synthetically generated. xspells generates neighbors of the text to explain in a latent space using Variational Autoencoders for encoding text and decoding latent instances. A decision tree is learned from randomly generated neighbors, and used to drive the selection of the exemplars and counter-exemplars. We report experiments on two datasets showing that xspells outperforms the well-known lime method in terms of quality of explanations, fidelity, and usefulness, and that is comparable to it in terms of stability.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-nine" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-61527-7_24" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="PGM2019">51.</h4><small>[PGM2019]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Explaining Multi-label Black-Box Classifiers for Health Applications.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-fifty" type="button">
                <div class="modal fade" id="modal-fifty" tabindex="-1" role="dialog" aria-labelledby="#modal-fifty-Title" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Explaining Multi-label Black-Box Classifiers for Health Applications</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/Explaining Multi-label Black-Box Classifiers for Health Applications.png " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty"></div><strong>Explaining Multi-label Black-Box Classifiers for Health Applications</strong><br><em>Panigutti Cecilia, Guidotti Riccardo, Monreale Anna, Pedreschi Dino</em> (2021) - Precision Health and Medicine. In International Workshop on Health Intelligence (pp. 97-110). Springer, Cham.
                <div class="collapse" id="collapse-fifty" aria-labelledby="heading-fifty" data-parent="#accordion-fifty">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Today the state-of-the-art performance in classification is achieved by the so-called “black boxes”, i.e. decision-making systems whose internal logic is obscure. Such models could revolutionize the health-care system, however their deployment in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of transparency. The typical classification problem in health-care requires a multi-label approach since the possible labels are not mutually exclusive, e.g. diagnoses. We propose MARLENA, a model-agnostic method which explains multi-label black box decisions. MARLENA explains an individual decision in three steps. First, it generates a synthetic neighborhood around the instance to be explained using a strategy suitable for multi-label decisions. It then learns a decision tree on such neighborhood and finally derives from it a decision rule that explains the black box decision. Our experiments show that MARLENA performs well in terms of mimicking the black box behavior while gaining at the same time a notable amount of interpretability through compact decision rules, i.e. rules with limited length.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-24409-5_9" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GMC2019">52.</h4><small>[GMC2019]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty-one"></div><strong>Investigating Neighborhood Generation Methods for Explanations of Obscure Image Classifiers</strong><br><em>Guidotti Riccardo, Monreale Anna, Cariaggi Leonardo</em> (2021) - Advances in Knowledge Discovery and Data Mining. In In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 55-68). Springer, Cham.
                <div class="collapse" id="collapse-fifty-one" aria-labelledby="heading-fifty-one" data-parent="#accordion-fifty-one">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Given the wide use of machine learning approaches based on opaque prediction models, understanding the reasons behind decisions of black box decision systems is nowadays a crucial topic. We address the problem of providing meaningful explanations in the widely-applied image classification tasks. In particular, we explore the impact of changing the neighborhood generation function for a local interpretable model-agnostic explanator by proposing four different variants. All the proposed methods are based on a grid-based segmentation of the images, but each of them proposes a different strategy for generating the neighborhood of the image for which an explanation is required. A deep experimentation shows both improvements and weakness of each proposed approach.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-16148-4_5" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GMS2020">53.</h4><small>[GMS2020]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/ex_time_series_calssifier.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-fifty-two" type="button">
                <div class="modal fade" id="modal-fifty-two" tabindex="-1" role="dialog" aria-labelledby="#modal-fifty-two-Title" aria-hidden="true">
                  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Explaining Any Time Series Classifier</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/ex_time_series_calssifier.png " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty-two"></div><strong>Explaining Any Time Series Classifier</strong><br><em>Guidotti Riccardo, Monreale Anna, Spinnato Francesco, Pedreschi Dino, Giannotti Fosca</em> (2020) - 2020 IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)
                <div class="collapse" id="collapse-fifty-two" aria-labelledby="heading-fifty-two" data-parent="#accordion-fifty-two">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">We present a method to explain the decisions of black box models for time series classification. The explanation consists of factual and counterfactual shapelet-based rules revealing the reasons for the classification, and of a set of exemplars and counter-exemplars highlighting similarities and differences with the time series under analysis. The proposed method first generates exemplar and counter-exemplar time series in the latent feature space and learns a local latent decision tree classifier. Then, it selects and decodes those respecting the decision rules explaining the decision. Finally, it learns on them a shapelet-tree that reveals the parts of the time series that must, and must not, be contained for getting the returned outcome from the black box. A wide experimentation shows that the proposed method provides faithful, meaningful and interpretable explanations.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/cogmi50398.2020.00029" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="PPP2020">54.</h4><small>[PPP2020]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Doctor XAI.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-fifty-three" type="button">
                <div class="modal fade" id="modal-fifty-three" tabindex="-1" role="dialog" aria-labelledby="#modal-fifty-three-Title" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Doctor XAI: an ontology-based approach to black-box sequential data classification explanations</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/Doctor XAI.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty-three"></div><strong>Doctor XAI: an ontology-based approach to black-box sequential data classification explanations</strong><br><em>Panigutti Cecilia, Perotti Alan, Pedreschi Dino</em> (2020) - FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. In FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency
                <div class="collapse" id="collapse-fifty-three" aria-labelledby="heading-fifty-three" data-parent="#accordion-fifty-three">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://dl.acm.org/doi/10.1145/3351095.3372855" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪3▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="RGG2020">55.</h4><small>[RGG2020]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty-four"></div><strong>Opening the black box: a primer for anti-discrimination</strong><br><em>Ruggieri Salvatore, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Pedreschi Dino, Turini Franco</em> (2020). In ANNUARIO DI DIRITTO COMPARATO E DI STUDI LEGISLATIVI
                <div class="collapse" id="collapse-fifty-four" aria-labelledby="heading-fifty-four" data-parent="#accordion-fifty-four">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The pervasive adoption of Artificial Intelligence (AI) models in the modern information society, requires counterbalancing the growing decision power demanded to AI models with risk assessment methodologies. In this paper, we consider the risk of discriminatory decisions and review approaches for discovering discrimination and for designing fair AI models. We highlight the tight relations between discrimination discovery and explainable AI, with the latter being a more general approach for understanding the behavior of black boxes.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://hdl.handle.net/11568/1088440" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GM2020">56.</h4><small>[GM2020]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty-five"></div><strong>Data-Agnostic Local Neighborhood Generation</strong><br><em>Guidotti Riccardo, Monreale Anna</em> (2020) - 2020 IEEE International Conference on Data Mining (ICDM). In 2020 IEEE International Conference on Data Mining (ICDM)
                <div class="collapse" id="collapse-fifty-five" aria-labelledby="heading-fifty-five" data-parent="#accordion-fifty-five">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Synthetic data generation has been widely adopted in software testing, data privacy, imbalanced learning, machine learning explanation, etc. In such contexts, it is important to generate data samples located within “local” areas surrounding specific instances. Local synthetic data can help the learning phase of predictive models, and it is fundamental for methods explaining the local behavior of obscure classifiers. The contribution of this paper is twofold. First, we introduce a method based on generative operators allowing the synthetic neighborhood generation by applying specific perturbations on a given input instance. The key factor consists in performing a data transformation that makes applicable to any type of data, i.e., data-agnostic. Second, we design a framework for evaluating the goodness of local synthetic neighborhoods exploiting both supervised and unsupervised methodologies. A deep experimentation shows the effectiveness of the proposed method.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://ieeexplore.ieee.org/document/9338395" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="BPP2020">57.</h4><small>[BPP2020]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty-six"></div><strong>Explainability Methods for Natural Language Processing: Applications to Sentiment Analysis</strong><br><em>Bodria Francesco, Panisson André , Perotti Alan, Piaggesi Simone </em> (2020) - Discussion Paper
                <div class="collapse" id="collapse-fifty-six" aria-labelledby="heading-fifty-six" data-parent="#accordion-fifty-six">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">nan</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://ceur-ws.org/Vol-2646/18-paper.pdf" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="GMP2019">60.</h4><small>[GMP2019]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-fifty-nine"></div><strong>The AI black box explanation problem</strong><br><em>Guidotti Riccardo, Monreale Anna, Pedreschi Dino</em> (2019) - ERCIM News, 116, 12-13. In ERCIM News, 116, 12-13
                <div class="collapse" id="collapse-fifty-nine" aria-labelledby="heading-fifty-nine" data-parent="#accordion-fifty-nine">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">nan</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://ercim-news.ercim.eu/images/stories/EN116/EN116-web.pdf#page=12" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪2▪3</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center"></div>
              <div class="col-lg-1 text-right">
                <h4 class="anchor" id="PGG2018">61.</h4><small>[PGG2018]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-sixty"></div><strong>Open the Black Box Data-Driven Explanation of Black Box Decision Systems</strong><br><em>Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna , Pappalardo Luca , Ruggieri Salvatore , Turini Franco </em> (2018) - Arxive preprint
                <div class="collapse" id="collapse-sixty" aria-labelledby="heading-sixty" data-parent="#accordion-sixty">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-sixty" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/1806.09936" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1</strong></small></p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="entry-content mt-0">
        <div class="container mb-10">
          <div class="row gx-lg-3 gy-3 mt-lg-4 mt-md-2 mt-2 mb-5">
            <h3>Researchers working on this line</h3>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Guidotti.jpg" alt="Riccardo Guidotti"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Riccardo<br/>Guidotti</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Assitant Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Turini.jpg" alt="Franco Turini"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Franco<br/>Turini</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Full Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 2 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Ruggieri.jpg" alt="Salvatore Ruggieri"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Salvatore<br/>Ruggieri</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Full Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 2</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Nanni.jpg" alt="Mirco Nanni"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Mirco<br/>Nanni</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Rinzivillo.jpg" alt="Salvo Rinzivillo"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Salvo<br/>Rinzivillo</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Beretta.jpg" alt="Andrea Beretta"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Andrea<br/>Beretta</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Monreale.jpg" alt="Anna Monreale"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Anna<br/>Monreale</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Associate Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Panigutti.jpg" alt="Cecilia Panigutti"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Cecilia<br/>Panigutti</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Setzu.jpg" alt="Mattia Setzu"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Mattia<br/>Setzu</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 2</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Spinnato.jpg" alt="Francesco Spinnato"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Francesco<br/>Spinnato</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Naretto.jpg" alt="Francesca Naretto"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Francesca<br/>Naretto</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Post Doctoral Researcher</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Bodria.jpg" alt="Francesco Bodria"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Francesco<br/>Bodria</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Metta.jpg" alt="Carlo Metta"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Carlo<br/>Metta</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 2 ▪ 3 ▪4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Malizia.jpg" alt="Alessio Malizia"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Alessio<br/>Malizia</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Associate Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Marchiori Manerba.jpg" alt="Marta Marchiori Manerba"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Marta<br/>Marchiori Manerba</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 2 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Cinquini.jpg" alt="Martina Cinquini"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Martina<br/>Cinquini</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 2</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Landi.jpg" alt="Cristiano Landi"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Cristiano<br/>Landi</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Tonati.jpg" alt="Samuele Tonati"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Samuele<br/>Tonati</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Arias Duart.jpg" alt="Anna Arias Duart"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Anna<br/>Arias Duart</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Visiting Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Fedele.jpg" alt="Andrea Fedele"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Andrea<br/>Fedele</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Mazzoni.jpg" alt="Federico Mazzoni"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Federico<br/>Mazzoni</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Punzi.jpg" alt="Clara Punzi"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Clara<br/>Punzi</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Pugnana.jpg" alt="Andrea Pugnana"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Andrea<br/>Pugnana</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>2</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Lalli.jpg" alt="Margherita Lalli"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Margherita<br/>Lalli</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">IMT Lucca</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Di Vece.jpg" alt="Marzio Di Vece"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Marzio<br/>Di Vece</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">IMT Lucca</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
          </div>
        </div>
      </div>
    </article>
    <!-- Footer-->
    <footer class="site-footer">
      <div class="footer-widgets">
        <div class="container">
          <div class="row gx-lg-5">
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-1">
                <h4 class="text-yellow mb-3">Principal Investigator</h4>
                <div class="text-white">
                  <p class="mb-0">Fosca Giannotti</p>
                  <p class="mb-0">Scuola Normale Superiore</p><br>
                  <p class="mb-0">Piazza dei Cavalieri, 7</p>
                  <p class="mb-0">56126 Pisa, Italy</p><br>
                  <p class="mb-0">Email: fosca.giannotti @ sns.it</p>
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-4">
                <div class="text-white">
                  <p>The XAI Project receives funding from the European Union's Horizon 2020 Excellent Science European Research Council (ERC) programme under grant agreement No. 834756</p>The views and opinions expressed in this website are the sole responsibility of the author and do not necessarily reflect the views of the European Commission.
                  <p></p><img class="mb-4" src="assets/images/logo-eu.jpg" alt="EU flag">
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-2">
                <ul>
                  <li><a href="index.html">Xai</a></li>
                  <li><a href="about.html">Project details</a></li>
                  <li><a href="research-lines.html">Research lines</a></li>
                  <li><a href="people.html">People</a></li>
                  <li><a href="resources.html">Resources</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="footer-bottom-area">
        <div class="container">
          <div class="row gx-lg-5 align-items-center">
            <div class="col-md-12"><span class="text-yellow strong">Webdesign: Daniele Fadda © 2023</span></div>
          </div>
        </div>
      </div>
    </footer>
    <!-- End Footer-->
    <!-- javascript files-->
    <!-- jquery-->
    <script src="assets/js/jquery.min.js"></script>
    <!-- lozad js-->
    <script src="assets/js/lozad.min.js"></script>
    <!-- Bootstrap js-->
    <script src="assets/js/bootstrap.bundle.min.js"></script>
    <!-- Aos js-->
    <script src="assets/js/aos.js"></script>
    <!-- Slick flickity js-->
    <script src="assets/js/flickity.pkgd.min.js"></script>
    <!-- Magnific popup js-->
    <script src="assets/js/jquery.magnific-popup.min.js"></script>
    <!-- Countdown js-->
    <script src="assets/js/jquery.countdown.js"></script>
    <!-- CountTo js-->
    <script src="assets/js/jquery.countTo.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/global.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/cookies.js"></script>
  </body>
</html>