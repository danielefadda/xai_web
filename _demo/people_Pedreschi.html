<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- favicon-->
    <link rel="shortcut icon" href="assets/images/favicon.ico">
    <!-- Site Title-->
    <title>Xai - Website</title>
    <meta name="description" content="Science and technology for the eXplanation of AI decision making">
    <!-- Bootstrap CSS file-->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Flickity CSS file-->
    <link href="assets/css/flickity.min.css" rel="stylesheet">
    <!-- Main CSS file-->
    <link href="assets/css/style.css" rel="stylesheet">
    <!-- Fontawesome 5 CSS file-->
    <link href="assets/css/fontawesome-all.min.css" rel="stylesheet">
    <!-- Magnific Popup CSS-->
    <link href="assets/css/magnific-popup.css" rel="stylesheet">
    <!-- Google Fonts-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&amp;display=swap">
    <script type="text/javascript">
      // Matomo Code
      var _paq = window._paq = window._paq || [];
      /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
      _paq.push(['trackPageView']);
      _paq.push(['enableLinkTracking']);
      (function () {
      var u="//piwikdd.isti.cnr.it/";
      _paq.push(['setTrackerUrl', u+'piwik.php']);
      _paq.push(['setSiteId', '12']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
      })
      ();
    </script>
  </head>
  <body>
    <!-- Navbar-->
    <div class="site-header header-bottom-area">
      <nav class="navbar navbar-expand-lg navbar-light sticky">
        <div class="container"><a class="navbar-brand" href="index.html"><img class="site-logo" src="assets/images/logo/xai_logo_color.png" alt="Logo"></a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarNavDropdown">
            <ul class="navbar-nav mx-auto">
              <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
              <li class="nav-item"><a class="nav-link" href="about.html">Project details</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="research-lines.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research Lines</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="line_1.html">1. Local to global</a><a class="dropdown-item" href="line_2.html">2. Casual reasoning</a><a class="dropdown-item" href="line_3.html">3. Platform and XUI</a><a class="dropdown-item" href="line_4.html">4. Case studies</a><a class="dropdown-item" href="line_5.html">5. Ethics and legal</a></div>
              </li>
              <li class="nav-item"><a class="nav-link" href="people.html">People</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="resources.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications and Resources</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="resources.html#publications">Publications</a>
                  <!-- a.dropdown-item(href='reports.html') Reports--><a class="dropdown-item" href="resources.html#thesis">Thesis</a><a class="dropdown-item" href="dissemination.html">Dissemination tools</a>
                </div>
              </li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Seminars</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="seminars.html">All Seminars</a><a class="dropdown-item" href="dist-seminars.html">Distinguished seminars</a>
                  <!--a.dropdown-item(href='internal-events.html') Internal Events-->
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
    <!-- End Navbar-->
    <!-- Cookies-->
    <div class="cookie-banner" style="display: none">
      <p class="mx-2">We use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. 
        <u><a href="privacy.html" link’="privacy.html">No, give me more info.</a></u>
      </p>
      <button class="close">&times;</button>
    </div>
    <!-- End Cookies-->
    <article class="entry"></article>
    <div class="entry-content">
      <div class="bg-yellow pt-6 pt-lg-8 pb-4 pb-lg-5">
        <div class="bg-half">
          <div class="container">
            <div class="row">
              <div class="col-lg-4 offset-lg-4">
                <h1 class="text-center">Dino Pedreschi</h1><img class="card-img" src="assets/images/p_Pedreschi.jpg" alt="Dino Pedreschi"/>
                <div class="member-content">
                  <div class="member-text px-0 py-3">
                    <p>Role: Full Professor</p>
                    <p class="mb-0">Affiliation: University of Pisa</p>
                    <hr/>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="container mt-6">
        <div class="row justify-content-lg-center">
          <div class="col-lg-8"></div>
        </div>
      </div>
      <div class="container mb-6">
        <div class="row">
          <div class="row mt-5 justify-content-center" id="GMR2018">
            <div class="col-lg-1 text-right">
              <h4>1.</h4><small>[GMR2018]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-zero"></div><strong>A Survey of Methods for Explaining Black Box Models</strong><br><em>Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco, Giannotti Fosca, Pedreschi Dino</em> (2022) - ACM Computing Surveys. In ACM computing surveys (CSUR), 51(5), 1-42.
              <div class="collapse" id="collapse-zero" aria-labelledby="heading-zero" data-parent="#accordion-zero">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-zero" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1145/3236009" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GMG2019"></div>
            <div class="col-lg-1 text-right">
              <h4>2.</h4><small>[GMG2019]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-one" type="button">
              <div class="modal fade" id="modal-one" tabindex="-1" role="dialog" aria-labelledby="#modal-one-Title" aria-hidden="true">
                <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Factual and Counterfactual Explanations for Black Box Decision Making</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-one"></div><strong>Factual and Counterfactual Explanations for Black Box Decision Making</strong><br><em>Guidotti Riccardo, Monreale Anna, Giannotti Fosca, Pedreschi Dino, Ruggieri Salvatore, Turini Franco</em> (2021) - IEEE Intelligent Systems. In IEEE Intelligent Systems
              <div class="collapse" id="collapse-one" aria-labelledby="heading-one" data-parent="#accordion-one">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/mis.2019.2957223" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="SGM2021"></div>
            <div class="col-lg-1 text-right">
              <h4>3.</h4><small>[SGM2021]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/02_glocalX.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-two" type="button">
              <div class="modal fade" id="modal-two" tabindex="-1" role="dialog" aria-labelledby="#modal-two-Title" aria-hidden="true">
                <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">GLocalX - From Local to Global Explanations of Black Box AI Models</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/02_glocalX.jpg " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-two"></div><strong>GLocalX - From Local to Global Explanations of Black Box AI Models</strong><br><em>Setzu Mattia, Guidotti Riccardo, Monreale Anna, Turini Franco, Pedreschi Dino, Giannotti Fosca</em> (2021) - Artificial Intelligence. In Artificial Intelligence
              <div class="collapse" id="collapse-two" aria-labelledby="heading-two" data-parent="#accordion-two">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are “black boxes” which we are not able to understand. Relying on these models has a multifaceted impact and raises significant concerns about their transparency. Applications in sensitive and critical domains are a strong motivational factor in trying to understand the behavior of black boxes. We propose to address this issue by providing an interpretable layer on top of black box models by aggregating “local” explanations. We present GLocalX, a “local-first” model agnostic explanation method. Starting from local explanations expressed in form of local decision rules, GLocalX iteratively generalizes them into global explanations by hierarchically aggregating them. Our goal is to learn accurate yet simple interpretable models to emulate the given black box, and, if possible, replace it entirely. We validate GLocalX in a set of experiments in standard and constrained settings with limited or no access to either data or local explanations. Experiments show that GLocalX is able to accurately emulate several models with simple and small models, reaching state-of-the-art performance against natively global solutions. Our findings show how it is often possible to achieve a high level of both accuracy and comprehensibility of classification models, even in complex domains with high-dimensional data, without necessarily trading one property for the other. This is a key requirement for a trustworthy AI, necessary for adoption in high-stakes decision making applications.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1016/j.artint.2021.103457" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GMR2018a"></div>
            <div class="col-lg-1 text-right">
              <h4>4.</h4><small>[GMR2018a]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-three"></div><strong>Local Rule-Based Explanations of Black Box Decision Systems</strong><br><em>Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore , Pedreschi Dino, Turini Franco , Giannotti Fosca</em> (2018) - Arxive preprint
              <div class="collapse" id="collapse-three" aria-labelledby="heading-three" data-parent="#accordion-three">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of achine learning components in socially sensitive and safety-critical contexts. Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/1805.10820" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="BGG2023"></div>
            <div class="col-lg-1 text-right">
              <h4>5.</h4><small>[BGG2023]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-four"></div><strong>Benchmarking and survey of explanation methods for black box models</strong><br><em>Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, Salvatore Rinzivillo</em> (2023) - Springer Science+Business Media, LLC, part of Springer Nature. In Data Mining and Knowledge Discovery
              <div class="collapse" id="collapse-four" aria-labelledby="heading-four" data-parent="#accordion-four">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/s10618-023-00933-9" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="BGG2023c"></div>
            <div class="col-lg-1 text-right">
              <h4>12.</h4><small>[BGG2023c]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-eleven"></div><strong>Interpretable Latent Space to Enable Counterfactual Explanations</strong><br><em>Francesco Bodria, Riccardo Guidotti, Fosca Giannotti & Dino Pedreschi </em> (2022) - Proceedings of the 25th international conference on Discovery Science (DS), 2022, Montpellier. In Lecture Notes in Computer Science()
              <div class="collapse" id="collapse-eleven" aria-labelledby="heading-eleven" data-parent="#accordion-eleven">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Many dimensionality reduction methods have been introduced to map a data space into one with fewer features and enhance machine learning models’ capabilities. This reduced space, called latent space, holds properties that allow researchers to understand the data better and produce better models. This work proposes an interpretable latent space that preserves the similarity of data points and supports a new way of learning a classification model that allows prediction and explanation through counterfactual examples. We demonstrate with extensive experiments the effectiveness of the latent space with respect to different metrics in comparison with several competitors, as well as the quality of the achieved counterfactual explanations.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-eleven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/978-3-031-18840-4_37" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="BGG2023b"></div>
            <div class="col-lg-1 text-right">
              <h4>13.</h4><small>[BGG2023b]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-twelve"></div><strong>Transparent Latent Space Counterfactual Explanations for Tabular Data</strong><br><em>Bodria Francesco, Riccardo Guidotti, Fosca Giannotti, Dino Pedreschi</em> (2022) - Proceedings of Data Science and Advanced Analytics (DSAA), 2022 IEEE 9th International Conference. In Proceedings of the 9th IEEE International Conference on Data Science and Advanced, Analytics (DSAA)
              <div class="collapse" id="collapse-twelve" aria-labelledby="heading-twelve" data-parent="#accordion-twelve">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Artificial Intelligence decision-making systems have dramatically increased their predictive performance in recent years, beating humans in many different specific tasks. However, with increased performance has come an increase in the complexity of the black-box models adopted by the AI systems, making them entirely obscure for the decision process adopted. Explainable AI is a field that seeks to make AI decisions more transparent by producing explanations. In this paper, we propose T-LACE, an approach able to retrieve post-hoc counterfactual explanations for a given pre-trained black-box model. T-LACE exploits the similarity and linearity proprieties of a custom-created transparent latent space to build reliable counterfactual explanations. We tested T-LACE on several tabular datasets and provided qualitative evaluations of the generated explanations in terms of similarity, robustness, and diversity. Comparative analysis against various state-of-the-art counterfactual explanation methods shows the higher effectiveness of our approach.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twelve" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://ieeexplore.ieee.org/document/10032407" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="FGP2022"></div>
            <div class="col-lg-1 text-right">
              <h4>15.</h4><small>[FGP2022]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-fourteen"></div><strong>Explaining Siamese Networks in Few-Shot Learning for Audio Data</strong><br><em>Fedele Andrea, Guidotti Riccardo, Pedreschi Dino</em> (2022) - International Conference on Discovery Science. In Discovery Science
              <div class="collapse" id="collapse-fourteen" aria-labelledby="heading-fourteen" data-parent="#accordion-fourteen">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Machine learning models are not able to generalize correctly when queried on samples belonging to class distributions that were never seen during training. This is a critical issue, since real world applications might need to quickly adapt without the necessity of re-training. To overcome these limitations, few-shot learning frameworks have been proposed and their applicability has been studied widely for computer vision tasks. Siamese Networks learn pairs similarity in form of a metric that can be easily extended on new unseen classes. Unfortunately, the downside of such systems is the lack of explainability. We propose a method to explain the outcomes of Siamese Networks in the context of few-shot learning for audio data. This objective is pursued through a local perturbation-based approach that evaluates segments-weighted-average contributions to the final outcome considering the interplay between different areas of the audio spectrogram. Qualitative and quantitative results demonstrate that our method is able to show common intra-class characteristics and erroneous reliance on silent sections.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fourteen" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/978-3-031-18840-4_36" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="NPR2022"></div>
            <div class="col-lg-1 text-right">
              <h4>18.</h4><small>[NPR2022]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-seventeen"></div><strong>Methods and tools for causal discovery and causal inference</strong><br><em>Ana Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi, João Gama</em> (2022) - Wires Data Mining and Knowledge Discovery. In Wires Data Mining and Knowledge Discovery
              <div class="collapse" id="collapse-seventeen" aria-labelledby="heading-seventeen" data-parent="#accordion-seventeen">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation-based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-seventeen" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href=" https://doi.org/10.1002/widm.1449" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>2</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="BRF2022"></div>
            <div class="col-lg-1 text-right">
              <h4>23.</h4><small>[BRF2022]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/latent-space-exploration.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-twenty-two" type="button">
              <div class="modal fade" id="modal-twenty-two" tabindex="-1" role="dialog" aria-labelledby="#modal-twenty-two-Title" aria-hidden="true">
                <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Explaining Black Box with visual exploration of Latent Space</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/latent-space-exploration.png " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-two"></div><strong>Explaining Black Box with visual exploration of Latent Space</strong><br><em>Bodria Francesco, Rinzivillo Salvatore, Fadda Daniele, Guidotti Riccardo, Fosca Giannotti, Pedreschi Dino</em> (2022) - EUROVIS 2022. In Proceedings of the 2022 Conference Eurovis 2022
              <div class="collapse" id="collapse-twenty-two" aria-labelledby="heading-twenty-two" data-parent="#accordion-twenty-two">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Autoencoders are a powerful yet opaque feature reduction technique, on top of which we propose a novel way for the joint visual exploration of both latent and real space. By interactively exploiting the mapping between latent and real features, it is possible to unveil the meaning of latent features while providing deeper insight into the original variables. To achieve this goal, we exploit and re-adapt existing approaches from eXplainable Artificial Intelligence (XAI) to understand the relationships between the input and latent features. The uncovered relationships between input features and latent ones allow the user to understand the data structure concerning external variables such as the predictions of a classification model. We developed an interactive framework that visually explores the latent space and allows the user to understand the relationships of the input features with model prediction.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://diglib.eg.org/handle/10.2312/evs20221098" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PBF2022"></div>
            <div class="col-lg-1 text-right">
              <h4>24.</h4><small>[PBF2022]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-three"></div><strong>Co-design of human-centered, explainable AI for clinical decision support</strong><br><em>Panigutti Cecilia, Beretta Andrea, Fadda Daniele , Giannotti Fosca, Pedreschi Dino, Perotti Alan, Rinzivillo Salvatore</em> (2022). In ACM Transactions on Interactive Intelligent Systems
              <div class="collapse" id="collapse-twenty-three" aria-labelledby="heading-twenty-three" data-parent="#accordion-twenty-three">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models, and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique, and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback, with a two-fold outcome: first, we obtain evidence that explanations increase users' trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so that we can re-design a better, more human-centered explanation interface.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PBP2022"></div>
            <div class="col-lg-1 text-right">
              <h4>25.</h4><small>[PBP2022]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-four"></div><strong>Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems</strong><br><em>Panigutti Cecilia, Beretta Andrea, Pedreschi Dino, Giannotti Fosca</em> (2022) - 2022 Conference on Human Factors in Computing Systems. In Proceedings of the 2022 Conference on Human Factors in Computing Systems
              <div class="collapse" id="collapse-twenty-four" aria-labelledby="heading-twenty-four" data-parent="#accordion-twenty-four">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The field of eXplainable Artificial Intelligence (XAI) focuses on providing explanations for AI systems' decisions. XAI applications to AI-based Clinical Decision Support Systems (DSS) should increase trust in the DSS by allowing clinicians to investigate the reasons behind its suggestions. In this paper, we present the results of a user study on the impact of advice from a clinical DSS on healthcare providers' judgment in two different cases: the case where the clinical DSS explains its suggestion and the case it does not. We examined the weight of advice, the behavioral intention to use the system, and the perceptions with quantitative and qualitative measures. Our results indicate a more significant impact of advice when an explanation for the DSS decision is provided. Additionally, through the open-ended questions, we provide some insights on how to improve the explanations in the diagnosis forecasts for healthcare assistants, nurses, and doctors.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><small>Research Line <strong>4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PMC2022"></div>
            <div class="col-lg-1 text-right">
              <h4>29.</h4><small>[PMC2022]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Ethical Societal and Legal.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-twenty-eight" type="button">
              <div class="modal fade" id="modal-twenty-eight" tabindex="-1" role="dialog" aria-labelledby="#modal-twenty-eight-Title" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Ethical, societal and legal issues in deep learning for healthcare</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/Ethical Societal and Legal.jpg " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-eight"></div><strong>Ethical, societal and legal issues in deep learning for healthcare</strong><br><em>Panigutti Cecilia, Monreale Anna, Comandè Giovanni, Pedreschi Dino</em> (2022) - Deep Learning in Biology and Medicine. In Deep Learning in Biology and Medicine
              <div class="collapse" id="collapse-twenty-eight" aria-labelledby="heading-twenty-eight" data-parent="#accordion-twenty-eight">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Biology, medicine and biochemistry have become data-centric fields for which Deep Learning methods are delivering groundbreaking results. Addressing high impact challenges, Deep Learning in Biology and Medicine provides an accessible and organic collection of Deep Learning essays on bioinformatics and medicine. It caters for a wide readership, ranging from machine learning practitioners and data scientists seeking methodological knowledge to address biomedical applications, to life science specialists in search of a gentle reference for advanced data analytics.With contributions from internationally renowned experts, the book covers foundational methodologies in a wide spectrum of life sciences applications, including electronic health record processing, diagnostic imaging, text processing, as well as omics-data processing. This survey of consolidated problems is complemented by a selection of advanced applications, including cheminformatics and biomedical interaction network analysis. A modern and mindful approach to the use of data-driven methodologies in the life sciences also requires careful consideration of the associated societal, ethical, legal and transparency challenges, which are covered in the concluding chapters of this book.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-eight" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://www.worldscientific.com/worldscibooks/10.1142/q0322" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GMP2021"></div>
            <div class="col-lg-1 text-right">
              <h4>32.</h4><small>[GMP2021]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/springer_book_explain.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-thirty-one" type="button">
              <div class="modal fade" id="modal-thirty-one" tabindex="-1" role="dialog" aria-labelledby="#modal-thirty-one-Title" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Explainable AI Within the Digital Transformation and Cyber Physical Systems: XAI Methods and Applications</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/springer_book_explain.jpg " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-thirty-one"></div><strong>Explainable AI Within the Digital Transformation and Cyber Physical Systems: XAI Methods and Applications</strong><br><em>Guidotti Riccardo, Monreale Anna, Pedreschi Dino, Giannotti Fosca</em> (2021) - Explainable AI Within the Digital Transformation and Cyber Physical Systems (pp. 9-31)
              <div class="collapse" id="collapse-thirty-one" aria-labelledby="heading-thirty-one" data-parent="#accordion-thirty-one">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">This book presents Explainable Artificial Intelligence (XAI), which aims at producing explainable models that enable human users to understand and appropriately trust the obtained results. The authors discuss the challenges involved in making machine learning-based AI explainable. Firstly, that the explanations must be adapted to different stakeholders (end-users, policy makers, industries, utilities etc.) with different levels of technical knowledge (managers, engineers, technicians, etc.) in different application domains. Secondly, that it is important to develop an evaluation framework and standards in order to measure the effectiveness of the provided explanations at the human and the technical levels. This book gathers research contributions aiming at the development and/or the use of XAI techniques in order to address the aforementioned challenges in different applications such as healthcare, finance, cybersecurity, and document summarization. It allows highlighting the benefits and requirements of using explainable models in different application domains in order to provide guidance to readers to select the most adapted models to their specified problem and conditions. Includes recent developments of the use of Explainable Artificial Intelligence (XAI) in order to address the challenges of digital transition and cyber-physical systems; Provides a textual scientific description of the use of XAI in order to address the challenges of digital transition and cyber-physical systems; Presents examples and case studies in order to increase transparency and understanding of the methodological concepts.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/978-3-030-76409-8" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪5</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PPB2021"></div>
            <div class="col-lg-1 text-right">
              <h4>42.</h4><small>[PPB2021]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/FairLens.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-forty-one" type="button">
              <div class="modal fade" id="modal-forty-one" tabindex="-1" role="dialog" aria-labelledby="#modal-forty-one-Title" aria-hidden="true">
                <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">FairLens: Auditing black-box clinical decision support systems</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/FairLens.png " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-forty-one"></div><strong>FairLens: Auditing black-box clinical decision support systems</strong><br><em>Panigutti Cecilia, Perotti Alan, Panisson André, Bajardi Paolo, Pedreschi Dino</em> (2021) - Information Processing & Management. In Journal of Information Processing and Management
              <div class="collapse" id="collapse-forty-one" aria-labelledby="heading-forty-one" data-parent="#accordion-forty-one">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Highlights: We present a pipeline to detect and explain potential fairness issues in Clinical DSS. We study and compare different multi-label classification disparity measures. We explore ICD9 bias in MIMIC-IV, an openly available ICU benchmark dataset</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1016/j.ipm.2021.102657" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PGG2019"></div>
            <div class="col-lg-1 text-right">
              <h4>48.</h4><small>[PGG2019]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-forty-seven"></div><strong>Meaningful Explanations of Black Box AI Decision Systems</strong><br><em>Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco</em> (2021) - Proceedings of the AAAI Conference on Artificial Intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence
              <div class="collapse" id="collapse-forty-seven" aria-labelledby="heading-forty-seven" data-parent="#accordion-forty-seven">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Black box AI systems for automated decision making, often based on machine learning over (big) data, map a user’s features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases inherited by the algorithms from human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We focus on the urgent open challenge of how to construct meaningful explanations of opaque AI/ML systems, introducing the local-toglobal framework for black box explanation, articulated along three lines: (i) the language for expressing explanations in terms of logic rules, with statistical and causal interpretation; (ii) the inference of local explanations for revealing the decision rationale for a specific case, by auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of many local explanations into simple global ones, with algorithms that optimize for quality and comprehensibility. We argue that the local-first approach opens the door to a wide variety of alternative solutions along different dimensions: a variety of data sources (relational, text, images, etc.), a variety of learning problems (multi-label classification, regression, scoring, ranking), a variety of languages for expressing meaningful explanations, a variety of means to audit a black box.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-seven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1609/aaai.v33i01.33019780" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PGM2019"></div>
            <div class="col-lg-1 text-right">
              <h4>51.</h4><small>[PGM2019]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Explaining Multi-label Black-Box Classifiers for Health Applications.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-fifty" type="button">
              <div class="modal fade" id="modal-fifty" tabindex="-1" role="dialog" aria-labelledby="#modal-fifty-Title" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Explaining Multi-label Black-Box Classifiers for Health Applications</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/Explaining Multi-label Black-Box Classifiers for Health Applications.png " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-fifty"></div><strong>Explaining Multi-label Black-Box Classifiers for Health Applications</strong><br><em>Panigutti Cecilia, Guidotti Riccardo, Monreale Anna, Pedreschi Dino</em> (2021) - Precision Health and Medicine. In International Workshop on Health Intelligence (pp. 97-110). Springer, Cham.
              <div class="collapse" id="collapse-fifty" aria-labelledby="heading-fifty" data-parent="#accordion-fifty">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Today the state-of-the-art performance in classification is achieved by the so-called “black boxes”, i.e. decision-making systems whose internal logic is obscure. Such models could revolutionize the health-care system, however their deployment in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of transparency. The typical classification problem in health-care requires a multi-label approach since the possible labels are not mutually exclusive, e.g. diagnoses. We propose MARLENA, a model-agnostic method which explains multi-label black box decisions. MARLENA explains an individual decision in three steps. First, it generates a synthetic neighborhood around the instance to be explained using a strategy suitable for multi-label decisions. It then learns a decision tree on such neighborhood and finally derives from it a decision rule that explains the black box decision. Our experiments show that MARLENA performs well in terms of mimicking the black box behavior while gaining at the same time a notable amount of interpretability through compact decision rules, i.e. rules with limited length.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-24409-5_9" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GMS2020"></div>
            <div class="col-lg-1 text-right">
              <h4>53.</h4><small>[GMS2020]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/ex_time_series_calssifier.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-fifty-two" type="button">
              <div class="modal fade" id="modal-fifty-two" tabindex="-1" role="dialog" aria-labelledby="#modal-fifty-two-Title" aria-hidden="true">
                <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Explaining Any Time Series Classifier</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/ex_time_series_calssifier.png " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-fifty-two"></div><strong>Explaining Any Time Series Classifier</strong><br><em>Guidotti Riccardo, Monreale Anna, Spinnato Francesco, Pedreschi Dino, Giannotti Fosca</em> (2020) - 2020 IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)
              <div class="collapse" id="collapse-fifty-two" aria-labelledby="heading-fifty-two" data-parent="#accordion-fifty-two">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">We present a method to explain the decisions of black box models for time series classification. The explanation consists of factual and counterfactual shapelet-based rules revealing the reasons for the classification, and of a set of exemplars and counter-exemplars highlighting similarities and differences with the time series under analysis. The proposed method first generates exemplar and counter-exemplar time series in the latent feature space and learns a local latent decision tree classifier. Then, it selects and decodes those respecting the decision rules explaining the decision. Finally, it learns on them a shapelet-tree that reveals the parts of the time series that must, and must not, be contained for getting the returned outcome from the black box. A wide experimentation shows that the proposed method provides faithful, meaningful and interpretable explanations.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/cogmi50398.2020.00029" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PPP2020"></div>
            <div class="col-lg-1 text-right">
              <h4>54.</h4><small>[PPP2020]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Doctor XAI.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-fifty-three" type="button">
              <div class="modal fade" id="modal-fifty-three" tabindex="-1" role="dialog" aria-labelledby="#modal-fifty-three-Title" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Doctor XAI: an ontology-based approach to black-box sequential data classification explanations</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/Doctor XAI.jpg " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-fifty-three"></div><strong>Doctor XAI: an ontology-based approach to black-box sequential data classification explanations</strong><br><em>Panigutti Cecilia, Perotti Alan, Pedreschi Dino</em> (2020) - FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. In FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency
              <div class="collapse" id="collapse-fifty-three" aria-labelledby="heading-fifty-three" data-parent="#accordion-fifty-three">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://dl.acm.org/doi/10.1145/3351095.3372855" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="RGG2020"></div>
            <div class="col-lg-1 text-right">
              <h4>55.</h4><small>[RGG2020]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-fifty-four"></div><strong>Opening the black box: a primer for anti-discrimination</strong><br><em>Ruggieri Salvatore, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Pedreschi Dino, Turini Franco</em> (2020). In ANNUARIO DI DIRITTO COMPARATO E DI STUDI LEGISLATIVI
              <div class="collapse" id="collapse-fifty-four" aria-labelledby="heading-fifty-four" data-parent="#accordion-fifty-four">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The pervasive adoption of Artificial Intelligence (AI) models in the modern information society, requires counterbalancing the growing decision power demanded to AI models with risk assessment methodologies. In this paper, we consider the risk of discriminatory decisions and review approaches for discovering discrimination and for designing fair AI models. We highlight the tight relations between discrimination discovery and explainable AI, with the latter being a more general approach for understanding the behavior of black boxes.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://hdl.handle.net/11568/1088440" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GPP2019"></div>
            <div class="col-lg-1 text-right">
              <h4>59.</h4><small>[GPP2019]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/mulino_pandemia.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-fifty-eight" type="button">
              <div class="modal fade" id="modal-fifty-eight" tabindex="-1" role="dialog" aria-labelledby="#modal-fifty-eight-Title" aria-hidden="true">
                <div class="modal-dialog modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">I.A. comprensibile per il supporto alle decisioni: doctor XAI</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/mulino_pandemia.jpg " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-fifty-eight"></div><strong>I.A. comprensibile per il supporto alle decisioni: doctor XAI</strong><br><em>Giannotti Fosca, Pedreschi Dino, Panigutti Cecilia</em> (2019) - Biopolitica, Pandemia e democrazia. Rule of law nella società digitale. In BIOPOLITICA, PANDEMIA E DEMOCRAZIA Rule of law nella società digitale
              <div class="collapse" id="collapse-fifty-eight" aria-labelledby="heading-fifty-eight" data-parent="#accordion-fifty-eight">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">La crisi sanitaria ha trasformato le relazioni tra Stato e cittadini, conducendo a limitazioni temporanee dei diritti fondamentali e facendo emergere conflitti tra le due dimensioni della salute, come diritto della persona e come diritto della comunità, e tra il diritto alla salute e le esigenze del sistema economico. Per far fronte all’emergenza, si è modificato il tradizionale equilibrio tra i poteri dello Stato, in una prospettiva in cui il tempo dell’emergenza sembra proiettarsi ancora a lungo sul futuro. La pandemia ha inoltre potenziato la centralità del digitale, dall’utilizzo di software di intelligenza artificiale per il tracciamento del contagio alla nuova connettività del lavoro remoto, passando per la telemedicina. Le nuove tecnologie svolgono un ruolo di prevenzione e controllo, ma pongono anche delicate questioni costituzionali: come tutelare la privacy individuale di fronte al Panopticon digitale? Come inquadrare lo statuto delle piattaforme digitali, veri e propri poteri tecnologici privati, all’interno dei nostri ordinamenti? La ricerca presentata in questo volume e nei due volumi collegati propone le riflessioni su questi temi di studiosi afferenti a una moltitudine di aree disciplinari: medici, giuristi, ingegneri, esperti di robotica e di IA analizzano gli effetti dell’emergenza sanitaria sulla tenuta del modello democratico occidentale, con l’obiettivo di aprire una riflessione sulle linee guida per la ricostruzione del Paese, oltre la pandemia. In particolare, questo terzo volume affronta gli aspetti legati all’impatto della tecnologia digitale e dell’IA sui processi, sulla scuola e sulla medicina, con una riflessione su temi quali l’organizzazione della giustizia, le responsabilità, le carenze organizzative degli enti.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifty-eight" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://www.mulino.it/isbn/9788815293060#" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GMP2019"></div>
            <div class="col-lg-1 text-right">
              <h4>60.</h4><small>[GMP2019]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-fifty-nine"></div><strong>The AI black box explanation problem</strong><br><em>Guidotti Riccardo, Monreale Anna, Pedreschi Dino</em> (2019) - ERCIM News, 116, 12-13. In ERCIM News, 116, 12-13
              <div class="collapse" id="collapse-fifty-nine" aria-labelledby="heading-fifty-nine" data-parent="#accordion-fifty-nine">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">nan</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://ercim-news.ercim.eu/images/stories/EN116/EN116-web.pdf#page=12" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪2▪3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PGG2018"></div>
            <div class="col-lg-1 text-right">
              <h4>61.</h4><small>[PGG2018]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-sixty"></div><strong>Open the Black Box Data-Driven Explanation of Black Box Decision Systems</strong><br><em>Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna , Pappalardo Luca , Ruggieri Salvatore , Turini Franco </em> (2018) - Arxive preprint
              <div class="collapse" id="collapse-sixty" aria-labelledby="heading-sixty" data-parent="#accordion-sixty">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-sixty" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/1806.09936" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- Footer-->
    <footer class="site-footer">
      <div class="footer-widgets">
        <div class="container">
          <div class="row gx-lg-5">
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-1">
                <h4 class="text-yellow mb-3">Principal Investigator</h4>
                <div class="text-white">
                  <p class="mb-0">Fosca Giannotti</p>
                  <p class="mb-0">Scuola Normale Superiore</p><br>
                  <p class="mb-0">Piazza dei Cavalieri, 7</p>
                  <p class="mb-0">56126 Pisa, Italy</p><br>
                  <p class="mb-0">Email: fosca.giannotti @ sns.it</p>
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-4">
                <div class="text-white">
                  <p>The XAI Project receives funding from the European Union's Horizon 2020 Excellent Science European Research Council (ERC) programme under grant agreement No. 834756</p>The views and opinions expressed in this website are the sole responsibility of the author and do not necessarily reflect the views of the European Commission.
                  <p></p><img class="mb-4" src="assets/images/logo-eu.jpg" alt="EU flag">
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-2">
                <ul>
                  <li><a href="index.html">Xai</a></li>
                  <li><a href="about.html">Project details</a></li>
                  <li><a href="research-lines.html">Research lines</a></li>
                  <li><a href="people.html">People</a></li>
                  <li><a href="resources.html">Resources</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="footer-bottom-area">
        <div class="container">
          <div class="row gx-lg-5 align-items-center">
            <div class="col-md-12"><span class="text-yellow strong">Webdesign: Daniele Fadda © 2023</span></div>
          </div>
        </div>
      </div>
    </footer>
    <!-- End Footer-->
    <!-- javascript files-->
    <!-- jquery-->
    <script src="assets/js/jquery.min.js"></script>
    <!-- lozad js-->
    <script src="assets/js/lozad.min.js"></script>
    <!-- Bootstrap js-->
    <script src="assets/js/bootstrap.bundle.min.js"></script>
    <!-- Aos js-->
    <script src="assets/js/aos.js"></script>
    <!-- Slick flickity js-->
    <script src="assets/js/flickity.pkgd.min.js"></script>
    <!-- Magnific popup js-->
    <script src="assets/js/jquery.magnific-popup.min.js"></script>
    <!-- Countdown js-->
    <script src="assets/js/jquery.countdown.js"></script>
    <!-- CountTo js-->
    <script src="assets/js/jquery.countTo.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/global.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/cookies.js"></script>
  </body>
</html>