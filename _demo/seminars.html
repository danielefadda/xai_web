<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- favicon-->
    <link rel="shortcut icon" href="assets/images/favicon.ico">
    <!-- Site Title-->
    <title>Xai - Website</title>
    <meta name="description" content="Science and technology for the eXplanation of AI decision making">
    <!-- Bootstrap CSS file-->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Flickity CSS file-->
    <link href="assets/css/flickity.min.css" rel="stylesheet">
    <!-- Main CSS file-->
    <link href="assets/css/style.css" rel="stylesheet">
    <!-- Fontawesome 5 CSS file-->
    <link href="assets/css/fontawesome-all.min.css" rel="stylesheet">
    <!-- Magnific Popup CSS-->
    <link href="assets/css/magnific-popup.css" rel="stylesheet">
    <!-- Google Fonts-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&amp;display=swap">
    <script type="text/javascript">
      // Matomo Code
      var _paq = window._paq = window._paq || [];
      /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
      _paq.push(['trackPageView']);
      _paq.push(['enableLinkTracking']);
      (function () {
      var u="//piwikdd.isti.cnr.it/";
      _paq.push(['setTrackerUrl', u+'piwik.php']);
      _paq.push(['setSiteId', '12']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
      })
      ();
    </script>
  </head>
  <body>
    <!-- Navbar-->
    <div class="site-header header-bottom-area">
      <nav class="navbar navbar-expand-lg navbar-light sticky">
        <div class="container"><a class="navbar-brand" href="index.html"><img class="site-logo" src="assets/images/logo/xai_logo_color.png" alt="Logo"></a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarNavDropdown">
            <ul class="navbar-nav mx-auto">
              <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
              <li class="nav-item"><a class="nav-link" href="about.html">Project details</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="research-lines.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research Lines</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="line_1.html">1. Local to global</a><a class="dropdown-item" href="line_2.html">2. Casual reasoning</a><a class="dropdown-item" href="line_3.html">3. Platform and XUI</a><a class="dropdown-item" href="line_4.html">4. Case studies</a><a class="dropdown-item" href="line_5.html">5. Ethics and legal</a></div>
              </li>
              <li class="nav-item"><a class="nav-link" href="people.html">People</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="resources.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications and Resources</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="resources.html#publications">Publications</a>
                  <!-- a.dropdown-item(href='reports.html') Reports--><a class="dropdown-item" href="resources.html#thesis">Thesis</a><a class="dropdown-item" href="dissemination.html">Dissemination tools</a>
                </div>
              </li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Seminars</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="seminars.html">All Seminars</a><a class="dropdown-item" href="dist-seminars.html">Distinguished seminars</a>
                  <!--a.dropdown-item(href='internal-events.html') Internal Events-->
                </div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
    <!-- End Navbar-->
    <!-- Cookies-->
    <div class="cookie-banner" style="display: none">
      <p class="mx-2">We use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. 
        <u><a href="privacy.html" link’="privacy.html">No, give me more info.</a></u>
      </p>
      <button class="close">&times;</button>
    </div>
    <!-- End Cookies-->
    <article class="entry">
      <div class="entry-content" id="seminars">
        <!-- head blog-->
        <div class="bg-yellow">
          <div class="container pt-lg-5 pb-lg-5 pt-md-7 pb-md-5 pt-5 pb-4">
            <div class="row">
              <div class="col-lg-6">
                <h1 class="my-3">Seminars</h1>
                <p class="lead">and tutorials, round tables, conferences...</p>
              </div>
            </div>
          </div>
        </div>
        <div class="container">
          <div class="row">
            <div class="row mt-5 justify-content-center" id="Nov-09">
              <div class="col-lg-2 text-right">
                <h4>Nov 09, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-zero"></div><strong>FIPER: a visual approach to explainability methods</strong><br><br> </span><strong>Presenter: </strong><span>Eleonora Cappuccio <br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-zero" aria-labelledby="heading-zero" data-parent="#accordion-zero">
                  <div class="bg-yellow">
                    <hr><span></span><br><p><span>This seminar will present FIPER, a visualization tool that combines explanations through rules and feature importance.</span></p><p><span>An initial overview of the importance of designing human-centered explanations will be given. Use cases will be highlighted, and the results of a preliminary user test will be presented. The main purpose of the seminar will be to show and discuss new developments of the tool and possible applications.</span></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-zero" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-26">
              <div class="col-lg-2 text-right">
                <h4>Oct 26, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-one"></div><strong>Transformer models: from model inspection to applications in technical documentation</strong><br><br> <strong>Presenter: </strong><span>Giovanni Puccetti <br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-one" aria-labelledby="heading-one" data-parent="#accordion-one">
                  <div class="bg-yellow">
                    <hr><p></span></p><p> </p><p><strong>When &amp; Where </strong><span>Thursday 26th of October, at 11:30 @ Officine Garibaldi. Stream details below.</span></p><p> </p><p><strong>Abstract </strong><span>Language Models, of all sizes, have improved at a fast pace during the last years. However, besides measures of performance on downstream tasks, it is hard to understand what degree of linguistic knowledge they have and even more difficult to understand their inner workings.</span></p><p>Through linguistic probing of language models such as Bert and Roberta, I investigate their ability to encode linguistic properties and find a link between this ability and the phenomenon of outliers, parameters within language models that show unexpected behaviours. These findings help understand some of the properties that are typical of the attention mechanism at the core of such models.</p><p>Outliers also have a strong impact on the downstream performance of language models, therefore I apply these models to Named Entity Recognition in patents and study how they perform in this setting.</p><p>Finally, I present a brief study on the fine-tuning of Large Language Models to Italian.</p><p>______</p><br><p>Microsoft Teams meeting</p><br><br><b><p><strong>Join on your computer, mobile app or room device</strong></p></b><p><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1698044242903?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a></p><br><br><p>Meeting ID: 374 418 319 658 </p><p> </p><p>Passcode: zMGdht </p><br><p><a href="https://www.microsoft.com/en-us/microsoft-teams/download-app"><u>Download Teams</u></a> | <a href="https://www.microsoft.com/microsoft-teams/join-a-meeting"><u>Join on the web</u></a></p><br><br><p><strong>Or call in (audio only)</strong></p><br><p><a><u>+39 02 3056 4191,,45663040#</u></a>   Italy, Milano</p><p>Phone Conference ID: 456 630 40# </p><br><p><a href="https://dialin.teams.microsoft.com/e80d62af-367c-4976-9596-61ef054e4984?id=45663040"><u>Find a local number</u></a> | <a href="https://dialin.teams.microsoft.com/usp/pstnconferencing"><u>Reset PIN</u></a></p><br><p><a href="https://aka.ms/JoinTeamsMeeting"><u>Learn More</u></a> | <a href="https://teams.microsoft.com/meetingOptions/?organizerId=729b4d16-0567-46a8-a742-d2ae1bf09a4a&tenantId=c7456b31-a220-47f5-be52-473828670aa1&threadId=19_9a85abed0d8544a8bc86e6f8f42e599d@thread.tacv2&messageId=1698044242903&language=en-US"><u>Meeting options</u></a></p><br> <br> <br><p>______<br></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-one" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-19">
              <div class="col-lg-2 text-right">
                <h4>Oct 19, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-two"></div><strong>Welcome seminar by Marzio Di Vece</strong><br><br> <strong>Presenter: </strong><span>Marzio Di Vece <br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-two" aria-labelledby="heading-two" data-parent="#accordion-two">
                  <div class="bg-yellow">
                    <hr><p>Welcome seminar by new postdoc @SNS, Marzio Di Vece, who will tell us about his past and current research interests.<span> </span></p><p><br></p><p><strong>When &amp; Where</strong></p><p>October 19th, 11:30am @ Officine Garibaldi.</p><p>Stream details below.</p><p> </p><p></span></p><br><p>____</p><p>Microsoft Teams meeting</p><br><br><b><p><strong>Join on your computer, mobile app or room device</strong></p></b><p><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1697097711602?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a></p><br><br><p>Meeting ID: 372 902 472 996 </p><p> </p><p>Passcode: igJK69 </p><br><p><br></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-two" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-12">
              <div class="col-lg-2 text-right">
                <h4>Oct 12, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-three"></div><strong>ReasonX: Declarative Reasoning on Explanations Using Constraint Logic Programming</strong><br><br><strong>Presenter:</strong> Laura State <br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-three" aria-labelledby="heading-three" data-parent="#accordion-three">
                  <div class="bg-yellow">
                    <hr><p><i>In this talk, Laura will present us her work on ReasonX, a CLP-based tool for interactive reasoning-based explainability.</i></p><p><i> </i></p><p></p><p><strong>When &amp; Where.</strong> Thursday 12th, 11:30 @ Officine Garibaldi. Stream details below.</p><p><strong>Abstract.</strong> Explaining opaque Machine Learning (ML) models is an increasingly relevant problem. Current explanation in AI (XAI) methods suffer several shortcomings, among others an insufficient incorporation of background knowledge, and a lack of abstraction and interactivity with the user. We propose REASONX, an explanation method based on Constraint Logic Programming (CLP). REASONX can provide declarative, interactive explanations for decision trees, which can be the ML models under analysis or global/local surrogate models of any black-box model. Users can express background or common sense knowledge using linear constraints and MILP optimization over features of factual and contrastive instances, and interact with the answer constraints at different levels of abstraction through constraint projection. We present here the architecture of REASONX, which consists of a Python layer, closer to the user, and a CLP layer. REASONX's core execution engine is a Prolog meta-program with declarative semantics in terms of logic theories.</p><br><p>____</p><p>Microsoft Teams meeting</p><br><br><b><p><strong>Join on your computer, mobile app or room device</strong></p></b><p><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1696432157078?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a></p><br><br><p>Meeting ID: 341 121 035 466 </p><p> </p><p>Passcode: BHbWct </p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-three" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-05">
              <div class="col-lg-2 text-right">
                <h4>Oct 05, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-four"></div><strong>Welcome seminar by Antonio Mastropietro</strong><br><br><b>Presenter:</b>Antonio Mastropietro <br><em> Department of Computer Science, Piano Secondo, Largo Bruno Pontecorvo, 3, 56127 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-four" aria-labelledby="heading-four" data-parent="#accordion-four">
                  <div class="bg-yellow">
                    <hr><br><br><i>Welcome seminar by new postdoc@UniPI, Antonio Mastropietro, who will tell us about his past and current research interests.</i><br><br><b>When &amp; Where </b>October 5th, 11:30am. Sala Polifunzionale @ CS department.<br>Stream details below.<br><br><br><br><br><p>_____</p><br><p>Microsoft Teams meeting</p><br><br><b><p><strong>Join on your computer, mobile app or room device</strong></p></b><p><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1693224977005?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a></p><br><br><p>Meeting ID: 361 846 547 698 </p><p> </p><p>Passcode: wyaqvW </p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-four" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-04">
              <div class="col-lg-2 text-right">
                <h4>Oct 04, 2023</h4><small>h. 9:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-five"></div><strong>Welcome seminar by Daphne Lenders</strong><br><br><strong>Presenter: </strong><span>Daphne Lenders<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-five" aria-labelledby="heading-five" data-parent="#accordion-five">
                  <div class="bg-yellow">
                    <hr><p> </span></p><p>Welcome seminar by visiting PhD student, Daphne Lenders, who will tell us about her past and current research interests.</p><p> <span> </span></p><p><strong>When &amp; Where</strong></p><p>October 4th, 9:00am @ Officine Garibaldi.</p><p>Stream details below.</p><p><br></p><br><p>____</p><p>Microsoft Teams meeting</p><br><br><b><p><strong>Join on your computer, mobile app or room device</strong></p></b><p><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1694613663630?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-five" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Sep-27">
              <div class="col-lg-2 text-right">
                <h4>Sep 27, 2023</h4><small>h. 11:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-six"></div><strong>Welcome seminar by Margherita Lalli</strong><br><br><b>Presenter: </b>Margherita Lalli<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-six" aria-labelledby="heading-six" data-parent="#accordion-six">
                  <div class="bg-yellow">
                    <hr><br><br>Welcome seminar by new postdoc@SNS, Margherita Lalli, who will tell us about her past and current research interests.<br><br><b>When &amp; Where: </b>September 27th, 11am @ Officine Garibaldi.
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-six" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Sep-27">
              <div class="col-lg-2 text-right">
                <h4>Sep 27, 2023</h4><small>h. 10:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-seven"></div><strong>Introduction to LLMs</strong><br><br><b>Presenter: </b>Gizem Gezici<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-seven" aria-labelledby="heading-seven" data-parent="#accordion-seven">
                  <div class="bg-yellow">
                    <hr><br><br><br><b>Abstract:</b> Nowadays, Large Language Models (LLMs) are ubiquitous. In this talk, we will discuss LLMs, which have recently been the driving force behind a number of advances in artificial intelligence. This will cover some of the most well-known LLMs now available as well as the basic concepts underpinning LLMs, their general architecture, and some of their key features.<br><br><br><b>When &amp; Where:</b> 10a.m. on 27/9 @ Officine Garibaldi
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-seven" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Sep-15">
              <div class="col-lg-2 text-right">
                <h4>Sep 15, 2023</h4><small>h. 15:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-eight"></div><strong>AI, Meet Human: a Survey on Hybrid Decision-Making Systems</strong><br><br><b>Presenters: </b>Clara Punzi, Roberto Pellungrini, Mattia Setzu<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-eight" aria-labelledby="heading-eight" data-parent="#accordion-eight">
                  <div class="bg-yellow">
                    <hr><br><br><br><br><b>When &amp; Where: </b>September 15th, 15:00 @ Officine Garibaldi. Stream details below.<br><br><b>Abstract: </b>A growing body of interdisciplinary literature indicates that human decision-making processes can be enhanced by Artificial Intelligence (AI). Nevertheless, the use of AI in critical domains has also raised significant concerns regarding its final users, those affected by the undertaken decisions, and the broader society. Consequently, recent studies are shifting their focus towards the development of human-centered frameworks that facilitate a synergistic human-machine collaboration while upholding ethical and legal standards. In this work, we present a taxonomy for hybrid decision-making systems to classify systems according to the type of interaction that occurs between human and artificial intelligence. Furthermore, we identify gaps in the current body of literature and suggest potential directions for future research.
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-eight" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Sep-15">
              <div class="col-lg-2 text-right">
                <h4>Sep 15, 2023</h4><small>h. 14:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-nine"></div><strong>AUC-based Selective Classification</strong><br><br><b>Presenter:</b> <span>Andrea Pugnana<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-nine" aria-labelledby="heading-nine" data-parent="#accordion-nine">
                  <div class="bg-yellow">
                    <hr><p></span></p><p><br></p><p><b>Title: </b><span>AUC-based Selective Classification</span></p><p> </p><p><b>When &amp; where: </b><span>September 15th at 14:30 @Officine Garibaldi. Stream details below.</span></p><p> </p><p>Abstract: <span>Selective classification (or classification with a reject option) pairs a classifier with a selection function to determine whether or not a  prediction should be accepted. This framework trades off coverage (probability of accepting a prediction) with predictive performance, typically measured by distributive loss functions.</span></p><p>In many application scenarios, such as credit scoring, performance is instead measured by ranking metrics, such as the Area Under the ROC Curve (AUC). We propose a model-agnostic approach to associate a selection function to a given probabilistic binary classifier. The approach is specifically targeted at optimizing the AUC. We provide both theoretical justifications and a novel algorithm, called AUCROSS, to achieve such a goal. </p><p>Experiments show that our method succeeds in trading-off coverage for AUC, improving over existing selective classification methods targeted at optimizing accuracy.</p><p> </p><p> </p><p> </p><p> ____</p><br><p>Microsoft Teams meeting</p><br><br><b><p><strong>Join on your computer, mobile app or room device</strong></p></b><p><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1693918304957?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a></p><br><br><p>Meeting ID: 371 067 477 671 </p><p> </p><p>Passcode: UC8G3W </p><br><p><a href="https://www.microsoft.com/en-us/microsoft-teams/download-app"><u>Download Teams</u></a> | <a href="https://www.microsoft.com/microsoft-teams/join-a-meeting"><u>Join on the web</u></a></p><br><br><p><strong>Or call in (audio only)</strong></p><br><p><a><u>+39 02 3056 4191,,15636167#</u></a>   Italy, Milano</p><p>Phone Conference ID: 156 361 67# </p><br><p><a href="https://dialin.teams.microsoft.com/e80d62af-367c-4976-9596-61ef054e4984?id=15636167"><u>Find a local number</u></a> | <a href="https://dialin.teams.microsoft.com/usp/pstnconferencing"><u>Reset PIN</u></a></p><br><p><a href="https://aka.ms/JoinTeamsMeeting"><u>Learn More</u></a> | <a href="https://teams.microsoft.com/meetingOptions/?organizerId=729b4d16-0567-46a8-a742-d2ae1bf09a4a&tenantId=c7456b31-a220-47f5-be52-473828670aa1&threadId=19_9a85abed0d8544a8bc86e6f8f42e599d@thread.tacv2&messageId=1693918304957&language=en-US"><u>Meeting options</u></a></p><br> <br> <br><p>________________________________________________________________________________</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-nine" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Jun-07">
              <div class="col-lg-2 text-right">
                <h4>Jun 07, 2023</h4><small>h. 14:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-ten"></div><strong>Explainable and Interactive Robot Learning Systems</strong><br><br><b>Presenter:</b> Professor Dr. Matthew Gombolay<br><em> Online (see description)</em>
                <div class="collapse" id="collapse-ten" aria-labelledby="heading-ten" data-parent="#accordion-ten">
                  <div class="bg-yellow">
                    <hr><br><br><br> <p><i>Talk hosted by the International AI Doctoral Academy (AIDA). Talk webpage: <a href="https://www.i-aida.org/events/explainable-and-interactive-robot-learning-systems/">https://www.i-aida.org/events/explainable-and-interactive-robot-learning-systems/</a></i></p><p> </p><p><strong>Title: </strong><span>Explainable and Interactive Robot Learning Systems</span></p><p> </p><p><strong>When &amp; Where: </strong><span>Junte 7th, 2pm. Online at </span><a href="https://unitn.zoom.us/j/82021320646">https://unitn.zoom.us/j/82021320646</a><span> with ID </span><code>820 2132 0646</code><span> and passcode: </span><code>564616</code><span>.</span></p><p> </p><p><strong>Abstract: </strong><span>New advances in robotics and autonomy offer a promise of revitalizing final assembly manufacturing, assisting in personalized at-home healthcare, and even scaling the power of earth-bound scientists for robotic space exploration. Yet, in real-world applications, autonomy is often run in the O-F-F mode because researchers fail to understand the human end-user’s goals, needs, and wishes. In this talk, I will share exciting research we are conducting at the nexus of human factors engineering and cognitive robotics to inform the design of explainable and interactive machine learning systems. In my talk, I will focus on our recent work on enabling machines to learn human-interpretable decision-making and control policies from interaction with their environments. I will show that our novel explainable Artificial Intelligence (XAI) methods not only improves subjective measures of end-user interaction but also improves objective performance in human-machine teaming.</span></p><p> </p><p><strong>Bio: </strong><span>Dr. Matthew Gombolay is an Assistant Professor of Interactive Computing at Georgia Tech (GT) and was named Anne and Alan Taetle Early-Career Assistant Professor in 2018. He received a B.S. Mechanical Engineering from Johns Hopkins University in 2011, an S.M. AeroAstro from MIT in 2013, and a Ph.D. Autonomous Systems from MIT in 2017. Dr. Gombolay was a technical staff at MIT Lincoln Laboratory from 2017-2018, earning an R&amp;D 100 Award for transitioning research to the Navy. At GT, Dr. Gombolay founded the Cognitive Optimization and Relational (CORE) Robotics Lab, which places the power of robots in the hands of diverse, non-expert end-users by developing new computational methods and human factors insights. Dr. Gombolay’s research lab has received numerous best paper awards and nominations, including most recently at the 2022 ACM/IEEE International Conference on Human-Robot Interaction. Dr. Gombolay is a DARPA Riser, NASA Early Career Fellow, and an Associate Editor of Autonomous Robots and ACM Transactions on Human-Robot Interaction.</span></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-ten" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Jun-06">
              <div class="col-lg-2 text-right">
                <h4>Jun 06, 2023</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-eleven"></div><strong>Learning and inference with hybrid models</strong><br><br><b>Presenter: </b>Luciano Serafini<br><em> Online</em>
                <div class="collapse" id="collapse-eleven" aria-labelledby="heading-eleven" data-parent="#accordion-eleven">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><b>Title: </b>Learning and inference with hybrid models.<br><b><br></b><br><b>When &amp; Where: </b>June 6th, 4pm. Online only <a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">here</a>.<br><b><br></b><br><b>Topics:</b> Neuro-symbolic, Knowledge Injection, Logic, Knowledge Graphs.<br><br><b>Abstract.</b> Hybrid models are the result of the integration of two types of models, i.e., deep neural networks and logical models. The former are capable to process high throughput data in continuous spaces; the latter class contains models which are capable to express knowledge about the abstract properties and relations between the observed data. There is no unique way to integrate these two classes of models. A first method exploits logical knowledge to impose additional supervision during the training of a (set of) neural models that predicts abstract properties and relations. This idea is implemented in Logic Tensor Networks (LTN). A second method consists in using the background knowledge to correct/revise the prediction done by a (set of) neural models. This method has been implemented Iterative Local Refinement (ILR). A third method consists in defining a hybrid model as the composition of a (set of) deep learning model(s), which abstract the continuous perceptions in a finite and abstract representation--the symbols--and a discrete model that computes a finite function on the set of abstract symbols. This method is described in Deep Symbolic Learning. In the seminar, I will briefly introduce the hybrid model described above and the mechanism used for training and inference.<br><br><b>Bio: </b>Luciano Serafini is a researcher in the area of Artificial Intelligence. He currently coordinates the Data and Knowledge Management Research Unit at Fondazione Bruno Kessler. He graduated in information science with a thesis on logic for knowledge representation. Since 1990 he joined Fondazione Bruno Kessler (before IRST) as a researcher in the area of logic for knowledge representation and reasoning. He proposes a formalism called Multi-Context (MC) System for the representation of modular interconnected context-dependent knowledge. MC Systems has a great influence on the semantic web area and in the area of information integration. Between 2000-2013 he worked on semantic matching heterogeneous schemas and he was the first who suggested to encode this problem in propositional satisfiability. After 2010, he started his research about integrating machine learning and logical reasoning, and in 2016 he invented Logic Tensor Network, one of the first neuro-symbolic architectures. In the last years, he also has been interested in integrating learning, acting, and planning. PropHe has been a EurAI fellow since 2020 and teaches regular courses on Knowledge representation and learning at the University of Padova.
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-eleven" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Jun-01">
              <div class="col-lg-2 text-right">
                <h4>Jun 01, 2023</h4><small>h. 14:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twelve"></div><strong>Domain Adaptive Decision Trees: Implications for Accuracy and Fairness</strong><br><br><b>Presenter:</b> Jose Manuel Alvarez<br><em> Sala Polifunzionale @ Department of Computer Science, Piano Secondo, Largo Bruno Pontecorvo, 3, 56127 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-twelve" aria-labelledby="heading-twelve" data-parent="#accordion-twelve">
                  <div class="bg-yellow">
                    <hr><p>Dear KDDers, in this talk Jose will present us his <a href="https://facctconference.org/2023/">FAccT 2023</a> papere on Domain Adaptive Decision Trees, feel free to join!</p><p> </p><p><strong>Title.</strong> Domain Adaptive Decision Trees: Implications for Accuracy and Fairness</p><p><strong>Abstract.</strong><span> In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.</span><br></p><p> <strong>When &amp; Where.</strong><span> June 1st, 2pm. Sala Polifunzionale @ Department of Computer Science. Stream available </span><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1685213520877?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>here</u></a><span>.</span></p><p><b>Topics.</b><span> Decision Trees, Fairness, Explainability</span><br></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twelve" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-26">
              <div class="col-lg-2 text-right">
                <h4>May 26, 2023</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirteen"></div><strong>Shapley value in a nutshell</strong><br><br><b>Presenter: </b>Giang Pham<br><em> Sala Riunioni Ovest, Dipartimento di Informatica, Piano Secondo, Largo Bruno Pontecorvo, 3, 56127 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-thirteen" aria-labelledby="heading-thirteen" data-parent="#accordion-thirteen">
                  <div class="bg-yellow">
                    <hr><br><br><i>Seminar of the Pesaresi Seminar series, where 1st year PhD students present their research area.<br></i><b><br></b><br><b>TItle</b><br>Gene regulatory network: current approaches and challenges<br><br><br><br><b>Abstract:</b><br>Since John von Neumann and Oskar Morgenstern published the book <i>Theory of Games and Economic Behavior</i> in 1944, modern game theory has been developed and applied in many fields such as economics, social science and also computer science. In which, coalition game is a family of games that draws a lot of attention. The theory of coalition game provides a high-level approach to model many problems. In this seminar, we will briefly review coalition game and the solution proposed by Shapley for this game together with its application in several current issues such as Machine Learning, Computational Biology. <br><br><b>When &amp; Where</b><br>May 26th at 4pm, <span>Sala Riunioni Ovest @ Department of Computer Science.</span><br>Also available on stream at <a href="https://meet.google.com/xgq-arig-zzs"><u>https://meet.google.com/xgq-arig-zzs</u></a>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirteen" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-24">
              <div class="col-lg-2 text-right">
                <h4>May 24, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-fourteen"></div><strong>Trust and Reliance in AI: Designing for appropriate Human-AI collaboration</strong><br><br><b>Presenter: </b><span>Andrea Beretta</span><br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-fourteen" aria-labelledby="heading-fourteen" data-parent="#accordion-fourteen">
                  <div class="bg-yellow">
                    <hr><p></p><p><strong><br></strong></p><p><strong>Title: </strong><span>Trust and Reliance in AI: Designing for appropriate Human-AI collaboration</span></p><p> </p><p><strong>Abstract: </strong><span>The rapid advancements in artificial intelligence (AI) have emphasized the importance of making AI systems understandable to human users. The field of explainable AI (XAI) aims to enhance human understanding and promote collaboration between humans and AI by providing users with essential information about AI and its decision-making processes. At the same time, the literature on human factors has long focused on crucial factors that influence human performance, including human trust in autonomous systems. In this presentation, drawing from the literature on human-computer interaction, I delve into three significant considerations related to trust in the design of explainable systems: the foundation of trust, trust calibration, and trust specificity. Furthermore, I explore relevant literature on under-trust (algorithm aversion) and over-trust (overreliance) in technological artifacts. I will discuss common trust calibration interventions for XAI system designers and present potential paths for future research in this domain.</span></p><p><strong><br></strong></p><p><strong>When &amp; Where: </strong><span>May 24th, 11:30am. </span><span>Officine Garibaldi, Aula Master. Stream available </span><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1684759869717?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>here</u></a>.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fourteen" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-23">
              <div class="col-lg-2 text-right">
                <h4>May 23, 2023</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-fifteen"></div><strong>On the Synergy between Robustness and Explainability</strong><br><br><b>Presenter: </b>Matthew Wicker<br><em> Online</em>
                <div class="collapse" id="collapse-fifteen" aria-labelledby="heading-fifteen" data-parent="#accordion-fifteen">
                  <div class="bg-yellow">
                    <hr><br><br><b>Title: </b>On the Synergy between Robustness and Explainability<br><br><b>When &amp; Where:</b> Tuesday 23rd at 4pm. Online only, link <a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">here</a>.<br><br><b>Abstract: </b>With machine learning models finding wider deployment in recent months, trustworthy machine learning has become an increasingly important topic. In this talk, we will explore the connections between two core trustworthiness properties: explainability and (adversarial) robustness. In particular, this talk will discuss recent advances in adapting optimization techniques originally designed for adversarial robustness to suit the goals of explainability. This talk will cover recent certification techniques for gradient-based explanations, i.e., how to formally prove the robustness of an explanation, as well as how such techniques can be combined with human feedback to counter spurious correlation. Finally, the talk will conclude with open discussion of future challenges at the intersection of robustness and explainability.<br><br><br><b>Bio:</b> Matthew Wicker is currently a postdoctoral research associate at The Alan Turing Institute where he works on developing formal methods for guaranteeing trustworthiness in machine learning. Prior to this, Matthew was a Google DeepMind scholar at the University of Oxford where he completed his PhD in computer science in 2021.
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-fifteen" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-17">
              <div class="col-lg-2 text-right">
                <h4>May 17, 2023</h4><small>h. 14:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-sixteen"></div><strong>Multi-layered XAI System for Satellite Imagery Analysis</strong><br><br><b>Presenter. </b>Chan-Hyun Youn<br><em> Sala Seminari Ovest @ Department of Computer Science, Piano Secondo, Largo Bruno Pontecorvo, 3, 56127 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-sixteen" aria-labelledby="heading-sixteen" data-parent="#accordion-sixteen">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><b>Title &amp; Abstract. </b><span>Development of microsatellite system and high-resolution sensors has led to a dramatic increase in the scale of RAW image data, demanding significant additional efforts for real-time data analysis. Neural network-based AI models can address the requirements for satellite imagery analysis, however, there are still required for new approaches to improve the explainability of the smoothness of AI training and inference results.</span><span>This seminar introduces the technology of multi-layer-based eXplainable Artificial Intelligence (XAI) systems for satellite image analysis. In particular, it introduces a Feature Pyramid Network-based multi-layer integrated object detection model structure and an attention-based technique that visualizes the key input attributes of prediction results to handle variable-sized satellite images with varying scales. Especially, I will talk about the technological trends in multi-layer visual explanations that compensate for the loss of object boundary information caused by the structural characteristics of deep neural networks.  And for practical system, I will present a prototyped satellite-ground collaborative computing system with onboard DL accelerating systems.</span><b><br></b><br><b>When &amp; Where. </b>Sala Seminari Ovest @ Department of Computer Science, 2pm. In presence-only seminar.
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-sixteen" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-17">
              <div class="col-lg-2 text-right">
                <h4>May 17, 2023</h4><small>h. 10:15</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-seventeen"></div><strong>Welcome seminar by Simone Poggiesi</strong><br><br><b>Presenter: </b>Simone Piaggesi<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-seventeen" aria-labelledby="heading-seventeen" data-parent="#accordion-seventeen">
                  <div class="bg-yellow">
                    <hr><p></p><p><br></p><p>Welcome seminar by the new research fellow Simone Piaggesi.</p><p> </p><p><strong>When &amp; Where </strong><span>Wednesday 17 at 9:30am @ </span><a href="https://goo.gl/maps/QzgqkNZs3vtLDVNL6">Officine Garibaldi</a><span>, Aula Master.</span></p><p>Stream details below.</p><p><br></p><p><br></p><p>____ </p><p>Microsoft Teams meeting</p><b><p><strong>Join on your computer, mobile app or room device</strong></p></b><p><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1684145208358?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-seventeen" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-10">
              <div class="col-lg-2 text-right">
                <h4>May 10, 2023</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-eighteen"></div><strong>Argumentation: a foundation for Human-centric Cognitive AI</strong><br><br><b>Presenter:</b> Antonis C. Kakas<br><em> Dipartimento di Informatica - seminari EST, Piano Secondo, Largo Bruno Pontecorvo, 3, 56127 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-eighteen" aria-labelledby="heading-eighteen" data-parent="#accordion-eighteen">
                  <div class="bg-yellow">
                    <hr><p><br><br><b>Abstract: </b><span>The talk will explore the possible role of argumentation as a logical foundation of (human-centric) AI. It will present how Argumentation Logic forms a universal notion of reasoning spanning from informal common-sense logic to the formal logical systems of classical logic. It will then connect this to a general and flexible software methodology for building real-life AI applications and showcase this methodology in a variety of different application domains, such as medical decision support, personal assistants, and data access risk management.</span><br></p><p><span><br></span></p><p><span><b>Where &amp; When: </b></span><span>Sala Seminari Est @ </span><span>Department of Computer Science, at 16:00</span></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-eighteen" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-10">
              <div class="col-lg-2 text-right">
                <h4>May 10, 2023</h4><small>h. 11:15</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-nineteen"></div><strong>Manipulation of Latent Space for Counterfactual Explanation and Fairness</strong><br><br><b>Presenter:</b> Xuan Zhao<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italia</em>
                <div class="collapse" id="collapse-nineteen" aria-labelledby="heading-nineteen" data-parent="#accordion-nineteen">
                  <div class="bg-yellow">
                    <hr><br><p><i><br></i></p><p><i>Introductory presentation by Xuan, a visiting Ph.D. student from Germany! Xuan will tell us of her work on counterfactual explanations and AI fairness, you can find the details below.</i></p><p><strong>Title: </strong><span>Manipulation of Latent Space for Counterfactual Explanation and Fairness</span><br></p> <p><strong>Abstract: </strong><span>With the help of neural nets (NN), we could manipulate latent space to achieve different tasks. In my research, perturbation in the latent space with help of Gaussian mixed model for counterfactual explanation first got my attention, then I move to pertubation in latent space to achieve counterfactual explanation for image dataset on regression task. Later, I came up with an idea to reweigh samples in latent space to achieve fairness (demographic parity) for the situation where the minor group just has less samples in the dataset. In the most recent work, I move to reweigting for causal fairness with the help of NN and adversarial design.</span></p><p><strong>Bio: </strong><span>Xuan Zhao is a third year PhD student at SCHUFA holding AG in Wiesbaden, Germany. She is in a Marie Curie training network NOBIAS. She is very interested in the research field of Explainability and Fairness. Her research focuses on counterfactual explanation and reweighting to achieve different fairness criteria. Before starting her PhD studies, she completed a master's degree in data science and analytics in UCC, Ireland.</span></p><p> </p><p><strong>When &amp; Where: </strong><span>Wednesday 10/5, at 11:15</span></p><p>Live at <a href="https://goo.gl/maps/1BmCDJfcCKtH2t2p6">Officine Garibaldi</a> (Aula Master). Stream details below.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-nineteen" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-03">
              <div class="col-lg-2 text-right">
                <h4>May 03, 2023</h4><small>h. 9:15</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty"></div><strong>Generating realistic and robust counterfactuals</strong><br><br><b>Presenter: </b>Victor Guyomard <br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-twenty" aria-labelledby="heading-twenty" data-parent="#accordion-twenty">
                  <div class="bg-yellow">
                    <hr><br><br><b>Title: </b>Generating realistic and robust counterfactuals<br><b><br></b><br><b>Abstract: </b>The objective of my thesis is to explain individual decisions made by AI with a specific focus on counterfactual explanation. In this presentation, I will introduce two major contributions: First, the development of VCnet, a self-explaining model for counterfactual generation. VCnet combines a predictor and a counterfactual generator that are jointly trained to generate counterfactuals that are close to the distribution of the predicted class. The proposed method employs a variational autoencoder that is conditioned to the output of the predictor to generate realistic counterfactuals. VCnet is capable of generating predictions as well as counterfactual explanations without having to solve another minimization problem. Second, the proposal of a new framework, CROCO, for generating robust counterfactual explanations regarding counterfactual input changes. This form of robustness is particularly challenging as it involves a trade-off between the robustness of the counterfactual and the proximity with the example to explain. CostRC generates robust counterfactuals while effectively managing this trade-off and guarantees the user a minimal robustness. Empirical evaluations on tabular datasets confirm the relevance and effectiveness of the proposed approach.<b><br></b><br><b><br></b><br><b>Bio: </b>Victor Guyomard is a third year PhD student at Orange and Irisa lab in Brittany, France. He is passionate about the field of Explainable Artificial Intelligence (XAI), and the title of his thesis is "explain an invididual decision made by an AI". Specifically, his research delves into counterfactual explanations particularly the generation of realistic and robust counterfactual explanations. Before starting his PhD studies, he completed a master's degree in data science and engineering, which provided him with a strong foundation in machine learning and statistical analysis.<br><br><b>When &amp; Where: </b>9:15 @ Officine Garibaldi - In presence only
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Mar-15">
              <div class="col-lg-2 text-right">
                <h4>Mar 15, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-one"></div><strong>Explanation visualization at scale: progress on decision rules and feature importance</strong><br><br><b>Presenter:</b> Eleonora Cappuccio<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-twenty-one" aria-labelledby="heading-twenty-one" data-parent="#accordion-twenty-one">
                  <div class="bg-yellow">
                    <hr><p><br></p><p><strong>Title: </strong><span>Explanation visualization at scale: progress on decision rules and feature importance</span></p><p><strong></strong><br></p><p><strong>Abstract: </strong><span>Starting from a more user-centered approach to XAI our latest work proposes a new visualization method for the combination of rules and features importance for datasets with a large number of features. We will be showing an interface through which the users can interact with these visualizations.</span></p><p><br></p>____<br><br>Microsoft Teams meeting<br><br><b>Join on your computer, mobile app or room device</b><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1678720786057?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a><br><br>Meeting ID: 372 276 263 699<br>Passcode: Q9Vnq2<br><a href="https://www.microsoft.com/en-us/microsoft-teams/download-app"><u>Download Teams</u></a> | <a href="https://www.microsoft.com/microsoft-teams/join-a-meeting"><u>Join on the web</u></a><br><br><b>Or call in (audio only)</b><br><a><u>+39 02 3056 4191,,648642497#</u></a>   Italy, MilanoPhone Conference ID: 648 642 497#<br><a href="https://dialin.teams.microsoft.com/e80d62af-367c-4976-9596-61ef054e4984?id=648642497"><u>Find a local number</u></a> | <a href="https://dialin.teams.microsoft.com/usp/pstnconferencing"><u>Reset PIN</u></a><br><a href="https://aka.ms/JoinTeamsMeeting"><u>Learn More</u></a> | <a href="https://teams.microsoft.com/meetingOptions/?organizerId=729b4d16-0567-46a8-a742-d2ae1bf09a4a&tenantId=c7456b31-a220-47f5-be52-473828670aa1&threadId=19_9a85abed0d8544a8bc86e6f8f42e599d@thread.tacv2&messageId=1678720786057&language=en-US"><u>Meeting options</u></a><br>________________________________________________________________________________
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-one" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Mar-01">
              <div class="col-lg-2 text-right">
                <h4>Mar 01, 2023</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-two"></div><strong>Querying Neural Networks: The BERT Case</strong><br><br><b>Presenter:</b> Ana Ozaki<br><em> Online on Teams</em>
                <div class="collapse" id="collapse-twenty-two" aria-labelledby="heading-twenty-two" data-parent="#accordion-twenty-two">
                  <div class="bg-yellow">
                    <hr><br><br><br>Event hosted by the XAI seminar series@Imperial.<br><br><br><b>Title: </b>Querying Neural Networks: The BERT Case<br><br><b>Abstract: </b>We investigate an approach for extracting knowledge from trained neural networks based on Angluin’s exact learning model with membership and equivalence queries to an oracle. In this approach, the oracle is a trained neural network. We consider Angluin’s classical algorithm for exact learning Horn theories and study the necessary changes to make it applicable to learn from neural networks. In particular, we have to consider that trained neural networks may not behave as Horn oracles, meaning that their underlying target theory may not be Horn. We propose a new algorithm that aims at extracting a Horn-approximation of the target theory and that is guaranteed to terminate in exponential time (in the worst case) and in polynomial time if the target is Horn. To showcase the applicability of the approach, we perform experiments on BERT based language models and extract rules that expose biases in these models.<br><br><br><br><b>Bio: </b>Ana Ozaki is an associate professor at the University of Bergen, Norway. Her research area is Artificial Intelligence (AI). She is an AI researcher in the field of knowledge representation and reasoning and in learning theory.<br>Ozaki is interested in the formalisation of the learning phenomenon so that questions involving learnability, complexity, and reducibility can be systematically investigated and understood. Her research focuses on learning logical theories formulated in description logic and related formalisms for knowledge representation.<br>She is a member of the editorial boards of the Journal of Machine Learning Research and the Journal of Web Semantics. She has recently worked as Program Committee Chair for the 27th International Symposium on Temporal Representation and Reasoning.<br>Ozaki is fascinated by learning and reasoning processes and how they interact. There is a mode of learning which is actually how we learn very often but that is not the mode of learning we refer to when we talk about machine learning. We learn very often by posing queries (questions). Our intuition is that by obtaining answers to our questions from a teacher we can learn faster and better (more accurately) than by randomly selecting learning material. In learning theory, one can show that learners can efficiently exactly identify an unknown target concept formulated e.g. as a deterministic finite automaton, a decision tree, or a set of Horn rules if they can pose queries to a teacher.<br>Ozaki is currently working with her team on strategies to learn Horn rules from neural networks by posing them queries. She believes that exciting and recent advances in machine learning need to be complemented with theoretical development so that systems can provide formal guarantees of classification results and become trustable.<br><br><br><br><br>___<br>Microsoft Teams meeting<br><br>Join on your computer or mobile app<br><br><br>Click here to join the meeting&lt;<a href="https://deref-gmx.net/mail/client/_gKBGVbcTE4/dereferrer/?redirectUrl=https%3A%2F%2Fteams.microsoft.com%2Fl%2Fmeetup-join%2F19%253ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%2540thread.v2%2F0%3Fcontext%3D%257b%2522Tid%2522%253a%25222b897507-ee8c-4575-830b-4f8267c3d307%2522%252c%2522Oid%2522%253a%252220acca1b-c0bb-40c6-8476-210f47b15899%2522%257d">https://deref-gmx.net/mail/client/_gKBGVbcTE4/dereferrer/?redirectUrl=https%3A%2F%2Fteams.microsoft.com%2Fl%2Fmeetup-join%2F19%253ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%2540thread.v2%2F0%3Fcontext%3D%257b%2522Tid%2522%253a%25222b897507-ee8c-4575-830b-4f8267c3d307%2522%252c%2522Oid%2522%253a%252220acca1b-c0bb-40c6-8476-210f47b15899%2522%257d</a>&gt;<br><br><br><br>Learn More&lt;<a href="https://deref-gmx.net/mail/client/9tPcldHD9bA/dereferrer/?redirectUrl=https%3A%2F%2Faka.ms%2FJoinTeamsMeeting">https://deref-gmx.net/mail/client/9tPcldHD9bA/dereferrer/?redirectUrl=https%3A%2F%2Faka.ms%2FJoinTeamsMeeting</a>&gt; | Meeting options&lt;<a href="https://deref-gmx.net/mail/client/JH1scxj_Z_8/dereferrer/?redirectUrl=https%3A%2F%2Fteams.microsoft.com%2FmeetingOptions%2F%3ForganizerId%3D20acca1b-c0bb-40c6-8476-210f47b15899%26tenantId%3D2b897507-ee8c-4575-830b-4f8267c3d307%26threadId%3D19_meeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2%26messageId%3D0%26language%3Den-US">https://deref-gmx.net/mail/client/JH1scxj_Z_8/dereferrer/?redirectUrl=https%3A%2F%2Fteams.microsoft.com%2FmeetingOptions%2F%3ForganizerId%3D20acca1b-c0bb-40c6-8476-210f47b15899%26tenantId%3D2b897507-ee8c-4575-830b-4f8267c3d307%26threadId%3D19_meeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2%26messageId%3D0%26language%3Den-US</a>&gt; | Legal&lt;<a href="https://deref-gmx.net/mail/client/9tBii7p2aog/dereferrer/?redirectUrl=https%3A%2F%2Fwww.imperial.ac.uk%2Fmedia%2Fimperial-college%2Fadministration-and-support-services%2Fsecretariat%2Fpublic%2FICL---Events-privacy-notice---10-October-2018.pdf">https://deref-gmx.net/mail/client/9tBii7p2aog/dereferrer/?redirectUrl=https%3A%2F%2Fwww.imperial.ac.uk%2Fmedia%2Fimperial-college%2Fadministration-and-support-services%2Fsecretariat%2Fpublic%2FICL---Events-privacy-notice---10-October-2018.pdf</a>&gt;
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-two" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Feb-15">
              <div class="col-lg-2 text-right">
                <h4>Feb 15, 2023</h4><small>h. 10:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-three"></div><strong>Focus and Bias: Rating XAI Methods and Finding Biases</strong><br><br><b>Presenter:</b> Anna Arias Duart<br><em> Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-twenty-three" aria-labelledby="heading-twenty-three" data-parent="#accordion-twenty-three">
                  <div class="bg-yellow">
                    <hr><br><p><b>Title: </b><span>Focus and Bias: Rating XAI Methods and Finding Biases</span></p><p><b>Abstract</b><span>: </span><span>The evaluation of the explainability techniques is a challenge. To trust the explanations, we must first check that they reliably approximate the model behaviour. This becomes a complex task due to the lack of a ground truth specifying what defines a correct explanation. Even if different feature attribution methods have been proposed in the literature, there is no standardization of the metrics to assess and select these methods. In our work, we introduce an evaluation score for feature attribution methods designed to quantify their coherency to the task. We call it the Focus. To calculate this score, we generate mosaics composed of instances of different classes, and we apply, on top of them, the explainability method under assessment. The Focus metric will measure the proportion of attribution lying on the correct squares with respect to the total mosaic attribution.</span><br></p><p>On the other hand, explainability can also be used for bias detection. This process typically consists of a domain expert visualizing all the explanations in order to find unwanted biases. However, the vast amount of samples the domain experts must review makes this task more challenging as the dataset grows. In an ideal case, the system should provide domain experts only with a small number of selected samples containing potential biases. Since the Focus errors may correspond to visual biases in the model, the Focus score seems to be a promising tool for the selection of those samples containing potential unwanted biases.</p><p></p><p></p><p><b>Bio</b>: <span>Anna Arias Duart obtained a Bachelor's degree in Telecommunications Technology and Services Engineering in 2015 from the Universitat Politècnica de València (UPV). In 2018 she completed the Double Diploma awarded by the UPV and Télécom ParisTech (Paris). She is currently a student of the Doctoral Program in Artificial Intelligence at the Universitat Politècnica de Catalunya within the Industrial Doctorate Program in collaboration with SEAT, S.A. Her research is focused on Explainable Artificial Intelligence (XAI) and, more specifically, on the explainability of neural networks.</span></p><p><b>Where: </b>Officine Garibaldi, online at <a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1675113931377?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%226bdb21d3-848c-4944-bb98-9dbc84077ea0%22%7d">this link</a></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-three" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Feb-08">
              <div class="col-lg-2 text-right">
                <h4>Feb 08, 2023</h4><small>h. 11:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-four"></div><strong>Visual Analytics for Human-Centered Machine Learning</strong><br><br><b>Presenter: </b>Eleonora Cappuccio
                <div class="collapse" id="collapse-twenty-four" aria-labelledby="heading-twenty-four" data-parent="#accordion-twenty-four">
                  <div class="bg-yellow">
                    <hr><br><p><strong>Title</strong></p><p>Visual Analytics for Human-Centered Machine Learning</p><p><br></p><p><strong>Abstract</strong></p><p>We  introduce  a  new  research area in Visual Analytics(VA)aiming  to bridge existing gaps between  methods of interactive Machine Learning (ML) and eXplainable Artificial Intelligence (XAI), on one side, and human minds, on the other side.The gaps are, first, a conceptual mismatch between  ML/XAI  outputs  and  human mental  models and  ways of  reasoning,  second,  a  mismatch between  the  information  quantityand  level  of  detailand  human capabilities  to  perceive  and understand. A grand challenge is to adapt ML and XAI to human goals, concepts, values, and ways of thinking. Complementing   the   current   efforts   in   XAI   towards solving this challenge, VA can contribute  by  exploiting  the potential of  visualization as an  effective  way  of  communicating information  to  humans and a  strong  trigger  of  human  abstractive  perception  and  thinking. We propose a cross-disciplinary research framework and formulate research directions for VA.</p><br> <br>___<br><br>Microsoft Teams meeting<br><br><b>Join on your computer, mobile app or room device</b><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1675768130106?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a><br><br>Meeting ID: 374 597 281 40 <br>Passcode: eYfSRc <br><a href="https://www.microsoft.com/en-us/microsoft-teams/download-app"><u>Download Teams</u></a> | <a href="https://www.microsoft.com/microsoft-teams/join-a-meeting"><u> Join on the web</u></a><br><br><b>Or call in (audio only)</b><br><a><u>+39 02 3056 4191,,201586023#</u></a>   Italy, Milano Phone Conference ID: 201 586 023# <br><a href="https://dialin.teams.microsoft.com/e80d62af-367c-4976-9596-61ef054e4984?id=201586023"><u>Find a local number</u></a> | <a href="https://dialin.teams.microsoft.com/usp/pstnconferencing"><u> Reset PIN</u></a><br><a href="https://aka.ms/JoinTeamsMeeting"><u>Learn More</u></a> | <a href="https://teams.microsoft.com/meetingOptions/?organizerId=729b4d16-0567-46a8-a742-d2ae1bf09a4a&tenantId=c7456b31-a220-47f5-be52-473828670aa1&threadId=19_9a85abed0d8544a8bc86e6f8f42e599d@thread.tacv2&messageId=1675768130106&language=en-US"><u> Meeting options</u></a><br><br>________________________________________________________________________________
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-four" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Dec-21">
              <div class="col-lg-2 text-right">
                <h4>Dec 21, 2022</h4><small>h. 12:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-five"></div><strong>GLocalX</strong><br><br><b>Presenter: </b>Mattia Setzu <br><em> Room 1 @ Officine Garibaldi - il Cantiere delle Idee, Via Vincenzo Gioberti, 39, 56124 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-twenty-five" aria-labelledby="heading-twenty-five" data-parent="#accordion-twenty-five">
                  <div class="bg-yellow">
                    <hr><br><br><p><strong>Abstract</strong><br></p><p><strong></strong>Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are “black boxes” which we are not able to understand. Relying on these models has a multifaceted impact and raises significant concerns about their transparency. Applications in sensitive and critical domains are a strong motivational factor in trying to understand the behavior of black boxes. We propose to address this issue by providing an interpretable layer on top of black box models by aggregating “local” explanations. We present GLocalX, a “local-first” model agnostic explanation method. Starting from local explanations expressed in form of local decision rules, GLocalX iteratively generalizes them into global explanations by hierarchically aggregating them. Our goal is to learn accurate yet simple interpretable models to emulate the given black box, and, if possible, replace it entirely. We validate GLocalX in a set of experiments in standard and constrained settings with limited or no access to either data or local explanations. Experiments show that GLocalX is able to accurately emulate several models with simple and small models, reaching state-of-the-art performance against natively global solutions. Our findings show how it is often possible to achieve a high level of both accuracy and comprehensibility of classification models, even in complex domains with high-dimensional data, without necessarily trading one property for the other. This is a key requirement for a trustworthy AI, necessary for adoption in high-stakes decision making applications.<br></p><br><br><br><br>___<br><br>Microsoft Teams meeting<br><br><b>Join on your computer, mobile app or room device</b><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1671204581740?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a><br><br>Meeting ID: 340 547 739 918<br>Passcode: FNLU3Y<br><a href="https://www.microsoft.com/en-us/microsoft-teams/download-app"><u>Download Teams</u></a> | <a href="https://www.microsoft.com/microsoft-teams/join-a-meeting"><u>Join on the web</u></a><br><br><b>Or call in (audio only)</b><br><a><u>+39 02 3056 4191,,780108878#</u></a>   Italy, MilanoPhone Conference ID: 780 108 878#<br><a href="https://dialin.teams.microsoft.com/e80d62af-367c-4976-9596-61ef054e4984?id=780108878"><u>Find a local number</u></a> | <a href="https://dialin.teams.microsoft.com/usp/pstnconferencing"><u>Reset PIN</u></a><br><a href="https://aka.ms/JoinTeamsMeeting"><u>Learn More</u></a> | <a href="https://teams.microsoft.com/meetingOptions/?organizerId=729b4d16-0567-46a8-a742-d2ae1bf09a4a&tenantId=c7456b31-a220-47f5-be52-473828670aa1&threadId=19_9a85abed0d8544a8bc86e6f8f42e599d@thread.tacv2&messageId=1671204581740&language=en-US"><u>Meeting options</u></a>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-five" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Dec-13">
              <div class="col-lg-2 text-right">
                <h4>Dec 13, 2022</h4><small>h. 14:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-six"></div><strong>Bias Evaluation in Search Platforms through Rank and Relevance Based Measures</strong><br><br><b>Presenter: </b>Gizem Gezici, new research fellow at KDD Lab <br><em> Sala Seminari Ovest @ Department of Computer Science, Piano Secondo, Largo Bruno Pontecorvo, 3, 56127 Pisa PI, Italy</em>
                <div class="collapse" id="collapse-twenty-six" aria-labelledby="heading-twenty-six" data-parent="#accordion-twenty-six">
                  <div class="bg-yellow">
                    <hr><br><br><p><strong>Abstract: </strong><span>Search engines decide what we see for a given search query. Since many people are exposed to information through search engines, it is fair to expect that search engines are neutral. However, search engine results do not necessarily cover all the viewpoints of a search query topic, and they can be biased towards a specific view since search engine results are returned based on relevance, which is calculated using many features and sophisticated algorithms where search neutrality is not necessarily the focal point. Therefore, it is important to evaluate the search engine results with respect to bias. In this seminar, we will firstly examine the stance (in support or against), as well as the ideological bias (conservative or liberal) in search results of two popular search engines with respect to controversial query topics such as abortion, medical marijuana, and gay marriage.</span></p><p>In the second part of this seminar, we will investigate gender bias in online education. Students are increasingly using online materials to learn new subjects or to supplement their learning process in educational institutions. Nonetheless, online educational materials in terms of possible gender bias and stereotypes which may appear in different forms are yet to be investigated in the context of search bias in a widely-used search platform. Thus, as a first step towards measuring possible gender bias, we will analyse online educational videos of YouTube in terms of the perceived, i.e. from the viewer's perspective, gender of their narrators. Then, we will evaluate possible perceived gender bias in ranked educational video search results returned by YouTube in response to queries related to STEM (Science, Technology, Engineering, and Mathematics) and NON-STEM fields of education. Lastly, we will also discuss our attempts for bias mitigation in the scope of perceived gender bias in YouTube.</p><br>---<br><br>Microsoft Teams meeting<br><br><b>Join on your computer, mobile app or room device</b><a href="https://teams.microsoft.com/l/meetup-join/19%3a9a85abed0d8544a8bc86e6f8f42e599d%40thread.tacv2/1670422989832?context=%7b%22Tid%22%3a%22c7456b31-a220-47f5-be52-473828670aa1%22%2c%22Oid%22%3a%22729b4d16-0567-46a8-a742-d2ae1bf09a4a%22%7d"><u>Click here to join the meeting</u></a><br><br>Meeting ID: 322 172 125 021 <br>Passcode: re9bSM <br><a href="https://www.microsoft.com/en-us/microsoft-teams/download-app"><u>Download Teams</u></a> | <a href="https://www.microsoft.com/microsoft-teams/join-a-meeting"><u> Join on the web</u></a><br><br><b>Or call in (audio only)</b><br><a><u>+39 02 3056 4191,,261413320#</u></a>   Italy, Milano Phone Conference ID: 261 413 320# <br><a href="https://dialin.teams.microsoft.com/e80d62af-367c-4976-9596-61ef054e4984?id=261413320"><u>Find a local number</u></a> | <a href="https://dialin.teams.microsoft.com/usp/pstnconferencing"><u> Reset PIN</u></a><br><a href="https://aka.ms/JoinTeamsMeeting"><u>Learn More</u></a> | <a href="https://teams.microsoft.com/meetingOptions/?organizerId=729b4d16-0567-46a8-a742-d2ae1bf09a4a&tenantId=c7456b31-a220-47f5-be52-473828670aa1&threadId=19_9a85abed0d8544a8bc86e6f8f42e599d@thread.tacv2&messageId=1670422989832&language=en-US"><u> Meeting options</u></a>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-six" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Dec-08">
              <div class="col-lg-2 text-right">
                <h4>Dec 08, 2022</h4><small>h. 17:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-seven"></div><strong>Reasoning on Arguments and Beliefs with Probabilistic Logic Programs</strong><br><br><b>Presenter: </b>Pietro Totis
                <div class="collapse" id="collapse-twenty-seven" aria-labelledby="heading-twenty-seven" data-parent="#accordion-twenty-seven">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><b>Title: </b>Reasoning on Arguments and Beliefs with Probabilistic Logic Programs<br><br><br><b>Abstract: </b>The goal of probabilistic logic programming (PLP) languages and systems is to offer a general framework to reason and learn in structured, uncertain domains. An example of such domains are probabilistic argumentation problems, which describe arguments and their relations under uncertainty. However, whether PLP systems can effectively model and reason over these problems is still an open question.<br>In this talk I will discuss the relations between the semantics of PLP and the semantics of probabilistic argumentation frameworks. I will establish a connection by describing a novel mapping of probabilistic argument frameworks to probabilistic logic programs. By encoding the beliefs and relations of arguments in a PLP model, we can use on probabilistic argumentation problems the traditional reasoning and learning tools available in PLP frameworks, such as conditional queries, most probable explanation queries, and parameter learning. The last part of the talk will be dedicated to an overview of smProbLog, a novel PLP framework where these tools are available.<br><b><br></b><br><b>Bio: </b>Pietro Totis completed (cum laude) a BSc in Computer Science in 2016 at Università degli Studi di Udine.<br>In 2018, he completed (cum laude) a MSc in Computer Science at Università degli Studi di Udine, after an Erasmus visit at Potsdam Universität, taking courses in the MSc in Cognitive Systems.He joined KU Leuven in 2018 as a doctoral student at the Declarative Languages and Artificial Intelligence (DTAI) Lab under the supervision of Luc De Raedt, Angelika Kimmig and Jesse Davis. His research has been funded through the Fonds Wetenschappelijk Onderzoek (FWO) project "Solving Combinatorial and Probabilistic Problems in Natural Language", and he will defend his doctoral thesis in February 2023. His research interests include (probabilistic) logic programming, knowledge representation and reasoning, argumentation, and natural language processing.<p><br></p><p>  ____</p><p><span>Microsoft Teams meeting    </span></p><p><b>Join on your computer or mobile app </b>   </p><p><br></p><p><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">Click here to join the meeting</a>    </p><p>     </p><p><a href="https://aka.ms/JoinTeamsMeeting">Learn More</a> | <a href="https://teams.microsoft.com/meetingOptions/?organizerId=20acca1b-c0bb-40c6-8476-210f47b15899&tenantId=2b897507-ee8c-4575-830b-4f8267c3d307&threadId=19_meeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3@thread.v2&messageId=0&language=en-US">Meeting options</a> | <a href="https://www.imperial.ac.uk/media/imperial-college/administration-and-support-services/secretariat/public/ICL---Events-privacy-notice---10-October-2018.pdf">Legal</a>    </p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-seven" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Nov-09">
              <div class="col-lg-2 text-right">
                <h4>Nov 09, 2022</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-eight"></div><strong>Non-Classical Logics for Explanations in AI Systems</strong><br><br><b>Presenter: </b>Emiliano Lorini
                <div class="collapse" id="collapse-twenty-eight" aria-labelledby="heading-twenty-eight" data-parent="#accordion-twenty-eight">
                  <div class="bg-yellow">
                    <hr><br><br><br><b>Title: </b>Non-Classical Logics for Explanations in AI Systems<br><br><br><b>Abstract: </b>I will show how non-classical logics with special emphasis on modal logic, epistemic logic and conditional logic can be used to represent and compare a rich variety of explanations in classifier systems including abductive, contrastive, counterfactual, objective vs subjective, interactive. I will present proof-theoretic and complexity results for these logics and illustrate their expressiveness through some concrete examples.<br><br><b>Bio: </b>Emiliano Lorini is CNRS research director ("directeur de recherche") and co-head of the LILaC team (Logic, Interaction, Language and Computation) at the Institut de Recherche en Informatique de Toulouse (IRIT), Université Paul Sabatier. He obtained a master's degree in computer science from Toulouse University in 2004 and a Ph.D in Cognitive Sciences from the University of Siena (Italy) in 2007. He obtained the HDR (“Habilitation à diriger des recherches”) from Université Paul Sabatier, Toulouse in 2016. He is member of the Institute for Advanced Study in Toulouse (IAST), a Laboratory of Excellence (LabEx) at the Université de Toulouse Capitole. He has been awarded the CNRS bronze medal in 2014 for his early achievements in the area of artificial intelligence (AI). His main interest is in the formal analysis, with the aid of logic and game theory, of the reasoning, decision-making and emotions of both human agents and artificial agents as well as of several aspects of social interaction such as the concepts of trust, reputation, power and social influence.<br><br><b>Where: </b>Online at <a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">this link</a>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-eight" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-26">
              <div class="col-lg-2 text-right">
                <h4>Oct 26, 2022</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-nine"></div><strong>Responsible AI: Towards a Hybrid Method for Evaluating Data-Driven Decision-making</strong><br><br><b>Presenter:</b> Cor Steging<br><em> Online</em>
                <div class="collapse" id="collapse-twenty-nine" aria-labelledby="heading-twenty-nine" data-parent="#accordion-twenty-nine">
                  <div class="bg-yellow">
                    <hr><u></u><br><b><br></b><br><b>Title: </b>Responsible AI: Towards a Hybrid Method for Evaluating Data-Driven Decision-making<br><br><b>Abstract: </b>In order for machine learning systems to be fair and responsible, they should make correct decisions using a sound and transparent rationale. In this paper, we introduce a hybrid, model-agnostic method for rationale evaluation using dedicated test cases, similar to unit-testing in professional software development. We apply this new method in a set of machine learning experiments aimed at extracting known knowledge structures from artificial datasets from fictional and non-fictional legal domains. We demonstrate that our method allows us to analyze the rationale of black box machine learning systems by assessing which rationale elements are learned or not. We show that the networks do not learn all of the conditions that define the data despite a high classification accuracy. Furthermore, contemporary explainable AI methods cannot come to the same conclusion and thus cannot guarantee a sound rationale. Lastly, we show that the rationale can be improved by using tailor-made training data based on the results of the rationale evaluation.<br><br><b>Presenter bio: </b><span>Cor Steging is PhD candidate at the University of Groningen. He completed both his Bachelor’s and Master’s degrees in Artificial Intelligence, focusing on explainable AI. After completing his degree, he worked as a machine learning engineer in industry, providing AI solutions in domains such as healthcare, finance, insurance, academia, IT, and the energy industry.  He is now working as a part of the Hybrid Intelligence Center on research that aims to align learning and reasoning for responsible AI.</span><br><br><br><b>Where: </b>Online at <a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">this link</a><br><u></u>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-nine" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-20">
              <div class="col-lg-2 text-right">
                <h4>Oct 20, 2022</h4><small>h. 14:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty"></div><strong>Towards Interpretable Computer Vision and Thorough Evaluation of Explanations</strong><br><br><b>Presenter:</b> <span>Meike Nauta</span><b>
                <div class="collapse" id="collapse-thirty" aria-labelledby="heading-thirty" data-parent="#accordion-thirty">
                  <div class="bg-yellow">
                    <hr></b><br><br><b>Title: </b>Towards Interpretable Computer Vision and Thorough Evaluation of Explanations<br><b><br></b><br><b>Abstract: </b>Explainable AI methods aim to explain black-box machine<br>learning models and thereby give users insight into the model's<br>decision-making process. But what makes a good explanation and how do we know whether the explanation is correct and complete? And wouldn’t it be better to build interpretability directly into the predictive model?<br>In this talk, Meike Nauta will present 12 desired properties for a good explanation and provides an extensive collection of quantitative evaluation methods for explainable AI. Secondly, she will present ProtoTree: an interpretable image classifier where prototypical parts learned by a neural network are combined in an interpretable decision tree.<br><br><b>Bio: </b>Meike Nauta is finalizing her PhD on explainable AI and<br>interpretable computer vision. She is affiliated with the University of Twente in Enschede, the Netherlands and the Institute of AI in Medicine in Essen, Germany. She received her Master's degree with distinction in Computer Science at the University of Twente, and her master thesis was awarded as best computer science thesis of the Netherlands. Her view on explainable AI is that the representational power of neural networks should be used to move from post-hoc explainability to interpretability by design.<br><br><br><b>Where: </b>online at <a href="https://es.sonicurlprotection-fra.com/click?PV=2&amp;MSGID=202210181523500255800&amp;URLID=6&amp;ESV=10.0.18.7423&amp;IV=81747DCC08672C5BF907008BF1972F81&amp;TT=1666106630536&amp;ESN=0iOSIyIECOLRq%2BL0Xb5AUkJXhB5cusXJyhbJLsamd6I%3D&amp;KV=1536961729280&amp;B64_ENCODED_URL=aHR0cHM6Ly91bml2LWdyZW5vYmxlLWFscGVzLWZyLnpvb20udXMvai85MjQ4ODM5OTYxMz9wd2Q9WTNGdVRrWjZhVW8xT0d3eWEzWmFhbXByZWxZelp6MDk&amp;HK=61A573B8ED681008402A474FEE6DE31FD001228A10942921523DE4039395CF81">this link</a><b><br></b>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Oct-14">
              <div class="col-lg-2 text-right">
                <h4>Oct 14, 2022</h4><small>h. 17:30</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-one"></div><strong>Automating Commonsense Reasoning</strong><br><br><b>Presenter: </b>Prof. Gopal Gupta<br><em> Online</em>
                <div class="collapse" id="collapse-thirty-one" aria-labelledby="heading-thirty-one" data-parent="#accordion-thirty-one">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><u></u><b>Title: </b>Automating Commonsense Reasoning<br><br><b>Abstract: </b>Automating commonsense reasoning, i.e., automating the human thought process, has been considered fiendishly difficult. It is widely believed that automation of commonsense reasoning is needed to build intelligent systems that can rival humans. We argue that answer set programming (ASP) along with its goal-directed implementation allows us to reach this automation goal. We discuss essential elements needed for automating the human thought process and show how they are realized in ASP and the s(CASP) goal-directed ASP engine developed in our lab. We also show how default rules, expressible in ASP, help solve the explainable AI problem.<br><br><b>Presenter: </b>Gopal Gupta is Professor of Computer Science at the University of Texas at Dallas and co-director of its Center for Applied AI and Machine Learning. From 2009 to 2020, he served as head of the CS department at UT Dallas. His areas of research interest are in automated reasoning, computational logic, and explainable AI. His group has published extensively in these areas and has authored many software systems, several of which are publicly available.  His current research is focused on automating commonsense reasoning with the goal of achieving advanced general intelligence (AGI). To reach this goal his lab has developed several advanced reasoning systems and applied them to solving practical problems in AI. His lab has also developed explainable/interpretable AI systems. His research work has also resulted in commercial software systems that have formed the basis of start-up companies. His group has won several best-paper awards as well as the ICLP 2016 test-of-time award. Recently his research group was selected to compete in the 4th Amazon Alexa Prize Socialbot Challenge for 2020-2021. His research is currently supported by the US National Science Foundation and DARPA. He obtained his MS &amp; PhD degrees from UNC Chapel Hill and his B.Tech. in Computer Science from IIT Kanpur, India. From 1989 to 1991, he was a research associate in David H.D. Warren's Logic Programming group at the University of Bristol, UK.<br><br><b>Where: </b>Online at<b> </b><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">this link</a><br><u></u>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-one" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Jul-13">
              <div class="col-lg-2 text-right">
                <h4>Jul 13, 2022</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-two"></div><strong>Effects of machine-learned logic theories on human comprehension in machine-human teaching</strong><br><br><b>Presenter:</b> Lun Ai<br><em> Online</em>
                <div class="collapse" id="collapse-thirty-two" aria-labelledby="heading-thirty-two" data-parent="#accordion-thirty-two">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><u></u><b>Title: </b>Effects of machine-learned logic theories on human comprehension in machine-human teaching<b><br></b><br><b>Abstract: </b>In an era when interactions between humans and AI-driven technology become inevitable, the evaluation of human comprehension of the information provided by AI systems is essential in many domains. However, the lack of an operational definition of comprehensibility and limited empirical support in studies have become concerns in XAI. A recent work provides a distinct definition of comprehensibility which relates comprehension to the human out-of-sample classification accuracy. This definition allows an operational and objective metric of human comprehension in empirical trials as humans learn from examples of some tasks along with machine-learned explanations in a teaching curriculum. Based on this definition, we explore the effects of symbolic machine-learned explanations and presentation order of concepts on human comprehension in machine-human teaching curricula. We use Inductive Logic Programming (ILP) to derive logic theories from small data based on abduction and induction techniques. Learned theories are converted into visual and textual explanations as declarative demonstrations of obtained knowledge. We employ a novel approach to explore both the beneficial and harmful effects of machine-learned logic theories on human comprehension via multiple machine-human teaching trials. We propose the cognitive window constraints on human comprehension and new theoretical frameworks based on the Cognitive Science and Psychology literature. In our first experiment, results demonstrate both beneficial and harmful effects of explanations on human comprehension. In more recent experiments, results show that sequential teaching of concepts with increasing complexity a) has a beneficial effect on human comprehension, b) leads to human re-discovery of recursive problem-solving strategies, and c) studying machine-learned explanations allows adaptation of human problem-solving strategy with better performance.<br><br><b>Bio: </b>Lun Ai is a doctoral student in the Department of Computing at Imperial College London. He was awarded an MEng degree in Computing with a specialist in Artificial Intelligence at Imperial College London. He specialises in the theory and methods of Inductive Logic Programming (ILP) in particular Meta-Interpretive Learning (MIL). He has strong interests in Explainable AI and the theory of machine learning comprehensibility. He has been supported by the UK’s EPSRC Human-Like Computing network and the TAILOR network (under EU Horizon 2020 research and innovation programme). His recent works have focused on investigating constraints on the explanatory beneficiality and harmfulness of symbolic machine-learned theory in the scope of MIL, sequential human-machine interactions and effects of the explanatory teaching curriculum.<br><b><br>When: </b>Wednesday 13/7, 16:00<br><br><b>Where: </b>Online at <a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">this link</a><u></u>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-two" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Jun-09">
              <div class="col-lg-2 text-right">
                <h4>Jun 09, 2022</h4><small>h. 17:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-three"></div><strong>Logic-based Explanations for Neural Networks</strong><br><br><b>Presenter:</b> Manuel de Sousa Ribeiro<br><em> Online</em>
                <div class="collapse" id="collapse-thirty-three" aria-labelledby="heading-thirty-three" data-parent="#accordion-thirty-three">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><b>Title</b><br>Logic-based Explanations for Neural Networks <br><br><b>Abstract</b><br>Neural networks have been the key to solve a variety of different problems. However, neural network models are still regarded as black boxes, since they do not provide any human-interpretable evidence as to why they output a certain result. In this talk, we will explore a procedure to induce human-understandable logic-based theories that attempt to represent the classification process of a given neural network model, based on the idea of establishing mappings from the values of the activations produced by the neurons of that model to human-defined concepts to be used in the induced logic-based theory. Through a series of experiments, we discuss how to map the internal state of a neural network to the human-defined concepts, examine whether the results obtained by the established mappings match our understanding of the mapped concepts, and analyse the fidelity of the resulting theory and how it can be used to generate symbolic justifications for the output of neural network models. <br><br>This work was carried out in collaboration with Manuel de Sousa Ribeiro, João Ferreira, and Ricardo Gonçalves.  Bio:  João Leite is Associate Professor and Head of the Computer Science Department of FCT NOVA, Portugal, and Senior Invited Fellow of the School of Computer Science and Engineering, University of New South Wales, Sydney, Australia. He is a founding member of the Nova Laboratory for Informatics and Computer Science (NOVA LINCS) since 2015 and, before that, he was a member of the Centre for Artificial Intelligence since 1997, of which he was Executive Director between 2012 and 2015. His main research area is Artificial Intelligence with a focus on Knowledge Representation and Reasoning, Multi-Agent Systems, Semantic Web, and Neural-Symbolic Systems. He has authored one book, co-edited 25 books and journal special issues, co-authored more than 120 papers, presented several keynote talks, courses and tutorials in international Conferences and Summer Schools. He was Conference Chair of JELIA-2004, Program Committee Co-Chair of JELIA-2014, and Co-Chair of several editions of the CLIMA, LADS and DALT workshops. He regularly serves in the Program Committees of major international conferences (IJCAI, AAAI, KR, AAMAS,...). He was co-recipient of the 2017 Collaborative Research Award Santander Totta.<br><br><b>Link</b><br><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d</a>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-three" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-26">
              <div class="col-lg-2 text-right">
                <h4>May 26, 2022</h4><small>h. 16:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-four"></div><strong>Neural Networks with Natural Language Explanations</strong><br><br><b>Presenter:</b> Oana Camburu<br><em> Online</em>
                <div class="collapse" id="collapse-thirty-four" aria-labelledby="heading-thirty-four" data-parent="#accordion-thirty-four">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><u></u><b>Title: </b>Neural Networks with Natural Language Explanations<br><br><b>Abstract: </b>In order for machine learning to garner widespread public adoption, models must be able to provide human-understandable explanations for their decisions. In this talk, we will focus on the emerging direction of building neural networks that learn from natural language explanations at training time and generate such explanations at testing time. We will start with an extension of the Stanford Natural Language Inference (SNLI) dataset with an additional layer of human-written natural language explanations for the inference relations, called e-SNLI. We will see different types of architectures that incorporate these explanations into their training process and generate them at testing time. We will further see a similar approach for vision-language models, where we introduce e-SNLI-VE, a large dataset of visual-textual entailment with natural language explanations. We will also see e-ViL, a benchmark for natural language explanations in vision-language tasks, and e-UG, the SOTA model for natural language explanation generation on such tasks. These large datasets of explanations open up a range of research directions for using natural language explanations both for improving models and for asserting their trust. However, models trained on such datasets may nonetheless generate inconsistent explanations. An adversarial framework for sanity checking models over generating such inconsistencies will be presented. Finally, we will see RExC, an architecture that grounds both prediction and explanations into background knowledge. RExC improves over the previous methods by: (i) providing two types of explanations while existing models usually provide only one type, and (ii) beating by a large margin the previous SOTA in terms of the quality of its explanations.<br>References:<br>* <a href="https://proceedings.neurips.cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/4c7a167bb329bd92580a99ce422d6fa6-Paper.pdf</a><br>* <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kayser_E-ViL_A_Dataset_and_Benchmark_for_Natural_Language_Explanations_in_ICCV_2021_paper.pdf">https://openaccess.thecvf.com/content/ICCV2021/papers/Kayser_E-ViL_A_Dataset_and_Benchmark_for_Natural_Language_Explanations_in_ICCV_2021_paper.pdf</a><br>* <a href="https://aclanthology.org/2020.acl-main.382/">https://aclanthology.org/2020.acl-main.382/</a><br>* A freshly accepted paper at ICML 2022 (a previous version is here <a href="https://arxiv.org/abs/2106.13876">https://arxiv.org/abs/2106.13876</a> and we will soon update it with the improved accepted version)<br><br><b>Presenter: </b>Oana is a Research Fellow in the Department of Computer Science at the University College London, holding an Early Career Leverhulme Fellowship. Prior to this, Oana was a postdoc in the Department of Computer Science at the University of Oxford, from where she also obtained her PhD with the thesis "Explaining Deep Neural Networks". Her main research interests lie in explainability for deep learning with applications in both natural language processing and computer vision, for which she received several fellowships and grants.<br><br><b>Link: </b><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">Microsoft Teams Link</a><br><u></u>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-four" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="May-04">
              <div class="col-lg-2 text-right">
                <h4>May 04, 2022</h4><small>h. 11:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-five"></div><strong>ISTI-CNR YRA Seminar: Guidotti and Muntean</strong><br><br><b>Presenter:</b> Riccardo Guidotti and Cristina Muntean <br><em> Online</em>
                <div class="collapse" id="collapse-thirty-five" aria-labelledby="heading-thirty-five" data-parent="#accordion-thirty-five">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><b>Title:</b> ISTI-CNR Young Research Award<br><br><b>Abstract: </b>Riccardo Guidotti and Cristina Muntean present their awarded works during the ISTI-CNR ceremony<br><br>Riccardo Guidotti - Ensemble of Counterfactual Explainers<br>Cristina Muntean - Question Rewriting for Conversational Search<br><br><a href="https://us02web.zoom.us/j/89533255027?pwd=S3FhbXZTMjNMdHN3Yit5VTU2UlVKQT09">https://us02web.zoom.us/j/<u></u>89533255027?pwd=<u></u>S3FhbXZTMjNMdHN3Yit5VTU2UlVKQT<u></u>09</a><br>Meeting ID: 868 7157 3691<br>Passcode: ISTIYoung
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-five" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Apr-27">
              <div class="col-lg-2 text-right">
                <h4>Apr 27, 2022</h4><small>h. 17:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-six"></div><strong>Learning Neural Causal Models with Active Interventions</strong><br><br><b>Presenter:</b> Nino Scherrer<br><em> Online</em>
                <div class="collapse" id="collapse-thirty-six" aria-labelledby="heading-thirty-six" data-parent="#accordion-thirty-six">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><br><u></u><b>Title: </b>Learning Neural Causal Models with Active Interventions<br><br><b>Abstract: </b>Inferring causal structure from data is a challenging but important task that lies at the heart of scientific reasoning and accompanying progress. The appealing properties of neural networks have recently led to a surge of interest in differentiable neural network based methods for learning causal structures from data. So far, neural causal discovery has focused on static datasets of observational or fixed interventional origin. We introduce an active intervention targeting (AIT) method which enables a quick identification of the underlying causal structure of the data-generating process. Our method significantly reduces the required number of interactions compared with random intervention targeting and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data. The proposed method is evaluated across multiple frameworks in a wide range of settings and demonstrates superior performance on multiple benchmarks from simulated to real-world data.<br><br><b>Bio: </b>Nino Scherrer is a research intern at MILA, Montreal, under the supervision of Prof. Yoshua Bengio and Nan Rosemary Ke (DeepMind). His research interest centers around the causal perspective on machine learning with a focus on causal discovery, representation learning and reinforcement learning. He received his Masters and Bachelors in computer science at ETH Zurich and worked previsouly multiple years as a systems engineer.<u></u><br><u></u><br><u></u><br><u></u><b>Link: </b><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">Microsof Teams link</a><br><u></u>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-six" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Apr-06">
              <div class="col-lg-2 text-right">
                <h4>Apr 06, 2022</h4><small>h. 17:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-seven"></div><strong>Explain What You See: Argumentation-Based Learning for 3D Object Recognition</strong><br><br><b>Presenter: </b>Hamed Ayoobi<br><em> Online</em>
                <div class="collapse" id="collapse-thirty-seven" aria-labelledby="heading-thirty-seven" data-parent="#accordion-thirty-seven">
                  <div class="bg-yellow">
                    <hr><p><br><br><b>Title: </b><span>Explain What You See: Argumentation-Based Learning for 3D Object Recognition</span><br></p><p><b>Abstract: </b><span>Service robots are integrating more and more into our daily lives to help us in various tasks. Such robots often face objects in cluttered situations where object recognition and semantic part segmentation become challenging tasks due to occlusion. In order to overcome this limitation, we present a novel approach based on the hierarchical Bayesian method to handle semantic parts segmentation and object recognition tasks simultaneously. This technique is integrated with a recently introduced argumentation-based online incremental learning method, thereby enabling the robot to handle a high degree of occlusion. Furthermore, we show that the model is suitable for open-ended scenarios where the number of 3D objects or object parts are not fixed and can grow over time. We have performed extensive sets of experiments to assess the performance of our approach. Experimental results show that the proposed method has a higher percentage of mean intersection over union using a smaller number of learning instances. Furthermore, we show that the resulting model produces an explicit set of explanations for the 3D object category recognition task.</span></p><p> </p><p><b>Short bio: </b><span>Hamed Ayoobi is a research assistant in the computing department of imperial college of London and a Ph.D. researcher in the artificial intelligence department of the University of Groningen, Netherlands. He investigates topics in Explainable AI, Computer Vision, Robotics, and Argumentation Theory. He is currently working on argumentation-based explanations for deep neural models and argumentation-based explainable learning techniques. He received the MSc degree in Artificial Intelligence and Robotics in 2018 from Yazd University, Iran. He graduated as a top student in the master program. He was a research intern at Linnaeus University, Sweden working on verification of concurrent machine code using formal methods.</span></p><p><br></p><p><b>Link</b></p><p><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">Microsoft Teams link</a></p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-seven" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Mar-17">
              <div class="col-lg-2 text-right">
                <h4>Mar 17, 2022</h4><small>h. 17:00</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-eight"></div><strong>(XAI @ Imperial) Breaking the Local/Global explanation dichotomy: GLocalX and the Local to Global explanation paradigm</strong><br><br><b>Presenter: </b>Mattia Setzu<br><em> Online on Teams</em>
                <div class="collapse" id="collapse-thirty-eight" aria-labelledby="heading-thirty-eight" data-parent="#accordion-thirty-eight">
                  <div class="bg-yellow">
                    <hr><br><b><br></b><u></u><u></u><a href="http://xaiseminars.doc.ic.ac.uk/">http://xaiseminars.doc.ic.ac.uk/</a><br><u></u><br><br><u></u>The next talk of the XAI seminar series will take place on Thursday 17th March at 4pm GMT and will be given by Mattia Setzu&lt;<a href="https://msetzu.github.io/about/">https://msetzu.github.io/about/</a>&gt;. An abstract and short bio are given below.<br>The talk will be held remotely on Microsoft Teams. The link to join is:<br><a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d</a><br><br><br><br>Title: Breaking the Local/Global explanation dichotomy: GLocalX and the Local to Global explanation paradigm<br><br><b>Abstract: </b>Explanations come in many forms and shapes: feature importance and decision rules for tabular models, saliency maps for images, and rationales for text.  With some exceptions, explanations fall in one of two families: local explanations, which aim to explain the prediction on a single instance, and global explanations, which aim to explain predictions on any instance through a set of explanations. The former locally approximates the decision boundary of the given model, thus providing local explanations, while the latter learns global approximations, thus providing global explanations. The two have complementary strengths: local explanations tend to be precise and complex, while global ones tend to be more general, simple, and less accurate.<br>In this talk, I'll introduce the Local to Global paradigm, which rejects this dichotomy and generates global explanations directly from local ones, trying to strike a balance between the accuracy and simplicity of the two. After introducing the paradigm, I'll present GLocalX, a model-agnostic Local to Global explanation algorithm for tabular models.<br><br><b>Short bio:</b> Mattia Setzu is a Ph.D. candidate and research fellow at the University of Pisa, Italy. His research focus is on Explainable AI (XAI). He studies the Local to Global paradigm, in which explanations of single predictions are merged together to provide large scope explanations. His more recent interest include sub-global explainability, parametric explanations, and globally explaining language models.<br><br><br>-----<br><br><br><br>We are looking forward to welcoming you at the event.<br><br><br><br>Kind Regards,<br><br>Antonio, David and Fabrizio<br><br><br><br><br><br>________________________________________________________________________________<br><br>Microsoft Teams meeting<br><br>Join on your computer or mobile app<br><br><br>Click here to join the meeting&lt;<a href="https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d">https://teams.microsoft.com/l/meetup-join/19%3ameeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3%40thread.v2/0?context=%7b%22Tid%22%3a%222b897507-ee8c-4575-830b-4f8267c3d307%22%2c%22Oid%22%3a%2220acca1b-c0bb-40c6-8476-210f47b15899%22%7d</a>&gt;<br><br><br><br>Learn More&lt;<a href="https://aka.ms/JoinTeamsMeeting">https://aka.ms/JoinTeamsMeeting</a>&gt; | Meeting options&lt;<a href="https://teams.microsoft.com/meetingOptions/?organizerId=20acca1b-c0bb-40c6-8476-210f47b15899&tenantId=2b897507-ee8c-4575-830b-4f8267c3d307&threadId=19_meeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3@thread.v2&messageId=0&language=en-US">https://teams.microsoft.com/meetingOptions/?organizerId=20acca1b-c0bb-40c6-8476-210f47b15899&amp;tenantId=2b897507-ee8c-4575-830b-4f8267c3d307&amp;threadId=19_meeting_MGZkNDlmNTgtZmQ4Yi00M2JmLThmMDEtNWRiMjI4ZjNjOWY3@thread.v2&amp;messageId=0&amp;language=en-US</a>&gt; | Legal&lt;<a href="https://www.imperial.ac.uk/media/imperial-college/administration-and-support-services/secretariat/public/ICL---Events-privacy-notice---10-October-2018.pdf">https://www.imperial.ac.uk/media/imperial-college/administration-and-support-services/secretariat/public/ICL---Events-privacy-notice---10-October-2018.pdf</a>&gt;<br><br><u></u><u></u>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-eight" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Feb-03">
              <div class="col-lg-2 text-right">
                <h4>Feb 03, 2022</h4><small>h. 17:10</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-nine"></div><strong>Zhi Chen Lecture: Concept Whitening for Interpretable Image Recognition</strong><br><br><b>Presenter:</b>Zhi Chen
                <div class="collapse" id="collapse-thirty-nine" aria-labelledby="heading-thirty-nine" data-parent="#accordion-thirty-nine">
                  <div class="bg-yellow">
                    <hr><br><br><br><u></u><b>Title: </b>Concept Whitening for Interpretable Image Recognition<br><br><b>Abstract: </b>What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging tounderstand. Attempts to see inside their hidden layers can either be misleading, unusable, or rely on the latent space to possess properties that it may not have. In this work, rather than attempting to analyze a neural network posthoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a CNN, the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us a much clearer understanding for how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens) the latent space. CW can be used in any layer of the network without hurting predictive performance.<br><br><b>Bio: </b>Zhi Chen is a PhD candidate at Duke University, advised by Prof. Cynthia Rudin. His research focuses on the interpretability of machine learning models. He builds machine learning models that are inherently interpretable for different application domains like computer vision, materials science, healthcare.<u></u>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-nine" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
            <div class="row mt-5 justify-content-center" id="Feb-03">
              <div class="col-lg-2 text-right">
                <h4>Feb 03, 2022</h4><small>h. 16:10</small>
              </div>
              <div class="col-lg-7 bg-yellow p-3">
                <div class="accordion" id="accordion-forty"></div><strong>"This Looks Like That"  -- Interpretable Neural Networks for Computer Vision and Applications in Medical Image Analysis</strong><br><br><b>Presenter:</b> Dr. Chaofan Chen
                <div class="collapse" id="collapse-forty" aria-labelledby="heading-forty" data-parent="#accordion-forty">
                  <div class="bg-yellow">
                    <hr><b><br></b><br><b><br></b><br><br><b>Title:</b> "This Looks Like That" -- Interpretable Neural Networks for Computer Vision and Applications in Medical Image Analysis<br><br><b>Abstract:</b>The use of deep neural networks has become increasingly popular for computer vision tasks. These models have the potential to achieve great accuracy in recognizing images, but they are often called "black boxes" because they generally suffer from a lack of interpretability. In fact, evidence has emerged, where some deep learning methods appeared to perform well, but based their decisions on confounding rather than truly relevant information. In this talk, I will present my work in developing a prototypical case-based interpretable neural network known as a prototypical part network (ProtoPNet), which classifies an input image by comparing various parts of the image with learned prototypical features known as prototypes. I will also present the use of a prototypical case-based interpretable neural network model for analyzing medical images, with a focus on mammograms. Such an interpretable model will automatically detect areas of anomaly (such as spiculated mass margins) and present prototypical examples of similar anomalies found in other patients as explanations for their predictions. In particular, I will discuss a human-in-the-loop training scheme, which is based on a small set of doctor-annotated images and is specifically designed to prevent the model from using confounding information. The explanations generated by our interpretable neural network for analyzing mammograms thus augments the reports generated by radiologists, and may be able to assist doctors in diagnosing patients in the future.<br><br><b>Bio: </b>Dr. Chen is an Assistant Professor of Computer Science at the University of Maine. His research involves the design of interpretable machine learning models that can be understood (“interpreted”) by human beings. In particular, Dr. Chen is interested in developing new techniques to enhance the interpretability and transparency of machine learning models, especially deep learning models, and in applying such techniques to healthcare, finance, and other application domains, where high-stakes decisions are made and interpretability is key for whether one can trust the predictions made by machine learning models. Dr. Chen also participates in the Maine environmental DNA (eDNA) project: he is developing computational techniques to understand eDNA.
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty" aria-expanded="true" aria-controls="collapseAbs">Complete description</a></p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </article><br>
    <!-- Footer-->
    <footer class="site-footer">
      <div class="footer-widgets">
        <div class="container">
          <div class="row gx-lg-5">
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-1">
                <h4 class="text-yellow mb-3">Principal Investigator</h4>
                <div class="text-white">
                  <p class="mb-0">Fosca Giannotti</p>
                  <p class="mb-0">Scuola Normale Superiore</p><br>
                  <p class="mb-0">Piazza dei Cavalieri, 7</p>
                  <p class="mb-0">56126 Pisa, Italy</p><br>
                  <p class="mb-0">Email: fosca.giannotti @ sns.it</p>
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-4">
                <div class="text-white">
                  <p>The XAI Project receives funding from the European Union's Horizon 2020 Excellent Science European Research Council (ERC) programme under grant agreement No. 834756</p>The views and opinions expressed in this website are the sole responsibility of the author and do not necessarily reflect the views of the European Commission.
                  <p></p><img class="mb-4" src="assets/images/logo-eu.jpg" alt="EU flag">
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-2">
                <ul>
                  <li><a href="index.html">Xai</a></li>
                  <li><a href="about.html">Project details</a></li>
                  <li><a href="research-lines.html">Research lines</a></li>
                  <li><a href="people.html">People</a></li>
                  <li><a href="resources.html">Resources</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="footer-bottom-area">
        <div class="container">
          <div class="row gx-lg-5 align-items-center">
            <div class="col-md-12"><span class="text-yellow strong">Webdesign: Daniele Fadda © 2023</span></div>
          </div>
        </div>
      </div>
    </footer>
    <!-- End Footer-->
    <!-- javascript files-->
    <!-- jquery-->
    <script src="assets/js/jquery.min.js"></script>
    <!-- lozad js-->
    <script src="assets/js/lozad.min.js"></script>
    <!-- Bootstrap js-->
    <script src="assets/js/bootstrap.bundle.min.js"></script>
    <!-- Aos js-->
    <script src="assets/js/aos.js"></script>
    <!-- Slick flickity js-->
    <script src="assets/js/flickity.pkgd.min.js"></script>
    <!-- Magnific popup js-->
    <script src="assets/js/jquery.magnific-popup.min.js"></script>
    <!-- Countdown js-->
    <script src="assets/js/jquery.countdown.js"></script>
    <!-- CountTo js-->
    <script src="assets/js/jquery.countTo.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/global.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/cookies.js"></script>
  </body>
</html>