<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- favicon-->
    <link rel="shortcut icon" href="assets/images/favicon.ico">
    <!-- Site Title-->
    <title>Xai - Website</title>
    <meta name="description" content="Science and technology for the eXplanation of AI decision making">
    <!-- Bootstrap CSS file-->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Flickity CSS file-->
    <link href="assets/css/flickity.min.css" rel="stylesheet">
    <!-- Main CSS file-->
    <link href="assets/css/style.css" rel="stylesheet">
    <!-- Fontawesome 5 CSS file-->
    <link href="assets/css/fontawesome-all.min.css" rel="stylesheet">
    <!-- Magnific Popup CSS-->
    <link href="assets/css/magnific-popup.css" rel="stylesheet">
    <!-- Google Fonts-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&amp;display=swap">
  </head>
  <body>
    <!-- Navbar-->
    <div class="site-header header-bottom-area">
      <nav class="navbar navbar-expand-lg navbar-light sticky">
        <div class="container"><a class="navbar-brand" href="index.html"><img class="site-logo" src="assets/images/logo/xai_logo_color.png" alt="Logo"></a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarNavDropdown">
            <ul class="navbar-nav mx-auto">
              <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
              <li class="nav-item"><a class="nav-link" href="about.html">Project details</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="research-lines.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research Lines</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="line_1.html">Local to global</a><a class="dropdown-item" href="line_2.html">Casual reasoning</a><a class="dropdown-item" href="line_3.html">Platform and XUI</a><a class="dropdown-item" href="line_4.html">Case studies</a><a class="dropdown-item" href="line_5.html">Ethics and legal</a></div>
              </li>
              <li class="nav-item"><a class="nav-link" href="people.html">People</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="resources.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications and Resources</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="resources.html#publications">Publications</a><a class="dropdown-item" href="resources.html#thesis">Thesis</a></div>
              </li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">News</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="news.html">All News</a><a class="dropdown-item" href="dist-seminars.html">Distinguished seminars</a><a class="dropdown-item" href="internal-events.html">Internal Events</a></div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
    <!-- End Navbar-->
    <article class="entry">
      <div class="entry-content">
        <div class="bg-yellow pt-6 pt-lg-8 pb-4 pb-lg-5">
          <div class="container">
            <div class="row">
              <div class="col-lg-8 offset-lg-2">
                <header class="entry-header text-center">
                  <h2 class="entry-title display-5 font-weight-bold">Case studies</h2>
                </header>
              </div>
            </div>
          </div>
        </div>
        <!-- image-->
        <div class="bg-half">
          <div class="container">
            <div class="row justify-content-lg-center">
              <div class="col-lg-4">
                <div class="bg-black">
                  <div class="py-lg-7 px-lg-6 py-md-6 px-md-5 py-5 px-4">
                    <div class="text-white pr-lg-4 text-center">
                      <h1 class="display-1 mb-1 font-weight-bolder">4</h1>
                      <h5>LINE</h5>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="container mt-6">
          <div class="row justify-content-lg-center mt-6">
            <div class="col-lg-8">
              <p>In recent years, the development of AI systems focused on uncovering black-box systems through a wide range of explainability methods to make users more aware of why the AI gives the suggestion.</p>The scientific community's interest on eXplainable Artificial Intelligence (XAI) has produced a multitude of research on computational methods to make explainability possible. Nevertheless, the attention on the final user has been studied with less effort.<br>
              <p>This line address two main aspects:</p>
              <ol>
                <li>the user’s decision-making process with the eXplainable AI systems used to support high stakes decision;</li>
                <li>use cases to test explanation methods developed in XAI project.</li>
              </ol>Since the beginning of XAI project, we focused on mainly on the healthcare high-stakes decision.
              In the healthcare application domain, both AI and human doctors will have complementary roles reflecting their strengths and weaknesses. Therefore, it is of pivotal importance to develop an AI technology able to work synergistically with doctors. Current AI technologies have many shortcomings that hinder their adoption in the real world. In recent years, developing methods to explain AI models' reasoning has become the focus of many of the scientific community, particularly those in the field of eXplainable AI (XAI).
              While several XAI methods have been developed in the past years, only a few considered the specific application domain. Consider, for example, two of the most popular XAI methods: LIME [1] and SHAP [11]
              These two methods model-agnostic and application-agnostic, meaning that they can extract an explanation from any type of black-box AI model regardless of the application domain. While the model-agnostic approach to XAI offers great flexibility to the use of these methods, the application-agnostic approach implies that the specific user needs are not considered [12]. A few works have tried to close such a gap in the medical field by involving the doctors in the design procedure [13] or by performing exploratory surveys e.g. [14]. Despite these recent efforts, most of the research has been focused on laypeople [15]<br><br>In our first work, we tested the impact of AI explanation with healthcare professionals. Specifically, the context of this work is AI-supported decision-making for clinicians. Imagine, for example, a doctor who wants to have a second opinion before making a decision about the risk of a patient’s myocardial infarction. She forms her opinion on the previous visits and symptoms of the patient and then an AI suggestion is presented to her. What happens to her when she gets a second opinion? Does she trust herself or she will be more prone to follow the algorithmic suggestion to make her final decision?
              To answer this question, we collected data from 36 healthcare professionals to understand the impact of advice from a clinical DSS in two different cases: the case in which the clinical DSS explains the given suggestion and the case in which it does not.
              We adapted the judge-advisor system framework from Sniezek & Van Swol [Sniezek2001] to evaluate participants’ trust and behavioral intention to use the system in an online estimation task.
              Our main measure was the Weight of Advice, a measure of the degree to which the algorithmic suggestion (with or without explanation) influences the participant’s estimate.
              To have more meaningful insights from the participants, we collected qualitative and quantitative measures.
              Our results showed that participants relied more on the condition with the explanation compared to the condition with the sole suggestion. This happened even if participants found the explanation unsatisfying.
              It is interesting to notice that, despite the low perceived explanation quality, participants were influenced by it and relied more on the advice of the AI system. This finding might be in line with previous research on automation bias in medicine, i.e., the tendency to over-rely on automation.
              From the open questions at the end of the study, healthcare professionals showed an aversion to the use of algorithmic advice and a fear of being replaced by such AI systems.
              The importance of these results is twofold. Firstly, even if the explanation provided left most of the participants unsatisfied, they were strongly influenced by it and relied more on the advice given by the AI with the suggestion. Secondly, the importance of the ethnographic method, i.e., the open-ended questions, to get more insights from the participants that cannot be caught only with quantitative measures.<br><br>The limitations of this study need to be found in the presentation of a decision from the AI that was always correct. In future work, we aim to carry out a similar study testing if the overreliance is still maintained even when the suggestions are wrong.
              In the second work we performed, we tested how users react to a wrong suggestion when they have to evaluate different types of skin lesions images.
              The need is to develop AI systems that can assist doctors in making more informed decisions, complementing their own knowledge with the information and suggestion yielded by the AI system [MGY2021,PPP2020]. However, if the logic for the decisions of AI systems is not available it would be impossible to accomplish this goal. Skin image classification is a typical example of this problem.
              Here, the explanation is formed by synthetic exemplars and counter-exemplars of skin lesions (i.e. images generated and classified with the same outcome as the initial dataset, and with an outcome other than the original dataset, correspondingly). This explanation offers the practitioner a way to highlight the crucial traits responsible for the algorithmic classification decision
              We conducted a validation survey with 156 domain experts, novices, and laypeople to test whether the explanation increases the reliance and the confidence in the automatic decision system.
              The task was organized into ten questions. Each of those questions was presented as an image of a skin lesion without any label and its explanation was generated by ABELE. The participants were shown with two exemplars, classified as the presented skin lesion, and two counter-exemplars, i.e. another lesion class. They had to classify the presented image in a binary decision task to decide whether the class of the nevus by using the presented explanation.
              Here, one of the main points was to see how participants regain their trust after receiving a misclassified suggestion by the AI system. The results showed a slight reduction of trust towards the black box when the presented suggestion is wrong, although there is no statistically significant drop in confidence after receiving wrong advice from an AI model. However, if we restrict our analysis to the sub-sample of medical experts, we have noticed that they are more prone to lower their confidence in the system’s advice even in the subsequent trials compared to the other participants (beginners and laypeople).
              This study showed how domain experts are more prone to detect and adjust their estimates when the suggestion is not correct. This aspect can be important for the role of the final users of the system. That is to say, explanation methods without a consistent validation can be not taken into account as expected by the developers of such methods.
              Healthcare is one of the main areas in which we have put our effort to include real participants to get an insight into the effect of AI explanations during the use of clinical assisted decision-making systems.
              We are focusing on how to improve the explanations in the diagnosis forecasts to inform the design of healthcare systems to promote human-AI cooperation, avoid algorithm aversion and improve the overall decision-making process.
            </div>
          </div>
        </div>
      </div>
      <div class="entry-content">
        <div class="bg-yellow pt-6 pt-lg-8 pb-4 pb-lg-5 mt-5">
          <div class="container">
            <div class="row"></div>
          </div>
        </div>
        <!-- image-->
        <div class="bg-half">
          <div class="container">
            <div class="row">
              <div class="col-lg-4">
                <div class="bg-black">
                  <div class="py-lg-5 px-lg-6 py-md-5 px-md-5 py-5 px-4">
                    <div class="text-white pr-lg-4 text-center">
                      <h3 class="mb-1 font-weight-bolder" id="publications">Publications</h3>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <div class="container mb-6">
          <div class="row">
            <div class="row mt-5 justify-content-center" id="GMG2019">
              <div class="col-lg-1 text-right">
                <h4>2.</h4><small>[GMG2019]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-one" type="button">
                <div class="modal fade" id="modal-one" tabindex="-1" role="dialog" aria-labelledby="#modal-one-Title" aria-hidden="true">
                  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Factual and Counterfactual Explanations for Black Box Decision Making</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-one"></div><strong>Factual and Counterfactual Explanations for Black Box Decision Making</strong><br><em>Guidotti Riccardo, Monreale Anna, Giannotti Fosca, Pedreschi Dino, Ruggieri Salvatore, Turini Franco</em> (2021) - IEEE Intelligent Systems. In IEEE Intelligent Systems
                <div class="collapse" id="collapse-one" aria-labelledby="heading-one" data-parent="#accordion-one">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/mis.2019.2957223" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="SGM2021"></div>
              <div class="col-lg-1 text-right">
                <h4>3.</h4><small>[SGM2021]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/02_glocalX.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-two" type="button">
                <div class="modal fade" id="modal-two" tabindex="-1" role="dialog" aria-labelledby="#modal-two-Title" aria-hidden="true">
                  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">GLocalX - From Local to Global Explanations of Black Box AI Models</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/02_glocalX.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-two"></div><strong>GLocalX - From Local to Global Explanations of Black Box AI Models</strong><br><em>Setzu Mattia, Guidotti Riccardo, Monreale Anna, Turini Franco, Pedreschi Dino, Giannotti Fosca</em> (2021) - Artificial Intelligence. In Artificial Intelligence
                <div class="collapse" id="collapse-two" aria-labelledby="heading-two" data-parent="#accordion-two">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are “black boxes” which we are not able to understand. Relying on these models has a multifaceted impact and raises significant concerns about their transparency. Applications in sensitive and critical domains are a strong motivational factor in trying to understand the behavior of black boxes. We propose to address this issue by providing an interpretable layer on top of black box models by aggregating “local” explanations. We present GLocalX, a “local-first” model agnostic explanation method. Starting from local explanations expressed in form of local decision rules, GLocalX iteratively generalizes them into global explanations by hierarchically aggregating them. Our goal is to learn accurate yet simple interpretable models to emulate the given black box, and, if possible, replace it entirely. We validate GLocalX in a set of experiments in standard and constrained settings with limited or no access to either data or local explanations. Experiments show that GLocalX is able to accurately emulate several models with simple and small models, reaching state-of-the-art performance against natively global solutions. Our findings show how it is often possible to achieve a high level of both accuracy and comprehensibility of classification models, even in complex domains with high-dimensional data, without necessarily trading one property for the other. This is a key requirement for a trustworthy AI, necessary for adoption in high-stakes decision making applications.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1016/j.artint.2021.103457" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="PBP2022"></div>
              <div class="col-lg-1 text-right">
                <h4>8.</h4><small>[PBP2022]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-seven"></div><strong>Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems</strong><br><em>Cecilia Panigutti, Andrea Beretta, Dino Pedreschi, Fosca Giannotti</em> (2022) - 2022 Conference on Human Factors in Computing Systems. In Proceedings of the 2022 Conference on Human Factors in Computing Systems
                <div class="collapse" id="collapse-seven" aria-labelledby="heading-seven" data-parent="#accordion-seven">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The field of eXplainable Artificial Intelligence (XAI) focuses on providing explanations for AI systems' decisions. XAI applications to AI-based Clinical Decision Support Systems (DSS) should increase trust in the DSS by allowing clinicians to investigate the reasons behind its suggestions. In this paper, we present the results of a user study on the impact of advice from a clinical DSS on healthcare providers' judgment in two different cases: the case where the clinical DSS explains its suggestion and the case it does not. We examined the weight of advice, the behavioral intention to use the system, and the perceptions with quantitative and qualitative measures. Our results indicate a more significant impact of advice when an explanation for the DSS decision is provided. Additionally, through the open-ended questions, we provide some insights on how to improve the explanations in the diagnosis forecasts for healthcare assistants, nurses, and doctors.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-seven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="VMG2022"></div>
              <div class="col-lg-1 text-right">
                <h4>10.</h4><small>[VMG2022]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/understanding_peace.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-nine" type="button">
                <div class="modal fade" id="modal-nine" tabindex="-1" role="dialog" aria-labelledby="#modal-nine-Title" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Understanding peace through the world news</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/understanding_peace.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-nine"></div><strong>Understanding peace through the world news</strong><br><em>Voukelatou Vasiliki, Miliou Ioanna, Giannotti Fosca, Pappalardo Luca</em> (2022) - EPJ Data Science. In EPJ Data Science
                <div class="collapse" id="collapse-nine" aria-labelledby="heading-nine" data-parent="#accordion-nine">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Peace is a principal dimension of well-being and is the way out of inequity and violence. Thus, its measurement has drawn the attention of researchers, policymakers, and peacekeepers. During the last years, novel digital data streams have drastically changed the research in this field. The current study exploits information extracted from a new digital database called Global Data on Events, Location, and Tone (GDELT) to capture peace through the Global Peace Index (GPI). Applying predictive machine learning models, we demonstrate that news media attention from GDELT can be used as a proxy for measuring GPI at a monthly level. Additionally, we use explainable AI techniques to obtain the most important variables that drive the predictions. This analysis highlights each country’s profile and provides explanations for the predictions, and particularly for the errors and the events that drive these errors. We believe that digital data exploited by researchers, policymakers, and peacekeepers, with data science tools as powerful as machine learning, could contribute to maximizing the societal benefits and minimizing the risks to peace.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-nine" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1140/epjds/s13688-022-00315-z" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="PMC2022"></div>
              <div class="col-lg-1 text-right">
                <h4>12.</h4><small>[PMC2022]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Ethical Societal and Legal.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-eleven" type="button">
                <div class="modal fade" id="modal-eleven" tabindex="-1" role="dialog" aria-labelledby="#modal-eleven-Title" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Ethical, societal and legal issues in deep learning for healthcare</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/Ethical Societal and Legal.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-eleven"></div><strong>Ethical, societal and legal issues in deep learning for healthcare</strong><br><em>Panigutti Cecilia, Monreale Anna, Comandè Giovanni, Pedreschi Dino</em> (2022) - Deep Learning in Biology and Medicine. In Deep Learning in Biology and Medicine
                <div class="collapse" id="collapse-eleven" aria-labelledby="heading-eleven" data-parent="#accordion-eleven">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Biology, medicine and biochemistry have become data-centric fields for which Deep Learning methods are delivering groundbreaking results. Addressing high impact challenges, Deep Learning in Biology and Medicine provides an accessible and organic collection of Deep Learning essays on bioinformatics and medicine. It caters for a wide readership, ranging from machine learning practitioners and data scientists seeking methodological knowledge to address biomedical applications, to life science specialists in search of a gentle reference for advanced data analytics.With contributions from internationally renowned experts, the book covers foundational methodologies in a wide spectrum of life sciences applications, including electronic health record processing, diagnostic imaging, text processing, as well as omics-data processing. This survey of consolidated problems is complemented by a selection of advanced applications, including cheminformatics and biomedical interaction network analysis. A modern and mindful approach to the use of data-driven methodologies in the life sciences also requires careful consideration of the associated societal, ethical, legal and transparency challenges, which are covered in the concluding chapters of this book.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-eleven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://www.worldscientific.com/worldscibooks/10.1142/q0322" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="RAB2021"></div>
              <div class="col-lg-1 text-right">
                <h4>23.</h4><small>[RAB2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-two"></div><strong>Occlusion-Based Explanations in Deep Recurrent Models for Biomedical Signals</strong><br><em>Resta Michele, Monreale Anna, Bacciu Davide</em> (2021) - Entropy. In Entropy
                <div class="collapse" id="collapse-twenty-two" aria-labelledby="heading-twenty-two" data-parent="#accordion-twenty-two">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The biomedical field is characterized by an ever-increasing production of sequential data, which often come in the form of biosignals capturing the time-evolution of physiological processes, such as blood pressure and brain activity. This has motivated a large body of research dealing with the development of machine learning techniques for the predictive analysis of such biosignals. Unfortunately, in high-stakes decision making, such as clinical diagnosis, the opacity of machine learning models becomes a crucial aspect to be addressed in order to increase the trust and adoption of AI technology. In this paper, we propose a model agnostic explanation method, based on occlusion, that enables the learning of the input’s influence on the model predictions. We specifically target problems involving the predictive analysis of time-series data and the models that are typically used to deal with data of such nature, i.e., recurrent neural networks. Our approach is able to provide two different kinds of explanations: one suitable for technical experts, who need to verify the quality and correctness of machine learning models, and one suited to physicians, who need to understand the rationale underlying the prediction to make aware decisions. A wide experimentation on different physiological data demonstrates the effectiveness of our approach both in classification and regression tasks.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.3390/e23081064" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="PB2021"></div>
              <div class="col-lg-1 text-right">
                <h4>24.</h4><small>[PB2021]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-three"></div><strong>Intelligenza artificiale in ambito diabetologico: prospettive, dalla ricerca di base alle applicazioni cliniche</strong><br><em>Panigutti Cecilia, Bosi Emanuele</em> (2021) - Il Diabete Online, Organo ufficiale della Società Italiana di Diabetologia, Medicina traslazionale: Applicazioni cliniche della ricerca di base, Vol. 33, N. 1 2021. In Il Diabete
                <div class="collapse" id="collapse-twenty-three" aria-labelledby="heading-twenty-three" data-parent="#accordion-twenty-three">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">nan</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="10.30682/ildia2101f" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="PPB2021"></div>
              <div class="col-lg-1 text-right">
                <h4>25.</h4><small>[PPB2021]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/FairLens.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-twenty-four" type="button">
                <div class="modal fade" id="modal-twenty-four" tabindex="-1" role="dialog" aria-labelledby="#modal-twenty-four-Title" aria-hidden="true">
                  <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">FairLens: Auditing black-box clinical decision support systems</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/FairLens.png " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-four"></div><strong>FairLens: Auditing black-box clinical decision support systems</strong><br><em>Panigutti Cecilia, Perotti Alan, Panisson André, Bajardi Paolo, Pedreschi Dino</em> (2021) - Information Processing & Management. In Journal of Information Processing and Management
                <div class="collapse" id="collapse-twenty-four" aria-labelledby="heading-twenty-four" data-parent="#accordion-twenty-four">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Highlights: We present a pipeline to detect and explain potential fairness issues in Clinical DSS. We study and compare different multi-label classification disparity measures. We explore ICD9 bias in MIMIC-IV, an openly available ICU benchmark dataset</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1016/j.ipm.2021.102657" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="NPN2020"></div>
              <div class="col-lg-1 text-right">
                <h4>26.</h4><small>[NPN2020]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-five"></div><strong>Prediction and Explanation of Privacy Risk on Mobility Data with Neural Networks</strong><br><em>Naretto Francesca, Pellungrini Roberto, Nardini Franco Maria, Giannotti Fosca</em> (2021) - ECML PKDD 2020 Workshops. In ECML PKDD 2020 Workshops
                <div class="collapse" id="collapse-twenty-five" aria-labelledby="heading-twenty-five" data-parent="#accordion-twenty-five">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">The analysis of privacy risk for mobility data is a fundamental part of any privacy-aware process based on such data. Mobility data are highly sensitive. Therefore, the correct identification of the privacy risk before releasing the data to the public is of utmost importance. However, existing privacy risk assessment frameworks have high computational complexity. To tackle these issues, some recent work proposed a solution based on classification approaches to predict privacy risk using mobility features extracted from the data. In this paper, we propose an improvement of this approach by applying long short-term memory (LSTM) neural networks to predict the privacy risk directly from original mobility data. We empirically evaluate privacy risk on real data by applying our LSTM-based approach. Results show that our proposed method based on a LSTM network is effective in predicting the privacy risk with results in terms of F1 of up to 0.91. Moreover, to explain the predictions of our model, we employ a state-of-the-art explanation algorithm, Shap. We explore the resulting explanation, showing how it is possible to provide effective predictions while explaining them to the end-user.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-65965-3_34" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="NPM2020"></div>
              <div class="col-lg-1 text-right">
                <h4>27.</h4><small>[NPM2020]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-six"></div><strong>Predicting and Explaining Privacy Risk Exposure in Mobility Data</strong><br><em>Naretto Francesca, Pellungrini Roberto, Monreale Anna, Nardini Franco Maria, Musolesi Mirco</em> (2021) - Discovery Science. In Discovery Science Conference
                <div class="collapse" id="collapse-twenty-six" aria-labelledby="heading-twenty-six" data-parent="#accordion-twenty-six">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Mobility data is a proxy of different social dynamics and its analysis enables a wide range of user services. Unfortunately, mobility data are very sensitive because the sharing of people’s whereabouts may arise serious privacy concerns. Existing frameworks for privacy risk assessment provide tools to identify and measure privacy risks, but they often (i) have high computational complexity; and (ii) are not able to provide users with a justification of the reported risks. In this paper, we propose expert, a new framework for the prediction and explanation of privacy risk on mobility data. We empirically evaluate privacy risk on real data, simulating a privacy attack with a state-of-the-art privacy risk assessment framework. We then extract individual mobility profiles from the data for predicting their risk. We compare the performance of several machine learning algorithms in order to identify the best approach for our task. Finally, we show how it is possible to explain privacy risk prediction on real data, using two algorithms: Shap, a feature importance-based method and Lore, a rule-based method. Overall, expert is able to provide a user with the privacy risk and an explanation of the risk itself. The experiments show excellent performance for the prediction task.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-six" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-61527-7_27" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="GMM2020"></div>
              <div class="col-lg-1 text-right">
                <h4>29.</h4><small>[GMM2020]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-eight"></div><strong>Explaining Image Classifiers Generating Exemplars and Counter-Exemplars from Latent Representations</strong><br><em>Guidotti Riccardo, Monreale Anna, Matwin Stan, Pedreschi Dino</em> (2021) - Proceedings of the AAAI Conference on Artificial Intelligence
                <div class="collapse" id="collapse-twenty-eight" aria-labelledby="heading-twenty-eight" data-parent="#accordion-twenty-eight">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">We present an approach to explain the decisions of black box image classifiers through synthetic exemplar and counter-exemplar learnt in the latent feature space. Our explanation method exploits the latent representations learned through an adversarial autoencoder for generating a synthetic neighborhood of the image for which an explanation is required. A decision tree is trained on a set of images represented in the latent space, and its decision rules are used to generate exemplar images showing how the original image can be modified to stay within its class. Counterfactual rules are used to generate counter-exemplars showing how the original image can “morph” into another class. The explanation also comprehends a saliency map highlighting the areas that contribute to its classification, and areas that push it into another class. A wide and deep experimental evaluation proves that the proposed method outperforms existing explainers in terms of fidelity, relevance, coherence, and stability, besides providing the most useful and interpretable explanations.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-eight" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1609/aaai.v34i09.7116" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="GMM2019"></div>
              <div class="col-lg-1 text-right">
                <h4>30.</h4><small>[GMM2019]</small>
              </div>
              <div class="col-lg-8 bg-yellow p-3">
                <div class="accordion" id="accordion-twenty-nine"></div><strong>Black Box Explanation by Learning Image Exemplars in the Latent Feature Space</strong><br><em>Guidotti Riccardo, Monreale Anna, Matwin Stan, Pedreschi Dino</em> (2021) - Machine Learning and Knowledge Discovery in Databases. In Black Box Explanation by Learning Image Exemplars in the Latent Feature Space.
                <div class="collapse" id="collapse-twenty-nine" aria-labelledby="heading-twenty-nine" data-parent="#accordion-twenty-nine">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">We present an approach to explain the decisions of black box models for image classification. While using the black box to label images, our explanation method exploits the latent feature space learned through an adversarial autoencoder. The proposed method first generates exemplar images in the latent feature space and learns a decision tree classifier. Then, it selects and decodes exemplars respecting local decision rules. Finally, it visualizes them in a manner that shows to the user how the exemplars can be modified to either stay within their class, or to become counter-factuals by “morphing” into another class. Since we focus on black box decision systems for image classification, the explanation obtained from the exemplars also provides a saliency map highlighting the areas of the image that contribute to its classification, and areas of the image that push it into another class. We present the results of an experimental evaluation on three datasets and two black box models. Besides providing the most useful and interpretable explanations, we show that the proposed method outperforms existing explainers in terms of fidelity, relevance, coherence, and stability.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-nine" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-46150-8_12" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="PGM2019"></div>
              <div class="col-lg-1 text-right">
                <h4>34.</h4><small>[PGM2019]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Explaining Multi-label Black-Box Classifiers for Health Applications.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-thirty-three" type="button">
                <div class="modal fade" id="modal-thirty-three" tabindex="-1" role="dialog" aria-labelledby="#modal-thirty-three-Title" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Explaining Multi-label Black-Box Classifiers for Health Applications</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/Explaining Multi-label Black-Box Classifiers for Health Applications.png " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-three"></div><strong>Explaining Multi-label Black-Box Classifiers for Health Applications</strong><br><em>Panigutti Cecilia, Guidotti Riccardo, Monreale Anna, Pedreschi Dino</em> (2021) - Precision Health and Medicine. In International Workshop on Health Intelligence (pp. 97-110). Springer, Cham.
                <div class="collapse" id="collapse-thirty-three" aria-labelledby="heading-thirty-three" data-parent="#accordion-thirty-three">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Today the state-of-the-art performance in classification is achieved by the so-called “black boxes”, i.e. decision-making systems whose internal logic is obscure. Such models could revolutionize the health-care system, however their deployment in real-world diagnosis decision support systems is subject to several risks and limitations due to the lack of transparency. The typical classification problem in health-care requires a multi-label approach since the possible labels are not mutually exclusive, e.g. diagnoses. We propose MARLENA, a model-agnostic method which explains multi-label black box decisions. MARLENA explains an individual decision in three steps. First, it generates a synthetic neighborhood around the instance to be explained using a strategy suitable for multi-label decisions. It then learns a decision tree on such neighborhood and finally derives from it a decision rule that explains the black box decision. Our experiments show that MARLENA performs well in terms of mimicking the black box behavior while gaining at the same time a notable amount of interpretability through compact decision rules, i.e. rules with limited length.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-24409-5_9" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
              <div class="row mt-5 justify-content-center" id="PPP2020"></div>
              <div class="col-lg-1 text-right">
                <h4>37.</h4><small>[PPP2020]</small>
              </div>
              <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/Doctor XAI.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-thirty-six" type="button">
                <div class="modal fade" id="modal-thirty-six" tabindex="-1" role="dialog" aria-labelledby="#modal-thirty-six-Title" aria-hidden="true">
                  <div class="modal-dialog modal-dialog-centered" role="document">
                    <div class="modal-content">
                      <div class="modal-header">
                        <p class="small">Doctor XAI: an ontology-based approach to black-box sequential data classification explanations</p>
                        <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                      </div>
                      <div class="modal-body"><img src="assets/images/publications/Doctor XAI.jpg " alt="immagine"></div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-6 bg-yellow p-3">
                <div class="accordion" id="accordion-thirty-six"></div><strong>Doctor XAI: an ontology-based approach to black-box sequential data classification explanations</strong><br><em>Panigutti Cecilia, Perrotti Alan, Pedreschi Dino</em> (2020) - FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. In FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency
                <div class="collapse" id="collapse-thirty-six" aria-labelledby="heading-thirty-six" data-parent="#accordion-thirty-six">
                  <div class="bg-yellow">
                    <hr>
                    <p class="small"><strong>Abstract</strong></p>
                    <p class="small">Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.</p>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 pl-3">
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-six" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
                <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://dl.acm.org/doi/10.1145/3351095.3372855" target="_blank">External Link</a></p>
                <p class="my-1"><small>Research Line <strong>4</strong></small></p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="entry-content mt-0">
        <div class="container mb-10">
          <div class="row gx-lg-3 gy-3 mt-lg-4 mt-md-2 mt-2 mb-5">
            <h3>Researchers working on this line</h3>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Guidotti.jpg" alt="Riccardo Guidotti"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Riccardo<br/>Guidotti</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Assitant Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Nanni.jpg" alt="Mirco Nanni"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Mirco<br/>Nanni</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Pappalardo.jpg" alt="Luca Pappalardo"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Luca<br/>Pappalardo</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Rinzivillo.jpg" alt="Salvo Rinzivillo"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Salvo<br/>Rinzivillo</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Beretta.jpg" alt="Andrea Beretta"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Andrea<br/>Beretta</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Monreale.jpg" alt="Anna Monreale"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Anna<br/>Monreale</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Associate Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Panigutti.jpg" alt="Cecilia Panigutti"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Cecilia<br/>Panigutti</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Spinnato.jpg" alt="Francesco Spinnato"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Francesco<br/>Spinnato</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Naretto.jpg" alt="Francesca Naretto"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Francesca<br/>Naretto</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Scuola Normale</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4 ▪ 5</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Metta.jpg" alt="Carlo Metta"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Carlo<br/>Metta</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Researcher</span></div>
                  <p class="mb-0">ISTI - CNR Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 2 ▪ 3 ▪4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Cappuccio.jpg" alt="Eleonora Cappuccio"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Eleonora<br/>Cappuccio</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa - Bari</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>3 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Malizia.jpg" alt="Alessio Malizia"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Alessio<br/>Malizia</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Associate Professor</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1 ▪ 3 ▪ 4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Camarda.jpg" alt="Giovanni Camarda"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Giovanni<br/>Camarda</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">University of Pisa</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>1</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
            <div class="col-lg-2 col-md-4 col-sm-6">
              <div class="member-image"><img class="card-img" src="assets/images/p_Sajno.jpg" alt="Elena Sajno"/></div>
              <div class="member-content bg-yellow">
                <div class="member-text px-3 py-3">
                  <h5 class="member-name">Elena<br/>Sajno</h5>
                  <hr/>
                  <div class="member-tag"><span class="member-role">Phd Student</span></div>
                  <p class="mb-0">Università Cattolica di Milano</p>
                  <hr/>
                  <p class="text-uppercase">R. line <strong>4</strong></p>
                </div>
              </div>
              <!-- End People Card-->
            </div>
          </div>
        </div>
      </div>
    </article>
    <!-- Footer-->
    <footer class="site-footer">
      <div class="footer-widgets">
        <div class="container">
          <div class="row gx-lg-5">
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-1">
                <h4 class="text-yellow mb-3">Principal Investigator</h4>
                <div class="text-white">
                  <p class="mb-0">Fosca Giannotti</p>
                  <p class="mb-0">Scuola Normale Superiore</p><br>
                  <p class="mb-0">Piazza dei Cavalieri, 7</p>
                  <p class="mb-0">56126 Pisa, Italy</p><br>
                  <p class="mb-0">Email: fosca.giannotti @ sns.it</p>
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-4">
                <div class="text-white">
                  <p>The XAI Project receives funding from the European Union's Horizon 2020 Excellent Science European Research Council (ERC) programme under grant agreement No. 834756</p>The views and opinions expressed in this website are the sole responsibility of the author and do not necessarily reflect the views of the European Commission.
                  <p></p><img class="mb-4" src="assets/images/logo-eu.jpg" alt="EU flag">
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-2">
                <ul>
                  <li><a href="index.html">Xai</a></li>
                  <li><a href="about.html">Project details</a></li>
                  <li><a href="research-lines.html">Research lines</a></li>
                  <li><a href="people.html">People</a></li>
                  <li><a href="resources.html">Resources</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="footer-bottom-area">
        <div class="container">
          <div class="row gx-lg-5 align-items-center">
            <div class="col-md-12"><span class="text-yellow strong">Webdesign: Daniele Fadda © 2022</span></div>
          </div>
        </div>
      </div>
    </footer>
    <!-- End Footer-->
    <!-- javascript files-->
    <!-- jquery-->
    <script src="assets/js/jquery.min.js"></script>
    <!-- lozad js-->
    <script src="assets/js/lozad.min.js"></script>
    <!-- Bootstrap js-->
    <script src="assets/js/bootstrap.bundle.min.js"></script>
    <!-- Aos js-->
    <script src="assets/js/aos.js"></script>
    <!-- Slick flickity js-->
    <script src="assets/js/flickity.pkgd.min.js"></script>
    <!-- Magnific popup js-->
    <script src="assets/js/jquery.magnific-popup.min.js"></script>
    <!-- Countdown js-->
    <script src="assets/js/jquery.countdown.js"></script>
    <!-- CountTo js-->
    <script src="assets/js/jquery.countTo.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/global.js"></script>
  </body>
</html>