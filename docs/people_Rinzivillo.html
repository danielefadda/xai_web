<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- favicon-->
    <link rel="shortcut icon" href="assets/images/favicon.ico">
    <!-- Site Title-->
    <title>Xai - Website</title>
    <meta name="description" content="Science and technology for the eXplanation of AI decision making">
    <!-- Bootstrap CSS file-->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Flickity CSS file-->
    <link href="assets/css/flickity.min.css" rel="stylesheet">
    <!-- Main CSS file-->
    <link href="assets/css/style.css" rel="stylesheet">
    <!-- Fontawesome 5 CSS file-->
    <link href="assets/css/fontawesome-all.min.css" rel="stylesheet">
    <!-- Magnific Popup CSS-->
    <link href="assets/css/magnific-popup.css" rel="stylesheet">
    <!-- Google Fonts-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&amp;display=swap">
    <script type="text/javascript">
      // Matomo Code
      var _paq = window._paq = window._paq || [];
      /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
      _paq.push(['trackPageView']);
      _paq.push(['enableLinkTracking']);
      (function () {
      var u="//piwikdd.isti.cnr.it/";
      _paq.push(['setTrackerUrl', u+'piwik.php']);
      _paq.push(['setSiteId', '12']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
      })
      ();
    </script>
  </head>
  <body>
    <!-- Navbar-->
    <div class="site-header header-bottom-area">
      <nav class="navbar navbar-expand-lg navbar-light sticky">
        <div class="container"><a class="navbar-brand" href="index.html"><img class="site-logo" src="assets/images/logo/xai_logo_color.png" alt="Logo"></a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarNavDropdown">
            <ul class="navbar-nav mx-auto">
              <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
              <li class="nav-item"><a class="nav-link" href="about.html">Project details</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="research-lines.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research Lines</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="line_1.html">1. Local to global</a><a class="dropdown-item" href="line_2.html">2. Casual reasoning</a><a class="dropdown-item" href="line_3.html">3. Platform and XUI</a><a class="dropdown-item" href="line_4.html">4. Case studies</a><a class="dropdown-item" href="line_5.html">5. Ethics and legal</a></div>
              </li>
              <li class="nav-item"><a class="nav-link" href="people.html">People</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="resources.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications and Resources</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="resources.html#publications">Publications</a><a class="dropdown-item" href="reports.html">Reports</a><a class="dropdown-item" href="resources.html#thesis">Thesis</a><a class="dropdown-item" href="dissemination.html">Dissemination tools</a></div>
              </li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">News</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="news.html">All News</a><a class="dropdown-item" href="dist-seminars.html">Distinguished seminars</a><a class="dropdown-item" href="internal-events.html">Internal Events</a></div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
    <!-- End Navbar-->
    <!-- Cookies-->
    <div class="cookie-banner" style="display: none">
      <p class="mx-2">We use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. 
        <u><a href="privacy.html" link’="privacy.html">No, give me more info.</a></u>
      </p>
      <button class="close">&times;</button>
    </div>
    <!-- End Cookies-->
    <article class="entry"></article>
    <div class="entry-content">
      <div class="bg-yellow pt-6 pt-lg-8 pb-4 pb-lg-5">
        <div class="bg-half">
          <div class="container">
            <div class="row">
              <div class="col-lg-4 offset-lg-4">
                <h1 class="text-center">Salvo Rinzivillo</h1><img class="card-img" src="assets/images/p_Rinzivillo.jpg" alt="Salvo Rinzivillo"/>
                <hr/>
                <p class="text-uppercase">Research Line 3 - leader</p>
                <hr/>
                <div class="member-content">
                  <div class="member-text px-0 py-3">
                    <p class="text-uppercase">Involved in the research line <strong>1 ▪ 3 ▪ 4 ▪ 5</strong></p>
                    <p>Role: Researcher</p>
                    <p class="mb-0">Affiliation: ISTI - CNR Pisa</p>
                    <hr/>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="container mt-6">
        <div class="row justify-content-lg-center">
          <div class="col-lg-8"></div>
        </div>
      </div>
      <div class="container mb-6">
        <div class="row">
          <div class="row mt-5 justify-content-center" id="BGG2023">
            <div class="col-lg-1 text-right">
              <h4>5.</h4><small>[BGG2023]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-four"></div><strong>Benchmarking and survey of explanation methods for black box models</strong><br><em>Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, Salvatore Rinzivillo</em> (2023) - Springer Science+Business Media, LLC, part of Springer Nature. In Data Mining and Knowledge Discovery
              <div class="collapse" id="collapse-four" aria-labelledby="heading-four" data-parent="#accordion-four">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-four" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/s10618-023-00933-9" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="MBG2023"></div>
            <div class="col-lg-1 text-right">
              <h4>6.</h4><small>[MBG2023]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-five"></div><strong>Improving trust and confidence in medical skin lesion diagnosis through explainable deep learning</strong><br><em>Carlo Metta, Andrea Beretta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo, Fosca Giannotti </em> (2023) - Springer Nature. In International Journal of Data Science and Analytics
              <div class="collapse" id="collapse-five" aria-labelledby="heading-five" data-parent="#accordion-five">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">A key issue in critical contexts such as medical diagnosis is the interpretability of the deep learning models adopted in decision-making systems. Research in eXplainable Artificial Intelligence (XAI) is trying to solve this issue. However, often XAI approaches are only tested on generalist classifier and do not represent realistic problems such as those of medical diagnosis. In this paper, we aim at improving the trust and confidence of users towards automatic AI decision systems in the field of medical skin lesion diagnosis by customizing an existing XAI approach for explaining an AI model able to recognize different types of skin lesions. The explanation is generated through the use of synthetic exemplar and counter-exemplar images of skin lesions and our contribution offers the practitioner a way to highlight the crucial traits responsible for the classification decision. A validation survey with domain experts, beginners, and unskilled people shows that the use of explanations improves trust and confidence in the automatic decision system. Also, an analysis of the latent space adopted by the explainer unveils that some of the most frequent skin lesion classes are distinctly separated. This phenomenon may stem from the intrinsic characteristics of each class and may help resolve common misclassifications made by human experts.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://doi.org/10.1007/s41060-023-00401-z" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="BRF2022"></div>
            <div class="col-lg-1 text-right">
              <h4>23.</h4><small>[BRF2022]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/latent-space-exploration.png " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-twenty-two" type="button">
              <div class="modal fade" id="modal-twenty-two" tabindex="-1" role="dialog" aria-labelledby="#modal-twenty-two-Title" aria-hidden="true">
                <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Explaining Black Box with visual exploration of Latent Space</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/latent-space-exploration.png " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-two"></div><strong>Explaining Black Box with visual exploration of Latent Space</strong><br><em>Bodria Francesco, Rinzivillo Salvatore, Fadda Daniele, Guidotti Riccardo, Fosca Giannotti, Pedreschi Dino</em> (2022) - EUROVIS 2022. In Proceedings of the 2022 Conference Eurovis 2022
              <div class="collapse" id="collapse-twenty-two" aria-labelledby="heading-twenty-two" data-parent="#accordion-twenty-two">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Autoencoders are a powerful yet opaque feature reduction technique, on top of which we propose a novel way for the joint visual exploration of both latent and real space. By interactively exploiting the mapping between latent and real features, it is possible to unveil the meaning of latent features while providing deeper insight into the original variables. To achieve this goal, we exploit and re-adapt existing approaches from eXplainable Artificial Intelligence (XAI) to understand the relationships between the input and latent features. The uncovered relationships between input features and latent ones allow the user to understand the data structure concerning external variables such as the predictions of a classification model. We developed an interactive framework that visually explores the latent space and allows the user to understand the relationships of the input features with model prediction.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-two" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://diglib.eg.org/handle/10.2312/evs20221098" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PBF2022"></div>
            <div class="col-lg-1 text-right">
              <h4>24.</h4><small>[PBF2022]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-three"></div><strong>Co-design of human-centered, explainable AI for clinical decision support</strong><br><em>Panigutti Cecilia, Beretta Andrea, Fadda Daniele , Giannotti Fosca, Pedreschi Dino, Perotti Alan, Rinzivillo Salvatore</em> (2022). In ACM Transactions on Interactive Intelligent Systems
              <div class="collapse" id="collapse-twenty-three" aria-labelledby="heading-twenty-three" data-parent="#accordion-twenty-three">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models, and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique, and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback, with a two-fold outcome: first, we obtain evidence that explanations increase users' trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so that we can re-design a better, more human-centered explanation interface.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="MGY2021"></div>
            <div class="col-lg-1 text-right">
              <h4>30.</h4><small>[MGY2021]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-nine"></div><strong>Exemplars and Counterexemplars Explanations for Skin Lesion Classifiers</strong><br><em>Carlo Metta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo</em> (2021) - IOS Press. In HHAI2022: Augmenting Human Intellect, S. Schlobach et al. (Eds.)
              <div class="collapse" id="collapse-twenty-nine" aria-labelledby="heading-twenty-nine" data-parent="#accordion-twenty-nine">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Explainable AI consists in developing models allowing interaction between decision systems and humans by making the decisions understandable. We propose a case study for skin lesion diagnosis showing how it is possible to provide explanations of the decisions of deep neural network trained to label skin lesions.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-nine" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://ebooks.iospress.nl/volumearticle/60877" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="MGY2021"></div>
            <div class="col-lg-1 text-right">
              <h4>36.</h4><small>[MGY2021]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-thirty-five"></div><strong>Exemplars and Counterexemplars Explanations for Image Classifiers, Targeting Skin Lesion Labeling</strong><br><em>Metta Carlo, Guidotti Riccardo, Yin Yuan, Gallinari Patrick, Rinzivillo Salvatore</em> (2021) - 2021 IEEE Symposium on Computers and Communications (ISCC). In 2021 IEEE Symposium on Computers and Communications (ISCC)
              <div class="collapse" id="collapse-thirty-five" aria-labelledby="heading-thirty-five" data-parent="#accordion-thirty-five">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Explainable AI consists in developing mechanisms allowing for an interaction between decision systems and humans by making the decisions of the formers understandable. This is particularly important in sensitive contexts like in the medical domain. We propose a use case study, for skin lesion diagnosis, illustrating how it is possible to provide the practitioner with explanations on the decisions of a state of the art deep neural network classifier trained to characterize skin lesions from examples. Our framework consists of a trained classifier onto which an explanation module operates. The latter is able to offer the practitioner exemplars and counterexemplars for the classification diagnosis thus allowing the physician to interact with the automatic diagnosis system. The exemplars are generated via an adversarial autoencoder. We illustrate the behavior of the system on representative examples.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/iscc53001.2021.9631485" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="MBG2021"></div>
            <div class="col-lg-1 text-right">
              <h4>38.</h4><small>[MBG2021]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-thirty-seven"></div><strong>Explainable Deep Image Classifiers for Skin Lesion Diagnosis</strong><br><em>Metta Carlo, Beretta Andrea, Guidotti Riccardo, Yin Yuan, Gallinari Patrick, Rinzivillo Salvatore, Giannotti Fosca</em> (2021) - Arxive preprint. In International Journal of Data Science and Analytics
              <div class="collapse" id="collapse-thirty-seven" aria-labelledby="heading-thirty-seven" data-parent="#accordion-thirty-seven">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">A key issue in critical contexts such as medical diagnosis is the interpretability of the deep learning models adopted in decision-making systems. Research in eXplainable Artificial Intelligence (XAI) is trying to solve this issue. However, often XAI approaches are only tested on generalist classifier and do not represent realistic problems such as those of medical diagnosis. In this paper, we analyze a case study on skin lesion images where we customize an existing XAI approach for explaining a deep learning model able to recognize different types of skin lesions. The explanation is formed by synthetic exemplar and counter-exemplar images of skin lesion and offers the practitioner a way to highlight the crucial traits responsible for the classification decision. A survey conducted with domain experts, beginners and unskilled people proof that the usage of explanations increases the trust and confidence in the automatic decision system. Also, an analysis of the latent space adopted by the explainer unveils that some of the most frequent skin lesion classes are distinctly separated. This phenomenon could derive from the intrinsic characteristics of each class and, hopefully, can provide support in the resolution of the most frequent misclassifications by human experts.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-seven" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/2111.11863" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3▪4</strong></small></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- Footer-->
    <footer class="site-footer">
      <div class="footer-widgets">
        <div class="container">
          <div class="row gx-lg-5">
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-1">
                <h4 class="text-yellow mb-3">Principal Investigator</h4>
                <div class="text-white">
                  <p class="mb-0">Fosca Giannotti</p>
                  <p class="mb-0">Scuola Normale Superiore</p><br>
                  <p class="mb-0">Piazza dei Cavalieri, 7</p>
                  <p class="mb-0">56126 Pisa, Italy</p><br>
                  <p class="mb-0">Email: fosca.giannotti @ sns.it</p>
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-4">
                <div class="text-white">
                  <p>The XAI Project receives funding from the European Union's Horizon 2020 Excellent Science European Research Council (ERC) programme under grant agreement No. 834756</p>The views and opinions expressed in this website are the sole responsibility of the author and do not necessarily reflect the views of the European Commission.
                  <p></p><img class="mb-4" src="assets/images/logo-eu.jpg" alt="EU flag">
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-2">
                <ul>
                  <li><a href="index.html">Xai</a></li>
                  <li><a href="about.html">Project details</a></li>
                  <li><a href="research-lines.html">Research lines</a></li>
                  <li><a href="people.html">People</a></li>
                  <li><a href="resources.html">Resources</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="footer-bottom-area">
        <div class="container">
          <div class="row gx-lg-5 align-items-center">
            <div class="col-md-12"><span class="text-yellow strong">Webdesign: Daniele Fadda © 2023</span></div>
          </div>
        </div>
      </div>
    </footer>
    <!-- End Footer-->
    <!-- javascript files-->
    <!-- jquery-->
    <script src="assets/js/jquery.min.js"></script>
    <!-- lozad js-->
    <script src="assets/js/lozad.min.js"></script>
    <!-- Bootstrap js-->
    <script src="assets/js/bootstrap.bundle.min.js"></script>
    <!-- Aos js-->
    <script src="assets/js/aos.js"></script>
    <!-- Slick flickity js-->
    <script src="assets/js/flickity.pkgd.min.js"></script>
    <!-- Magnific popup js-->
    <script src="assets/js/jquery.magnific-popup.min.js"></script>
    <!-- Countdown js-->
    <script src="assets/js/jquery.countdown.js"></script>
    <!-- CountTo js-->
    <script src="assets/js/jquery.countTo.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/global.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/cookies.js"></script>
  </body>
</html>