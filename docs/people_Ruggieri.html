<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- favicon-->
    <link rel="shortcut icon" href="assets/images/favicon.ico">
    <!-- Site Title-->
    <title>Xai - Website</title>
    <meta name="description" content="Science and technology for the eXplanation of AI decision making">
    <!-- Bootstrap CSS file-->
    <link href="assets/css/bootstrap.min.css" rel="stylesheet">
    <!-- Flickity CSS file-->
    <link href="assets/css/flickity.min.css" rel="stylesheet">
    <!-- Main CSS file-->
    <link href="assets/css/style.css" rel="stylesheet">
    <!-- Fontawesome 5 CSS file-->
    <link href="assets/css/fontawesome-all.min.css" rel="stylesheet">
    <!-- Magnific Popup CSS-->
    <link href="assets/css/magnific-popup.css" rel="stylesheet">
    <!-- Google Fonts-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;700&amp;display=swap">
    <script type="text/javascript">
      // Matomo Code
      var _paq = window._paq = window._paq || [];
      /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
      _paq.push(['trackPageView']);
      _paq.push(['enableLinkTracking']);
      (function () {
      var u="//piwikdd.isti.cnr.it/";
      _paq.push(['setTrackerUrl', u+'piwik.php']);
      _paq.push(['setSiteId', '12']);
      var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
      g.type='text/javascript'; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
      })
      ();
    </script>
  </head>
  <body>
    <!-- Navbar-->
    <div class="site-header header-bottom-area">
      <nav class="navbar navbar-expand-lg navbar-light sticky">
        <div class="container"><a class="navbar-brand" href="index.html"><img class="site-logo" src="assets/images/logo/xai_logo_color.png" alt="Logo"></a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
          <div class="collapse navbar-collapse" id="navbarNavDropdown">
            <ul class="navbar-nav mx-auto">
              <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
              <li class="nav-item"><a class="nav-link" href="about.html">Project details</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="research-lines.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research Lines</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="line_1.html">1. Local to global</a><a class="dropdown-item" href="line_2.html">2. Casual reasoning</a><a class="dropdown-item" href="line_3.html">3. Platform and XUI</a><a class="dropdown-item" href="line_4.html">4. Case studies</a><a class="dropdown-item" href="line_5.html">5. Ethics and legal</a></div>
              </li>
              <li class="nav-item"><a class="nav-link" href="people.html">People</a></li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="resources.html" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications and Resources</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="resources.html#publications">Publications</a><a class="dropdown-item" href="resources.html#thesis">Thesis</a><a class="dropdown-item" href="dissemination.html">Dissemination tools</a></div>
              </li>
              <li class="nav-item dropdown"><a class="dropdown-toggle nav-link" href="#" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">News</a>
                <div class="dropdown-menu"><a class="dropdown-item" href="news.html">All News</a><a class="dropdown-item" href="dist-seminars.html">Distinguished seminars</a><a class="dropdown-item" href="internal-events.html">Internal Events</a></div>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </div>
    <!-- End Navbar-->
    <!-- Cookies-->
    <div class="cookie-banner" style="display: none">
      <p class="mx-2">We use cookies on this site to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. 
        <u><a href="privacy.html" link’="privacy.html">No, give me more info.</a></u>
      </p>
      <button class="close">&times;</button>
    </div>
    <!-- End Cookies-->
    <article class="entry"></article>
    <div class="entry-content">
      <div class="bg-yellow pt-6 pt-lg-8 pb-4 pb-lg-5">
        <div class="bg-half">
          <div class="container">
            <div class="row">
              <div class="col-lg-4 offset-lg-4">
                <h1 class="text-center">Salvatore Ruggieri</h1><img class="card-img" src="assets/images/p_Ruggieri.jpg" alt="Salvatore Ruggieri"/>
                <hr/>
                <p class="text-uppercase">Research Line 2 - leader</p>
                <hr/>
                <div class="member-content">
                  <div class="member-text px-0 py-3">
                    <p class="text-uppercase">Involved in the research line <strong>1 ▪ 2</strong></p>
                    <p>Role: Full Professor</p>
                    <p class="mb-0">Affiliation: University of Pisa</p>
                    <hr/>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="container mt-6">
        <div class="row justify-content-lg-center">
          <div class="col-lg-8"></div>
        </div>
      </div>
      <div class="container mb-6">
        <div class="row">
          <div class="row mt-5 justify-content-center" id="GMR2018">
            <div class="col-lg-1 text-right">
              <h4>1.</h4><small>[GMR2018]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-zero"></div><strong>A Survey of Methods for Explaining Black Box Models</strong><br><em>Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco, Giannotti Fosca, Pedreschi Dino</em> (2022) - ACM Computing Surveys. In ACM computing surveys (CSUR), 51(5), 1-42.
              <div class="collapse" id="collapse-zero" aria-labelledby="heading-zero" data-parent="#accordion-zero">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-zero" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1145/3236009" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪3</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GMG2019"></div>
            <div class="col-lg-1 text-right">
              <h4>2.</h4><small>[GMG2019]</small>
            </div>
            <div class="col-lg-2 pl-0"><img class="mr-3 border border-secondary bwc-image" src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine" style="width:100%;" data-toggle="modal" data-target="#modal-one" type="button">
              <div class="modal fade" id="modal-one" tabindex="-1" role="dialog" aria-labelledby="#modal-one-Title" aria-hidden="true">
                <div class="modal-dialog modal-lg modal-dialog-centered" role="document">
                  <div class="modal-content">
                    <div class="modal-header">
                      <p class="small">Factual and Counterfactual Explanations for Black Box Decision Making</p>
                      <button class="close" type="button" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
                    </div>
                    <div class="modal-body"><img src="assets/images/publications/03_factual_and_counterfactual.jpg " alt="immagine"></div>
                  </div>
                </div>
              </div>
            </div>
            <div class="col-lg-6 bg-yellow p-3">
              <div class="accordion" id="accordion-one"></div><strong>Factual and Counterfactual Explanations for Black Box Decision Making</strong><br><em>Guidotti Riccardo, Monreale Anna, Giannotti Fosca, Pedreschi Dino, Ruggieri Salvatore, Turini Franco</em> (2021) - IEEE Intelligent Systems. In IEEE Intelligent Systems
              <div class="collapse" id="collapse-one" aria-labelledby="heading-one" data-parent="#accordion-one">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-one" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1109/mis.2019.2957223" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1▪4</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GMR2018a"></div>
            <div class="col-lg-1 text-right">
              <h4>4.</h4><small>[GMR2018a]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-three"></div><strong>Local Rule-Based Explanations of Black Box Decision Systems</strong><br><em>Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore , Pedreschi Dino, Turini Franco , Giannotti Fosca</em> (2018) - Arxive preprint
              <div class="collapse" id="collapse-three" aria-labelledby="heading-three" data-parent="#accordion-three">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of achine learning components in socially sensitive and safety-critical contexts. Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/1805.10820" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="NPR2022"></div>
            <div class="col-lg-1 text-right">
              <h4>6.</h4><small>[NPR2022]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-five"></div><strong>Methods and tools for causal discovery and causal inference</strong><br><em>Ana Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi, João Gama</em> (2022) - Wires Data Mining and Knowledge Discovery. In Wires Data Mining and Knowledge Discovery
              <div class="collapse" id="collapse-five" aria-labelledby="heading-five" data-parent="#accordion-five">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation-based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href=" https://doi.org/10.1002/widm.1449" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>2</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="GR2021"></div>
            <div class="col-lg-1 text-right">
              <h4>26.</h4><small>[GR2021]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-twenty-five"></div><strong>Ensemble of Counterfactual Explainers</strong><br><em>Guidotti Riccardo, Ruggieri Salvatore</em> (2021)
              <div class="collapse" id="collapse-twenty-five" aria-labelledby="heading-twenty-five" data-parent="#accordion-twenty-five">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">In eXplainable Artificial Intelligence (XAI), several counterfactual explainers have been proposed, each focusing on some desirable properties of counterfactual instances: minimality, actionability, stability, diversity, plausibility, discriminative power. We propose an ensemble of counterfactual explainers that boosts weak explainers, which provide only a subset of such properties, to a powerful method covering all of them. The ensemble runs weak explainers on a sample of instances and of features, and it combines their results by exploiting a diversity-driven selection function. The method is model-agnostic and, through a wrapping approach based on autoencoders, it is also data-agnostic</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-twenty-five" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://pages.di.unipi.it/ruggieri/Papers/ds2021.pdf" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PGG2019"></div>
            <div class="col-lg-1 text-right">
              <h4>37.</h4><small>[PGG2019]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-thirty-six"></div><strong>Meaningful Explanations of Black Box AI Decision Systems</strong><br><em>Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco</em> (2021) - Proceedings of the AAAI Conference on Artificial Intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence
              <div class="collapse" id="collapse-thirty-six" aria-labelledby="heading-thirty-six" data-parent="#accordion-thirty-six">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Black box AI systems for automated decision making, often based on machine learning over (big) data, map a user’s features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases inherited by the algorithms from human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We focus on the urgent open challenge of how to construct meaningful explanations of opaque AI/ML systems, introducing the local-toglobal framework for black box explanation, articulated along three lines: (i) the language for expressing explanations in terms of logic rules, with statistical and causal interpretation; (ii) the inference of local explanations for revealing the decision rationale for a specific case, by auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of many local explanations into simple global ones, with algorithms that optimize for quality and comprehensibility. We argue that the local-first approach opens the door to a wide variety of alternative solutions along different dimensions: a variety of data sources (relational, text, images, etc.), a variety of learning problems (multi-label classification, regression, scoring, ranking), a variety of languages for expressing meaningful explanations, a variety of means to audit a black box.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-six" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1609/aaai.v33i01.33019780" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="LGR2020"></div>
            <div class="col-lg-1 text-right">
              <h4>39.</h4><small>[LGR2020]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-thirty-eight"></div><strong>Explaining Sentiment Classification with Synthetic Exemplars and Counter-Exemplars</strong><br><em>Lampridis Orestis, Guidotti Riccardo, Ruggieri Salvatore</em> (2021) - Discovery Science. In In International Conference on Discovery Science (pp. 357-373). Springer, Cham.
              <div class="collapse" id="collapse-thirty-eight" aria-labelledby="heading-thirty-eight" data-parent="#accordion-thirty-eight">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">We present xspells, a model-agnostic local approach for explaining the decisions of a black box model for sentiment classification of short texts. The explanations provided consist of a set of exemplar sentences and a set of counter-exemplar sentences. The former are examples classified by the black box with the same label as the text to explain. The latter are examples classified with a different label (a form of counter-factuals). Both are close in meaning to the text to explain, and both are meaningful sentences – albeit they are synthetically generated. xspells generates neighbors of the text to explain in a latent space using Variational Autoencoders for encoding text and decoding latent instances. A decision tree is learned from randomly generated neighbors, and used to drive the selection of the exemplars and counter-exemplars. We report experiments on two datasets showing that xspells outperforms the well-known lime method in terms of quality of explanations, fidelity, and usefulness, and that is comparable to it in terms of stability.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-thirty-eight" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://dx.doi.org/10.1007/978-3-030-61527-7_24" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="RGG2020"></div>
            <div class="col-lg-1 text-right">
              <h4>44.</h4><small>[RGG2020]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-forty-three"></div><strong>Opening the black box: a primer for anti-discrimination</strong><br><em>Ruggieri Salvatore, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Pedreschi Dino, Turini Franco</em> (2020). In ANNUARIO DI DIRITTO COMPARATO E DI STUDI LEGISLATIVI
              <div class="collapse" id="collapse-forty-three" aria-labelledby="heading-forty-three" data-parent="#accordion-forty-three">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">The pervasive adoption of Artificial Intelligence (AI) models in the modern information society, requires counterbalancing the growing decision power demanded to AI models with risk assessment methodologies. In this paper, we consider the risk of discriminatory decisions and review approaches for discovering discrimination and for designing fair AI models. We highlight the tight relations between discrimination discovery and explainable AI, with the latter being a more general approach for understanding the behavior of black boxes.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-three" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="http://hdl.handle.net/11568/1088440" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
            <div class="row mt-5 justify-content-center" id="PGG2018"></div>
            <div class="col-lg-1 text-right">
              <h4>50.</h4><small>[PGG2018]</small>
            </div>
            <div class="col-lg-8 bg-yellow p-3">
              <div class="accordion" id="accordion-forty-nine"></div><strong>Open the Black Box Data-Driven Explanation of Black Box Decision Systems</strong><br><em>Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna , Pappalardo Luca , Ruggieri Salvatore , Turini Franco </em> (2018) - Arxive preprint
              <div class="collapse" id="collapse-forty-nine" aria-labelledby="heading-forty-nine" data-parent="#accordion-forty-nine">
                <div class="bg-yellow">
                  <hr>
                  <p class="small"><strong>Abstract</strong></p>
                  <p class="small">Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.</p>
                </div>
              </div>
            </div>
            <div class="col-lg-2 pl-3">
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="#" data-toggle="collapse" data-target="#collapse-forty-nine" aria-expanded="true" aria-controls="collapseAbs">More information</a></p>
              <p class="my-1"><a class="btn-mini px-2 btn-secondary small" href="https://arxiv.org/abs/1806.09936" target="_blank">External Link</a></p>
              <p class="my-1"><small>Research Line <strong>1</strong></small></p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- Footer-->
    <footer class="site-footer">
      <div class="footer-widgets">
        <div class="container">
          <div class="row gx-lg-5">
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-1">
                <h4 class="text-yellow mb-3">Principal Investigator</h4>
                <div class="text-white">
                  <p class="mb-0">Fosca Giannotti</p>
                  <p class="mb-0">Scuola Normale Superiore</p><br>
                  <p class="mb-0">Piazza dei Cavalieri, 7</p>
                  <p class="mb-0">56126 Pisa, Italy</p><br>
                  <p class="mb-0">Email: fosca.giannotti @ sns.it</p>
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-4">
                <div class="text-white">
                  <p>The XAI Project receives funding from the European Union's Horizon 2020 Excellent Science European Research Council (ERC) programme under grant agreement No. 834756</p>The views and opinions expressed in this website are the sole responsibility of the author and do not necessarily reflect the views of the European Commission.
                  <p></p><img class="mb-4" src="assets/images/logo-eu.jpg" alt="EU flag">
                </div>
              </div>
            </div>
            <div class="col-lg-4">
              <div class="footer-widget footer-widget-2">
                <ul>
                  <li><a href="index.html">Xai</a></li>
                  <li><a href="about.html">Project details</a></li>
                  <li><a href="research-lines.html">Research lines</a></li>
                  <li><a href="people.html">People</a></li>
                  <li><a href="resources.html">Resources</a></li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="footer-bottom-area">
        <div class="container">
          <div class="row gx-lg-5 align-items-center">
            <div class="col-md-12"><span class="text-yellow strong">Webdesign: Daniele Fadda © 2022</span></div>
          </div>
        </div>
      </div>
    </footer>
    <!-- End Footer-->
    <!-- javascript files-->
    <!-- jquery-->
    <script src="assets/js/jquery.min.js"></script>
    <!-- lozad js-->
    <script src="assets/js/lozad.min.js"></script>
    <!-- Bootstrap js-->
    <script src="assets/js/bootstrap.bundle.min.js"></script>
    <!-- Aos js-->
    <script src="assets/js/aos.js"></script>
    <!-- Slick flickity js-->
    <script src="assets/js/flickity.pkgd.min.js"></script>
    <!-- Magnific popup js-->
    <script src="assets/js/jquery.magnific-popup.min.js"></script>
    <!-- Countdown js-->
    <script src="assets/js/jquery.countdown.js"></script>
    <!-- CountTo js-->
    <script src="assets/js/jquery.countTo.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/global.js"></script>
    <!-- Global - Main js-->
    <script src="assets/js/cookies.js"></script>
  </body>
</html>