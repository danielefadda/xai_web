extends includes/layout
include includes/mixin

block content
  article.entry
    .entry-content
      //- Page title
      // Hero
      .bg-yellow
        .container.py-lg-8.py-md-7.py-5
          .row
            .col-lg-6
              h2.my-3 Distinguished seminars on Explainable AI
              p.lead.mb-3 The Distinguished seminars on Explainable AI last for 90 minutes, the first 45 are dedicated to the seminar and the rest to a round table, we will allow some other guests to be able to ask questions to deepen the topic or opening our minds. Our goal is to bring together bright minds to give talks that are focused on various aspects that can affect Explainability and Artificial Intelligence to foster learning and inspirations that matter.
      // calendar
      .container.mt-lg-5.mt-3.mb-lg-5.mb-md-5.mb-3
        .row.mb-5
          .col-lg-2.mb-2.mb-lg-0
            a(href='#')
              .bg-yellow.h-100
                .text-center.align-items-center
                  .px-lg-4.py-lg-2.px-4.py-2
                    .mt-lg-4.mt-5
                      h1.display-1.text >
                      h3.text-uppercase dates
                    .px-lg-2.mt-lg-4.px-2.mt-3
                      p our guests
          .col-lg-2.mb-2.mb-lg-0
            a(href='#cynthia')
              .bg-gray-500.text-white.h-100
                .text-center.align-items-center
                  .px-lg-4.py-lg-2.px-4.py-2
                    .mt-lg-4.mt-5
                      h1.display-1 20
                      h3.text-uppercase April
                    .px-lg-2.mt-lg-4.px-2.mt-3
                      p Cynthia Rudin
          .col-lg-2.mb-2.mb-lg-0
            a(href='#biecek')
              .bg-gray-500.text-white.h-100
                .text-center.align-items-center
                  .px-lg-4.py-lg-2.px-4.py-2
                    .mt-lg-4.mt-5
                      h1.display-1 25
                      h3.text-uppercase May
                    .px-lg-2.mt-lg-4.px-2.mt-3
                      p Przemek Biecek
          .col-lg-2.mb-2.mb-lg-0
            a(href='#byrne')
              .bg-gray-500.text-white.h-100
                .text-center.align-items-center
                  .px-lg-4.py-lg-2.px-4.py-2
                    .mt-lg-4.mt-5
                      h1.display-1 15
                      h3.text-uppercase June
                    .px-lg-2.mt-lg-4.px-2.mt-3
                      p Ruth Byrne
          .col-lg-2.mb-2.mb-lg-0
            a(href='#lecue')
              .bg-gray-500.text-white.h-100
                .text-center.align-items-center
                  .px-lg-4.py-lg-2.px-4.py-2
                    .mt-lg-4.mt-5
                      h1.display-1 13
                      h3.text-uppercase July
                    .px-lg-2.mt-lg-4.px-2.mt-3
                      p Freddy Lecue
          .col-lg-2.d-none.d-lg-block
            .bg-grayphite.h-100



      .container.mt-lg-5.mt-md-6.mt-5
        // RUDIN SECTION //
        hr#cynthia
        .row.pt-9
          .col-lg-8
            img.img-fluid.mb-lg-5.mb-md-5.mb-4(src="assets/images/seminars/seminar_01_rudin.png" alt="Cynthia Rudin")
            #accordionCynthia.accordion
              .card.border-0
                #headingOne.card-header.bg-yellow
                  a.h5.mb-0.py-lg-4.px-lg-4.py-md-4.px-md-4.py-2.px-2.d-block(href='#' data-toggle='collapse' data-target='#collapseOneCynthia' aria-expanded='false' aria-controls='collapseOne')
                    | More on this talk
                #collapseOneCynthia.collapse(aria-labelledby='headingOne' data-parent='#accordionCynthia')
                  .py-lg-2.px-lg-2.bg-grayphite
                    .card-body.mt-2
                      p Let us consider a difficult computer vision challenge. Would you want an algorithm to determine whether you should get a biopsy, based on an xray? That's usually a decision made by a radiologist, based on years of training. We know that algorithms haven't worked perfectly for a multitude of other computer vision applications, and biopsy decisions are harder than just about any other application of computer vision that we typically consider. The interesting question is whether it is possible that an algorithm could be a true partner to a physician, rather than making the decision on its own. To do this, at the very least, we would need an interpretable neural network that is as accurate as its black box counterparts.
                      p This talk will discuss two approaches to interpretable neural networks: (1) case-based reasoning, where parts of images are compared to other parts of prototypical images for each class, and (2) neural disentanglement, using a technique called concept whitening. The case-based reasoning technique is strictly better than saliency maps, and the concept whitening technique provides a strict advantage over the posthoc use of concept vectors.
              .card.border-0.mt-lg-5.mt-4
                #headingTwoCynthia.card-header.bg-yellow
                  a.h5.mb-0.py-lg-4.px-lg-4.py-md-4.px-md-4.py-2.px-2.d-block(href='#' data-toggle='collapse' data-target='#collapseTwoCynthia' aria-expanded='false' aria-controls='collapseTwo')
                    | Cynthia Rudin - short bio
                #collapseTwoCynthia.collapse(aria-labelledby='headingTwoCynthia' data-parent='#accordionCynthia')
                  .py-lg-2.px-lg-2.bg-grayphite
                    .card-body.mt-2
                      p Cynthia Rudin is a professor of computer science, electrical and computer engineering, and statistical science at Duke University, and directs the Prediction Analysis Lab, whose main focus is in interpretable machine learning. She is also an associate director of the Statistical and Applied Mathematical Sciences Institute (SAMSI). Previously, Prof. Rudin held positions at MIT, Columbia, and NYU. She holds an undergraduate degree from the University at Buffalo, and a PhD from Princeton University.
              .card.border-0.mt-lg-5.mt-4

          .col-lg-4
            h2 Cynthia Rudin
            p.text-gray-500 Apr 20, 5.00pm - 6.30pm CEST
            h5 Interpretable Neural Networks for Computer Vision: Clinical Decisions that are Computer-Aided, not Automated
            hr
            p The talk will face the topic of clinical decision-making and interpretable deep neural networks. The main question is whether it is possible that an algorithm could be a true partner to a physician, rather than making the decision on its own. Two approaches will be discussed 1) case-based reasoning and 2) neural disentanglement, covering the advantages of a technique called concept whitening.
            hr
            ul.list-unstyled
              li #[strong Chair] -  Fosca Giannotti
              li #[strong Discussants]  - Riccardo Guidotti, Luca Pappalardo, Cecilia Panigutti
            hr
            a(href='https://twitter.com/cynthiarudin' target='_blank')
              i.m-2.fab.fa-2x.fa-twitter
              | follow on twitter
        // BIECEK SECTION
        #biecek
        .row.pt-9.pb-sm-10
          .col-lg-8
            img.img-fluid.mb-lg-5.mb-md-5.mb-4(src="assets/images/seminars/seminar_02_prez.png" alt="Przemek Biecek")
            #accordionExample.accordion
              .card.border-0.mt-2
                #headingTwo.card-header.bg-yellow
                  a.h5.mb-0.py-lg-4.px-lg-4.py-md-4.px-md-4.py-2.px-2.d-block(href='#' data-toggle='collapse' data-target='#collapseTwo' aria-expanded='false' aria-controls='collapseTwo')
                    | Przemyslaw Biecek - short bio
                #collapseTwo.collapse(aria-labelledby='headingTwo' data-parent='#accordionExample')
                  .py-lg-2.px-lg-2.bg-grayphite
                    .card-body.mt-2
                      p Dr Biecek is interested in methodology (explainable artificial intelligence) and applications (computational medicine/biology) of machine learning. He works as associate professor at Warsaw University of Technology. His background is in mathematical statistics and software engineering. He leads group MI2DataLab of mathematicians and computer scientists that love to play with data
              .card.border-0.mt-lg-5.mt-4

          .col-lg-4
            h2 Przemyslaw Biecek
            p.text-gray-500 May 25, 5.00pm - 6.30pm CEST
            h5 Explanatory Model Analysis
            hr
            p Explanatory Model Analysis Explore, Explain and Examine Predictive Models is a set of methods and tools designed to build better predictive models and to monitor their behaviour in a changing environment. Today, the true bottleneck in predictive modelling is neither the lack of data, nor the lack of computational power, nor inadequate algorithms, nor the lack of flexible models.
            p It is the lack of tools for model exploration (extraction of relationships learned by the model), model explanation (understanding the key factors influencing model decisions) and model examination (identification of model weaknesses and evaluation of model's performance). This book presents a collection of model agnostic methods that may be used for any black-box model together with real-world applications to classification and regression problems.
            //hr
            //ul.list-unstyled
            //  li #[strong Chair] -  Fosca Giannotti
            //  //- li #[strong Discussants]  - Riccardo Guidotti, Luca Pappalardo, Cecilia Panigutti
            //hr
            //a(href='https://twitter.com/freddylecue' target='_blank')
            //  i.m-2.fab.fa-2x.fa-twitter
            //  | follow on twitter
        // Byrne SECTION
        #byrne
        .row.pt-9.pb-sm-10
          .col-lg-8
            img.img-fluid.mb-lg-5.mb-md-5.mb-4(src="assets/images/seminars/seminar_03_Ruth.png" alt="Ruth Byrne")
            #accordionExample.accordion
              .card.border-0
                #headingTwo.card-header.bg-yellow
                  a.h5.mb-0.py-lg-4.px-lg-4.py-md-4.px-md-4.py-2.px-2.d-block(href='#' data-toggle='collapse' data-target='#collapseTwo' aria-expanded='false' aria-controls='collapseTwo')
                    | Ruth Byrne - short bio
                #collapseTwo.collapse(aria-labelledby='headingTwo' data-parent='#accordionExample')
                  .py-lg-2.px-lg-2.bg-grayphite
                    .card-body.mt-2
                      p "Ruth Byrne is the Professor of Cognitive Science at Trinity College Dublin, University of Dublin, in the School of Psychology and the Institute of Neuroscience, a Chair created for her by the University in 2005.
                      p Her research expertise is in the cognitive science of human thinking, including experimental and computational investigations of reasoning and imaginative thought. Her most recent book is a co-edited volume with Kinga Morsanyi on 'Thinking, Reasoning, and Decision-making in Autism' (2019, Routledge). She has also written 'The Rational Imagination: How People Create Alternatives to Reality' published in 2005 by MIT press (and selected for open peer commentary by the Behavioral and Brain Sciences journal in 2007), and 'Deduction', co-authored with Phil Johnson-Laird, published in 1991 by Erlbaum Associates. She has published over 100 articles in journals such as Annual Review of Psychology, Cognition, Cognitive Psychology, Cognitive Science, Current Directions in Psychological Science, Psychological Review, and Trends in Cognitive Sciences."
              .card.border-0.mt-lg-5.mt-4

          .col-lg-4
            h2 Ruth Byrne
            p.text-gray-500 June 15, 5.00pm - 6.30pm CEST
            h5 Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning.
            hr
            p Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI). Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.
            //hr
            //ul.list-unstyled
            //  li #[strong Chair] -  Fosca Giannotti
            //  //- li #[strong Discussants]  - Riccardo Guidotti, Luca Pappalardo, Cecilia Panigutti
            //hr
            //a(href='https://twitter.com/freddylecue' target='_blank')
            //  i.m-2.fab.fa-2x.fa-twitter
            //  | follow on twitter
        // Lecue SECTION //
        #lecue
        .row.pt-9.pb-sm-10
          .col-lg-8
            img.img-fluid.mb-lg-5.mb-md-5.mb-4(src="assets/images/seminars/seminar_02_Lecue.png" alt="Freddy Lecue")
            #accordionExample.accordion
              .card.border-0
                #headingOne.card-header.bg-yellow
                  a.h5.mb-0.py-lg-4.px-lg-4.py-md-4.px-md-4.py-2.px-2.d-block(href='#' data-toggle='collapse' data-target='#collapseOne' aria-expanded='true' aria-controls='collapseOne')
                    | More on this talk
                #collapseOne.collapse(aria-labelledby='headingOne' data-parent='#accordionExample')
                  .py-lg-2.px-lg-2.bg-grayphite
                    .card-body.mt-2
                      p As artificial intelligence has become tightly intervened in the society having tangible consequences and influence, calls for explainability and interpretability of these systems has also become increasingly prevalent. Explainable AI (XAI) attempts to alleviate concerns of transparency, trust and ethics in AI by making them accountable, interpretable and explainable to humans. This workshop aims to encapsulate these concepts under the umbrella of Explainable Agency and bring together researchers and practitioners working in different facets of explainable AI from diverse backgrounds to share challenges, new directions and recent research in the field. We especially welcome research from fields including but not limited to artificial intelligence, human-computer interaction, human-robot interaction, cognitive science, human factors and philosophy.
              .card.border-0.mt-lg-5.mt-4
                #headingTwo.card-header.bg-yellow
                  a.h5.mb-0.py-lg-4.px-lg-4.py-md-4.px-md-4.py-2.px-2.d-block(href='#' data-toggle='collapse' data-target='#collapseTwo' aria-expanded='false' aria-controls='collapseTwo')
                    | Freddy Lecue - short bio
                #collapseTwo.collapse(aria-labelledby='headingTwo' data-parent='#accordionExample')
                  .py-lg-2.px-lg-2.bg-grayphite
                    .card-body.mt-2
                      p Dr Freddy Lecue (PhD 2008, Habilitation 2015) is the Chief Artificial Intelligence (AI) Scientist at CortAIx (Centre of Research & Technology in Artificial Intelligence eXpertise) @Thales in Montreal, Canada. He is also a research associate at Inria, in WIMMICS, Sophia Antipolis - France.
                      p His research area is at the frontier of intelligent / learning / reasoning systems, and Internet of Things.
              .card.border-0.mt-lg-5.mt-4

          .col-lg-4
            h2 Freddy Lecue
            p.text-gray-500 July 13, 5.00pm - 6.30pm CEST
            h5 The role of Knowledge Graph
            hr
            p Explainable AI (XAI) attempts to alleviate concerns of transparency, trust and ethics in AI by making them accountable, interpretable and explainable to humans. This seminars aims to encapsulate these concepts under the umbrella of Explainable Agency and bring together researchers and practitioners working in different facets of explainable AI from diverse backgrounds to share challenges, new directions and recent research in the field.
            hr
            ul.list-unstyled
              li #[strong Chair] -  Fosca Giannotti
              //- li #[strong Discussants]  - Riccardo Guidotti, Luca Pappalardo, Cecilia Panigutti
            hr
            a(href='https://twitter.com/freddylecue' target='_blank')
              i.m-2.fab.fa-2x.fa-twitter
              | follow on twitter




