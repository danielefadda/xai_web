.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#NPR2022.anchor 6.
        small [NPR2022]
    .col-lg-8.bg-yellow.p-3
        #accordion-five.accordion
        | #[strong Methods and tools for causal discovery and causal inference]
        br
        | #[em Ana Rita Nogueira, Andrea Pugnana, Salvatore Ruggieri, Dino Pedreschi, João Gama] (2022) - Wires Data Mining and Knowledge Discovery. In Wires Data Mining and Knowledge Discovery
        #collapse-five.collapse(aria-labelledby='heading-five' data-parent='#accordion-five')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation-based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-five' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href=' https://doi.org/10.1002/widm.1449', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 2]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#SMM2022.anchor 14.
        small [SMM2022]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirteen.accordion
        | #[strong TriplEx: Triple Extraction for Explanation]
        br
        | #[em Setzu Mattia, Monreale Anna, Minervini Pasquale] (2022) - Third Conference on Cognitive Machine Intelligence (COGMI) 2021
        #collapse-thirteen.collapse(aria-labelledby='heading-thirteen' data-parent='#accordion-thirteen')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small nan
    .col-lg-2.pl-3
        
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='10.1109/CogMI52975.2021.00015', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪2]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#BGM2021.anchor 28.
        small [BGM2021]
    .col-lg-8.bg-yellow.p-3
        #accordion-twenty-seven.accordion
        | #[strong Deriving a Single Interpretable Model by Merging Tree-Based Classifiers]
        br
        | #[em Bonsignori Valerio, Guidotti Riccardo, Monreale Anna] (2021) - Discovery Science
        #collapse-twenty-seven.collapse(aria-labelledby='heading-twenty-seven' data-parent='#accordion-twenty-seven')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Decision tree classifiers have been proved to be among the most interpretable models due to their intuitive structure that illustrates decision processes in form of logical rules. Unfortunately, more complex tree-based classifiers such as oblique trees and random forests overcome the accuracy of decision trees at the cost of becoming non interpretable. In this paper, we propose a method that takes as input any tree-based classifier and returns a single decision tree able to approximate its behavior. Our proposal merges tree-based classifiers by an intensional and extensional approach and applies a post-hoc explanation strategy. Our experiments shows that the retrieved single decision tree is at least as accurate as the original tree-based model, faithful, and more interpretable.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-twenty-seven' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1007/978-3-030-88942-5_27', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪2]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#GMP2019.anchor 49.
        small [GMP2019]
    .col-lg-8.bg-yellow.p-3
        #accordion-forty-eight.accordion
        | #[strong The AI black box explanation problem]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Pedreschi Dino] (2019) - ERCIM News, 116, 12-13. In ERCIM News, 116, 12-13
        #collapse-forty-eight.collapse(aria-labelledby='heading-forty-eight' data-parent='#accordion-forty-eight')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small nan
    .col-lg-2.pl-3
        
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://ercim-news.ercim.eu/images/stories/EN116/EN116-web.pdf#page=12', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪2▪3]
    