.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#GMR2018.anchor 1.
        small [GMR2018]
    .col-lg-8.bg-yellow.p-3
        #accordion-zero.accordion
        | #[strong A Survey of Methods for Explaining Black Box Models]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco, Giannotti Fosca, Pedreschi Dino] (2022) - ACM Computing Surveys. In ACM computing surveys (CSUR), 51(5), 1-42.
        #collapse-zero.collapse(aria-labelledby='heading-zero' data-parent='#accordion-zero')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-zero' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1145/3236009', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#BGG2023.anchor 5.
        small [BGG2023]
    .col-lg-8.bg-yellow.p-3
        #accordion-four.accordion
        | #[strong Benchmarking and survey of explanation methods for black box models]
        br
        | #[em Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, Salvatore Rinzivillo] (2023) - Springer Science+Business Media, LLC, part of Springer Nature. In Data Mining and Knowledge Discovery
        #collapse-four.collapse(aria-labelledby='heading-four' data-parent='#accordion-four')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-four' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://doi.org/10.1007/s10618-023-00933-9', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#MBG2023.anchor 6.
        small [MBG2023]
    .col-lg-8.bg-yellow.p-3
        #accordion-five.accordion
        | #[strong Improving trust and confidence in medical skin lesion diagnosis through explainable deep learning]
        br
        | #[em Carlo Metta, Andrea Beretta, Riccardo Guidotti, Yuan Yin, Patrick Gallinari, Salvatore Rinzivillo, Fosca Giannotti ] (2023) - Springer Nature. In International Journal of Data Science and Analytics
        #collapse-five.collapse(aria-labelledby='heading-five' data-parent='#accordion-five')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small A key issue in critical contexts such as medical diagnosis is the interpretability of the deep learning models adopted in decision-making systems. Research in eXplainable Artificial Intelligence (XAI) is trying to solve this issue. However, often XAI approaches are only tested on generalist classifier and do not represent realistic problems such as those of medical diagnosis. In this paper, we aim at improving the trust and confidence of users towards automatic AI decision systems in the field of medical skin lesion diagnosis by customizing an existing XAI approach for explaining an AI model able to recognize different types of skin lesions. The explanation is generated through the use of synthetic exemplar and counter-exemplar images of skin lesions and our contribution offers the practitioner a way to highlight the crucial traits responsible for the classification decision. A validation survey with domain experts, beginners, and unskilled people shows that the use of explanations improves trust and confidence in the automatic decision system. Also, an analysis of the latent space adopted by the explainer unveils that some of the most frequent skin lesion classes are distinctly separated. This phenomenon may stem from the intrinsic characteristics of each class and may help resolve common misclassifications made by human experts.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-five' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://doi.org/10.1007/s41060-023-00401-z', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#BRF2022.anchor 23.
        small [BRF2022]
    .col-lg-2.pl-0
        img(src='assets/images/publications/latent-space-exploration.png ' alt="immagine" style='width:100%;' data-toggle='modal' data-target='#modal-twenty-two' type='button').mr-3.border.border-secondary.bwc-image
        #modal-twenty-two.modal.fade(tabindex='-1' role='dialog' aria-labelledby='#modal-twenty-two-Title' aria-hidden='true')
            .modal-dialog.modal-lg.modal-dialog-centered(role='document')
                .modal-content
                    .modal-header
                        p.small Explaining Black Box with visual exploration of Latent Space
                        button.close(type='button' data-dismiss='modal' aria-label='Close')
                            span(aria-hidden='true') &times;
                    .modal-body
                        img(src='assets/images/publications/latent-space-exploration.png ' alt="immagine" )
    .col-lg-6.bg-yellow.p-3
        #accordion-twenty-two.accordion
        | #[strong Explaining Black Box with visual exploration of Latent Space]
        br
        | #[em Bodria Francesco, Rinzivillo Salvatore, Fadda Daniele, Guidotti Riccardo, Fosca Giannotti, Pedreschi Dino] (2022) - EUROVIS 2022. In Proceedings of the 2022 Conference Eurovis 2022
        #collapse-twenty-two.collapse(aria-labelledby='heading-twenty-two' data-parent='#accordion-twenty-two')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Autoencoders are a powerful yet opaque feature reduction technique, on top of which we propose a novel way for the joint visual exploration of both latent and real space. By interactively exploiting the mapping between latent and real features, it is possible to unveil the meaning of latent features while providing deeper insight into the original variables. To achieve this goal, we exploit and re-adapt existing approaches from eXplainable Artificial Intelligence (XAI) to understand the relationships between the input and latent features. The uncovered relationships between input features and latent ones allow the user to understand the data structure concerning external variables such as the predictions of a classification model. We developed an interactive framework that visually explores the latent space and allows the user to understand the relationships of the input features with model prediction.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-twenty-two' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://diglib.eg.org/handle/10.2312/evs20221098', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 3]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#PBF2022.anchor 24.
        small [PBF2022]
    .col-lg-8.bg-yellow.p-3
        #accordion-twenty-three.accordion
        | #[strong Co-design of human-centered, explainable AI for clinical decision support]
        br
        | #[em Panigutti Cecilia, Beretta Andrea, Fadda Daniele , Giannotti Fosca, Pedreschi Dino, Perotti Alan, Rinzivillo Salvatore] (2022). In ACM Transactions on Interactive Intelligent Systems
        #collapse-twenty-three.collapse(aria-labelledby='heading-twenty-three' data-parent='#accordion-twenty-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models, and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique, and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback, with a two-fold outcome: first, we obtain evidence that explanations increase users' trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so that we can re-design a better, more human-centered explanation interface.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-twenty-three' aria-expanded='true' aria-controls='collapseAbs') More information
        
        p.my-1
                    small Research Line #[strong 1▪3▪4]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#MBG2021.anchor 38.
        small [MBG2021]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirty-seven.accordion
        | #[strong Explainable Deep Image Classifiers for Skin Lesion Diagnosis]
        br
        | #[em Metta Carlo, Beretta Andrea, Guidotti Riccardo, Yin Yuan, Gallinari Patrick, Rinzivillo Salvatore, Giannotti Fosca] (2021) - Arxive preprint. In International Journal of Data Science and Analytics
        #collapse-thirty-seven.collapse(aria-labelledby='heading-thirty-seven' data-parent='#accordion-thirty-seven')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small A key issue in critical contexts such as medical diagnosis is the interpretability of the deep learning models adopted in decision-making systems. Research in eXplainable Artificial Intelligence (XAI) is trying to solve this issue. However, often XAI approaches are only tested on generalist classifier and do not represent realistic problems such as those of medical diagnosis. In this paper, we analyze a case study on skin lesion images where we customize an existing XAI approach for explaining a deep learning model able to recognize different types of skin lesions. The explanation is formed by synthetic exemplar and counter-exemplar images of skin lesion and offers the practitioner a way to highlight the crucial traits responsible for the classification decision. A survey conducted with domain experts, beginners and unskilled people proof that the usage of explanations increases the trust and confidence in the automatic decision system. Also, an analysis of the latent space adopted by the explainer unveils that some of the most frequent skin lesion classes are distinctly separated. This phenomenon could derive from the intrinsic characteristics of each class and, hopefully, can provide support in the resolution of the most frequent misclassifications by human experts.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirty-seven' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://arxiv.org/abs/2111.11863', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3▪4]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#PPP2020.anchor 54.
        small [PPP2020]
    .col-lg-2.pl-0
        img(src='assets/images/publications/Doctor XAI.jpg ' alt="immagine" style='width:100%;' data-toggle='modal' data-target='#modal-fifty-three' type='button').mr-3.border.border-secondary.bwc-image
        #modal-fifty-three.modal.fade(tabindex='-1' role='dialog' aria-labelledby='#modal-fifty-three-Title' aria-hidden='true')
            .modal-dialog.modal-dialog-centered(role='document')
                .modal-content
                    .modal-header
                        p.small Doctor XAI: an ontology-based approach to black-box sequential data classification explanations
                        button.close(type='button' data-dismiss='modal' aria-label='Close')
                            span(aria-hidden='true') &times;
                    .modal-body
                        img(src='assets/images/publications/Doctor XAI.jpg ' alt="immagine" )
    .col-lg-6.bg-yellow.p-3
        #accordion-fifty-three.accordion
        | #[strong Doctor XAI: an ontology-based approach to black-box sequential data classification explanations]
        br
        | #[em Panigutti Cecilia, Perotti Alan, Pedreschi Dino] (2020) - FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. In FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency
        #collapse-fifty-three.collapse(aria-labelledby='heading-fifty-three' data-parent='#accordion-fifty-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-fifty-three' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://dl.acm.org/doi/10.1145/3351095.3372855', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3▪4]
    .row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4#GMP2019.anchor 60.
        small [GMP2019]
    .col-lg-8.bg-yellow.p-3
        #accordion-fifty-nine.accordion
        | #[strong The AI black box explanation problem]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Pedreschi Dino] (2019) - ERCIM News, 116, 12-13. In ERCIM News, 116, 12-13
        #collapse-fifty-nine.collapse(aria-labelledby='heading-fifty-nine' data-parent='#accordion-fifty-nine')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small nan
    .col-lg-2.pl-3
        
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://ercim-news.ercim.eu/images/stories/EN116/EN116-web.pdf#page=12', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪2▪3]
    