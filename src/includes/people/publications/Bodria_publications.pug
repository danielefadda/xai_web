#BRF2022.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 11.
        small [BRF2022]
    .col-lg-2.pl-0
        img(src='assets/images/publications/latent-space-exploration.png ' alt="immagine" style='width:100%;' data-toggle='modal' data-target='#modal-ten' type='button').mr-3.border.border-secondary.bwc-image
        #modal-ten.modal.fade(tabindex='-1' role='dialog' aria-labelledby='#modal-ten-Title' aria-hidden='true')
            .modal-dialog.modal-lg.modal-dialog-centered(role='document')
                .modal-content
                    .modal-header
                        p.small Explaining Black Box with visual exploration of Latent Space
                        button.close(type='button' data-dismiss='modal' aria-label='Close')
                            span(aria-hidden='true') &times;
                    .modal-body
                        img(src='assets/images/publications/latent-space-exploration.png ' alt="immagine" )
    .col-lg-6.bg-yellow.p-3
        #accordion-ten.accordion
        | #[strong Explaining Black Box with visual exploration of Latent Space]
        br
        | #[em Bodria Francesco, Rinzivillo Salvatore, Fadda Daniele, Guidotti Riccardo, Fosca Giannotti, Pedreschi Dino] (2022) - EUROVIS 2022. In Proceedings of the 2022 Conference Eurovis 2022
        #collapse-ten.collapse(aria-labelledby='heading-ten' data-parent='#accordion-ten')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Autoencoders are a powerful yet opaque feature reduction technique, on top of which we propose a novel way for the joint visual exploration of both latent and real space. By interactively exploiting the mapping between latent and real features, it is possible to unveil the meaning of latent features while providing deeper insight into the original variables. To achieve this goal, we exploit and re-adapt existing approaches from eXplainable Artificial Intelligence (XAI) to understand the relationships between the input and latent features. The uncovered relationships between input features and latent ones allow the user to understand the data structure concerning external variables such as the predictions of a classification model. We developed an interactive framework that visually explores the latent space and allows the user to understand the relationships of the input features with model prediction.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-ten' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://diglib.eg.org/handle/10.2312/evs20221098', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 3]
    #BGG2021.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 20.
        small [BGG2021]
    .col-lg-8.bg-yellow.p-3
        #accordion-nineteen.accordion
        | #[strong Benchmarking and survey of explanation methods for black box models]
        br
        | #[em Bodria Francesco, Giannotti Fosca, Guidotti Riccardo, Naretto Francesca, Pedreschi Dino, Rinzivillo Salvatore] (2021)
        #collapse-nineteen.collapse(aria-labelledby='heading-nineteen' data-parent='#accordion-nineteen')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-nineteen' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://arxiv.org/abs/2102.13076', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #BPP2020.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 46.
        small [BPP2020]
    .col-lg-8.bg-yellow.p-3
        #accordion-forty-five.accordion
        | #[strong Explainability Methods for Natural Language Processing: Applications to Sentiment Analysis]
        br
        | #[em Bodria Francesco, Panisson Andr√© , Perotti Alan, Piaggesi Simone ] (2020) - Discussion Paper
        #collapse-forty-five.collapse(aria-labelledby='heading-forty-five' data-parent='#accordion-forty-five')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small nan
    .col-lg-2.pl-3
        
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://ceur-ws.org/Vol-2646/18-paper.pdf', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    