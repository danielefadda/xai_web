#MM2023.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 7.
        small [MM2023]
    .col-lg-8.bg-yellow.p-3
        #accordion-six.accordion
        | #[strong Investigating Debiasing Effects on Classification and Explainability]
        br
        | #[em Marchiori Manerba Marta, Morini Virginia] (2023) - Machine Learning and Principles and Practice of Knowledge Discovery in Databases. ECML PKDD 2022. Communications in Computer and Information Science, vol 1752. In Machine Learning and Principles and Practice of Knowledge Discovery in Databases
        #collapse-six.collapse(aria-labelledby='heading-six' data-parent='#accordion-six')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Biases can arise and be introduced during each phase of a supervised learning pipeline, eventually leading to harm. Within the task of automatic abusive language detection, this matter becomes particularly severe since unintended bias towards sensitive topics such as gender, sexual orientation, or ethnicity can harm underrepresented groups. The role of the datasets used to train these models is crucial to address these challenges. In this contribution, we investigate whether explainability methods can expose racial dialect bias attested within a popular dataset for abusive language detection. Through preliminary experiments, we found that pure explainability techniques cannot effectively uncover biases within the dataset under analysis: the rooted stereotypes are often more implicit and complex to retrieve.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-six' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://doi.org/10.1007/978-3-031-23618-1_32', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 5]
    #MG2022.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 22.
        small [MG2022]
    .col-lg-8.bg-yellow.p-3
        #accordion-twenty-one.accordion
        | #[strong Investigating Debiasing Effects on Classification and Explainability]
        br
        | #[em Marta Marchiori Manerba, Guidotti Riccardo] (2022) - Conference on AI, Ethics, and Society (AIES 2022). In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES'22)
        #collapse-twenty-one.collapse(aria-labelledby='heading-twenty-one' data-parent='#accordion-twenty-one')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small During each stage of a dataset creation and development process, harmful biases can be accidentally introduced, leading to models that perpetuates marginalization and discrimination of minorities, as the role of the data used during the training is critical. We propose an evaluation framework that investigates the impact on classification and explainability of bias mitigation preprocessing techniques used to assess data imbalances concerning minorities' representativeness and mitigate the skewed distributions discovered. Our evaluation focuses on assessing fairness, explainability and performance metrics. We analyze the behavior of local model-agnostic explainers on the original and mitigated datasets to examine whether the proxy models learned by the explainability techniques to mimic the black-boxes disproportionately rely on sensitive attributes, demonstrating biases rooted in the explainers. We conduct several experiments about known biased datasets to demonstrate our proposal’s novelty and effectiveness for evaluation and bias detection purposes.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-twenty-one' aria-expanded='true' aria-controls='collapseAbs') More information
        
        p.my-1
                    small Research Line #[strong 1▪5]
    #MG2021.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 31.
        small [MG2021]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirty.accordion
        | #[strong FairShades: Fairness Auditing via Explainability in Abusive Language Detection Systems]
        br
        | #[em Marchiori Manerba Marta, Guidotti Riccardo] (2021) - Third Conference on Cognitive Machine Intelligence (COGMI) 2021. In 2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI)
        #collapse-thirty.collapse(aria-labelledby='heading-thirty' data-parent='#accordion-thirty')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small At every stage of a supervised learning process, harmful biases can arise and be inadvertently introduced, ultimately leading to marginalization, discrimination, and abuse towards minorities. This phenomenon becomes particularly impactful in the sensitive real-world context of abusive language detection systems, where non-discrimination is difficult to assess. In addition, given the opaqueness of their internal behavior, the dynamics leading a model to a certain decision are often not clear nor accountable, and significant problems of trust could emerge. A robust value-oriented evaluation of models' fairness is therefore necessary. In this paper, we present FairShades, a model-agnostic approach for auditing the outcomes of abusive language detection systems.  Combining explainability and fairness evaluation, FairShades can identify unintended biases and sensitive categories towards which models are most discriminative. This objective is pursued through the auditing of meaningful counterfactuals generated within CheckList framework. We conduct several experiments on BERT-based models to demonstrate our proposal's novelty and effectiveness for unmasking biases. 
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirty' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://doi.org/10.1109/CogMI52975.2021.00014', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪5]
    