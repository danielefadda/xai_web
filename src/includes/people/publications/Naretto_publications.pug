#BGG2023.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 5.
        small [BGG2023]
    .col-lg-8.bg-yellow.p-3
        #accordion-four.accordion
        | #[strong Benchmarking and survey of explanation methods for black box models]
        br
        | #[em Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, Salvatore Rinzivillo] (2023) - Springer Science+Business Media, LLC, part of Springer Nature. In Data Mining and Knowledge Discovery
        #collapse-four.collapse(aria-labelledby='heading-four' data-parent='#accordion-four')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-four' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://doi.org/10.1007/s10618-023-00933-9', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3]
    #NMG2022.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 14.
        small [NMG2022]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirteen.accordion
        | #[strong Evaluating the Privacy Exposure of Interpretable Global Explainers]
        br
        | #[em Naretto Francesca, Monreale Anna, Giannotti Fosca] (2022) - 2022 IEEE 4th International Conference on Cognitive Machine Intelligence (CogMI). In IEEE International Conference on Cognitive Machine Intelligence (CogMI)
        #collapse-thirteen.collapse(aria-labelledby='heading-thirteen' data-parent='#accordion-thirteen')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small In recent years we are witnessing the diffusion of AI systems based on powerful Machine Learning models which find application in many critical contexts such as medicine, financial market and credit scoring. In such a context it is particularly important to design Trustworthy AI systems while guaranteeing transparency, with respect to their decision reasoning and privacy protection. Although many works in the literature addressed the lack of transparency and the risk of privacy exposure of Machine Learning models, the privacy risks of explainers have not been appropriately studied. This paper presents a methodology for evaluating the privacy exposure raised by interpretable global explainers able to imitate the original black-box classifier. Our methodology exploits the well-known Membership Inference Attack. The experimental results highlight that global explainers based on interpretable trees lead to an increase in privacy exposure.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirteen' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://doi.org/10.1109%2Fcogmi56440.2022.00012', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 5]
    #NMG2022.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 17.
        small [NMG2022]
    .col-lg-8.bg-yellow.p-3
        #accordion-sixteen.accordion
        | #[strong Privacy Risk of Global Explainers]
        br
        | #[em Francesca Naretto, Anna Monreale, Fosca Giannotti] (2022) - Proceedings of the First International Conference on Hybrid Human-Artificial Intelligence. In Frontiers in Artificial Intelligence and Applications
        #collapse-sixteen.collapse(aria-labelledby='heading-sixteen' data-parent='#accordion-sixteen')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small nan
    .col-lg-2.pl-3
        
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://doi.org/10.3233/FAIA220206', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 5]
    #NPN2020.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 43.
        small [NPN2020]
    .col-lg-8.bg-yellow.p-3
        #accordion-forty-two.accordion
        | #[strong Prediction and Explanation of Privacy Risk on Mobility Data with Neural Networks]
        br
        | #[em Naretto Francesca, Pellungrini Roberto, Nardini Franco Maria, Giannotti Fosca] (2021) - ECML PKDD 2020 Workshops. In ECML PKDD 2020 Workshops
        #collapse-forty-two.collapse(aria-labelledby='heading-forty-two' data-parent='#accordion-forty-two')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The analysis of privacy risk for mobility data is a fundamental part of any privacy-aware process based on such data. Mobility data are highly sensitive. Therefore, the correct identification of the privacy risk before releasing the data to the public is of utmost importance. However, existing privacy risk assessment frameworks have high computational complexity. To tackle these issues, some recent work proposed a solution based on classification approaches to predict privacy risk using mobility features extracted from the data. In this paper, we propose an improvement of this approach by applying long short-term memory (LSTM) neural networks to predict the privacy risk directly from original mobility data. We empirically evaluate privacy risk on real data by applying our LSTM-based approach. Results show that our proposed method based on a LSTM network is effective in predicting the privacy risk with results in terms of F1 of up to 0.91. Moreover, to explain the predictions of our model, we employ a state-of-the-art explanation algorithm, Shap. We explore the resulting explanation, showing how it is possible to provide effective predictions while explaining them to the end-user.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-forty-two' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1007/978-3-030-65965-3_34', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 4▪5]
    #NPM2020.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 44.
        small [NPM2020]
    .col-lg-8.bg-yellow.p-3
        #accordion-forty-three.accordion
        | #[strong Predicting and Explaining Privacy Risk Exposure in Mobility Data]
        br
        | #[em Naretto Francesca, Pellungrini Roberto, Monreale Anna, Nardini Franco Maria, Musolesi Mirco] (2021) - Discovery Science. In Discovery Science Conference
        #collapse-forty-three.collapse(aria-labelledby='heading-forty-three' data-parent='#accordion-forty-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Mobility data is a proxy of different social dynamics and its analysis enables a wide range of user services. Unfortunately, mobility data are very sensitive because the sharing of people’s whereabouts may arise serious privacy concerns. Existing frameworks for privacy risk assessment provide tools to identify and measure privacy risks, but they often (i) have high computational complexity; and (ii) are not able to provide users with a justification of the reported risks. In this paper, we propose expert, a new framework for the prediction and explanation of privacy risk on mobility data. We empirically evaluate privacy risk on real data, simulating a privacy attack with a state-of-the-art privacy risk assessment framework. We then extract individual mobility profiles from the data for predicting their risk. We compare the performance of several machine learning algorithms in order to identify the best approach for our task. Finally, we show how it is possible to explain privacy risk prediction on real data, using two algorithms: Shap, a feature importance-based method and Lore, a rule-based method. Overall, expert is able to provide a user with the privacy risk and an explanation of the risk itself. The experiments show excellent performance for the prediction task.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-forty-three' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1007/978-3-030-61527-7_27', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 4▪5]
    