#GMR2018.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 1.
        small [GMR2018]
    .col-lg-8.bg-yellow.p-3
        #accordion-zero.accordion
        | #[strong A Survey of Methods for Explaining Black Box Models]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco, Giannotti Fosca, Pedreschi Dino] (2022) - ACM Computing Surveys. In ACM computing surveys (CSUR), 51(5), 1-42.
        #collapse-zero.collapse(aria-labelledby='heading-zero' data-parent='#accordion-zero')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-zero' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1145/3236009', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3]
    #GMG2019.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 2.
        small [GMG2019]
    .col-lg-2.pl-0
        img(src='assets/images/publications/03_factual_and_counterfactual.jpg ' alt="immagine" style='width:100%;' data-toggle='modal' data-target='#modal-one' type='button').mr-3.border.border-secondary.bwc-image
        #modal-one.modal.fade(tabindex='-1' role='dialog' aria-labelledby='#modal-one-Title' aria-hidden='true')
            .modal-dialog.modal-lg.modal-dialog-centered(role='document')
                .modal-content
                    .modal-header
                        p.small Factual and Counterfactual Explanations for Black Box Decision Making
                        button.close(type='button' data-dismiss='modal' aria-label='Close')
                            span(aria-hidden='true') &times;
                    .modal-body
                        img(src='assets/images/publications/03_factual_and_counterfactual.jpg ' alt="immagine" )
    .col-lg-6.bg-yellow.p-3
        #accordion-one.accordion
        | #[strong Factual and Counterfactual Explanations for Black Box Decision Making]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Giannotti Fosca, Pedreschi Dino, Ruggieri Salvatore, Turini Franco] (2021) - IEEE Intelligent Systems. In IEEE Intelligent Systems
        #collapse-one.collapse(aria-labelledby='heading-one' data-parent='#accordion-one')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-one' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1109/mis.2019.2957223', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 4]
    #GMR2018a.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 4.
        small [GMR2018a]
    .col-lg-8.bg-yellow.p-3
        #accordion-three.accordion
        | #[strong Local Rule-Based Explanations of Black Box Decision Systems]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore , Pedreschi Dino, Turini Franco , Giannotti Fosca] (2018) - Arxive preprint
        #collapse-three.collapse(aria-labelledby='heading-three' data-parent='#accordion-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of achine learning components in socially sensitive and safety-critical contexts. Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-three' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://arxiv.org/abs/1805.10820', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #GR2021.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 21.
        small [GR2021]
    .col-lg-8.bg-yellow.p-3
        #accordion-twenty.accordion
        | #[strong Ensemble of Counterfactual Explainers]
        br
        | #[em Guidotti Riccardo, Ruggieri Salvatore] (2021)
        #collapse-twenty.collapse(aria-labelledby='heading-twenty' data-parent='#accordion-twenty')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small In eXplainable Artificial Intelligence (XAI), several counterfactual explainers have been proposed, each focusing on some desirable properties of counterfactual instances: minimality, actionability, stability, diversity, plausibility, discriminative power. We propose an ensemble of counterfactual explainers that boosts weak explainers, which provide only a subset of such properties, to a powerful method covering all of them. The ensemble runs weak explainers on a sample of instances and of features, and it combines their results by exploiting a diversity-driven selection function. The method is model-agnostic and, through a wrapping approach based on autoencoders, it is also data-agnostic
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-twenty' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://pages.di.unipi.it/ruggieri/Papers/ds2021.pdf', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #PGG2019.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 32.
        small [PGG2019]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirty-one.accordion
        | #[strong Meaningful Explanations of Black Box AI Decision Systems]
        br
        | #[em Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco] (2021) - Proceedings of the AAAI Conference on Artificial Intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence
        #collapse-thirty-one.collapse(aria-labelledby='heading-thirty-one' data-parent='#accordion-thirty-one')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Black box AI systems for automated decision making, often based on machine learning over (big) data, map a user’s features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases inherited by the algorithms from human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We focus on the urgent open challenge of how to construct meaningful explanations of opaque AI/ML systems, introducing the local-toglobal framework for black box explanation, articulated along three lines: (i) the language for expressing explanations in terms of logic rules, with statistical and causal interpretation; (ii) the inference of local explanations for revealing the decision rationale for a specific case, by auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of many local explanations into simple global ones, with algorithms that optimize for quality and comprehensibility. We argue that the local-first approach opens the door to a wide variety of alternative solutions along different dimensions: a variety of data sources (relational, text, images, etc.), a variety of learning problems (multi-label classification, regression, scoring, ranking), a variety of languages for expressing meaningful explanations, a variety of means to audit a black box.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirty-one' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1609/aaai.v33i01.33019780', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #LGR2020.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 34.
        small [LGR2020]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirty-three.accordion
        | #[strong Explaining Sentiment Classification with Synthetic Exemplars and Counter-Exemplars]
        br
        | #[em Lampridis Orestis, Guidotti Riccardo, Ruggieri Salvatore] (2021) - Discovery Science. In In International Conference on Discovery Science (pp. 357-373). Springer, Cham.
        #collapse-thirty-three.collapse(aria-labelledby='heading-thirty-three' data-parent='#accordion-thirty-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small We present xspells, a model-agnostic local approach for explaining the decisions of a black box model for sentiment classification of short texts. The explanations provided consist of a set of exemplar sentences and a set of counter-exemplar sentences. The former are examples classified by the black box with the same label as the text to explain. The latter are examples classified with a different label (a form of counter-factuals). Both are close in meaning to the text to explain, and both are meaningful sentences – albeit they are synthetically generated. xspells generates neighbors of the text to explain in a latent space using Variational Autoencoders for encoding text and decoding latent instances. A decision tree is learned from randomly generated neighbors, and used to drive the selection of the exemplars and counter-exemplars. We report experiments on two datasets showing that xspells outperforms the well-known lime method in terms of quality of explanations, fidelity, and usefulness, and that is comparable to it in terms of stability.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirty-three' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1007/978-3-030-61527-7_24', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #RGG2020.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 39.
        small [RGG2020]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirty-eight.accordion
        | #[strong Opening the black box: a primer for anti-discrimination]
        br
        | #[em Ruggieri Salvatore, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Pedreschi Dino, Turini Franco] (2020). In ANNUARIO DI DIRITTO COMPARATO E DI STUDI LEGISLATIVI
        #collapse-thirty-eight.collapse(aria-labelledby='heading-thirty-eight' data-parent='#accordion-thirty-eight')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The pervasive adoption of Artificial Intelligence (AI) models in the modern information society, requires counterbalancing the growing decision power demanded to AI models with risk assessment methodologies. In this paper, we consider the risk of discriminatory decisions and review approaches for discovering discrimination and for designing fair AI models. We highlight the tight relations between discrimination discovery and explainable AI, with the latter being a more general approach for understanding the behavior of black boxes.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirty-eight' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://hdl.handle.net/11568/1088440', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #PGG2018.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 45.
        small [PGG2018]
    .col-lg-8.bg-yellow.p-3
        #accordion-forty-four.accordion
        | #[strong Open the Black Box Data-Driven Explanation of Black Box Decision Systems]
        br
        | #[em Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna , Pappalardo Luca , Ruggieri Salvatore , Turini Franco ] (2018) - Arxive preprint
        #collapse-forty-four.collapse(aria-labelledby='heading-forty-four' data-parent='#accordion-forty-four')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-forty-four' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://arxiv.org/abs/1806.09936', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    