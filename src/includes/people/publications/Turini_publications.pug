#GMR2018.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 1.
        small [GMR2018]
    .col-lg-8.bg-yellow.p-3
        #accordion-zero.accordion
        | #[strong A Survey of Methods for Explaining Black Box Models]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco, Giannotti Fosca, Pedreschi Dino] (2022) - ACM Computing Surveys. In ACM computing surveys (CSUR), 51(5), 1-42.
        #collapse-zero.collapse(aria-labelledby='heading-zero' data-parent='#accordion-zero')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-zero' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1145/3236009', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪3]
    #GMG2019.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 2.
        small [GMG2019]
    .col-lg-2.pl-0
        img(src='assets/images/publications/03_factual_and_counterfactual.jpg ' alt="immagine" style='width:100%;' data-toggle='modal' data-target='#modal-one' type='button').mr-3.border.border-secondary.bwc-image
        #modal-one.modal.fade(tabindex='-1' role='dialog' aria-labelledby='#modal-one-Title' aria-hidden='true')
            .modal-dialog.modal-lg.modal-dialog-centered(role='document')
                .modal-content
                    .modal-header
                        p.small Factual and Counterfactual Explanations for Black Box Decision Making
                        button.close(type='button' data-dismiss='modal' aria-label='Close')
                            span(aria-hidden='true') &times;
                    .modal-body
                        img(src='assets/images/publications/03_factual_and_counterfactual.jpg ' alt="immagine" )
    .col-lg-6.bg-yellow.p-3
        #accordion-one.accordion
        | #[strong Factual and Counterfactual Explanations for Black Box Decision Making]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Giannotti Fosca, Pedreschi Dino, Ruggieri Salvatore, Turini Franco] (2021) - IEEE Intelligent Systems. In IEEE Intelligent Systems
        #collapse-one.collapse(aria-labelledby='heading-one' data-parent='#accordion-one')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-one' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1109/mis.2019.2957223', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪4]
    #SGM2021.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 3.
        small [SGM2021]
    .col-lg-2.pl-0
        img(src='assets/images/publications/02_glocalX.jpg ' alt="immagine" style='width:100%;' data-toggle='modal' data-target='#modal-two' type='button').mr-3.border.border-secondary.bwc-image
        #modal-two.modal.fade(tabindex='-1' role='dialog' aria-labelledby='#modal-two-Title' aria-hidden='true')
            .modal-dialog.modal-lg.modal-dialog-centered(role='document')
                .modal-content
                    .modal-header
                        p.small GLocalX - From Local to Global Explanations of Black Box AI Models
                        button.close(type='button' data-dismiss='modal' aria-label='Close')
                            span(aria-hidden='true') &times;
                    .modal-body
                        img(src='assets/images/publications/02_glocalX.jpg ' alt="immagine" )
    .col-lg-6.bg-yellow.p-3
        #accordion-two.accordion
        | #[strong GLocalX - From Local to Global Explanations of Black Box AI Models]
        br
        | #[em Setzu Mattia, Guidotti Riccardo, Monreale Anna, Turini Franco, Pedreschi Dino, Giannotti Fosca] (2021) - Artificial Intelligence. In Artificial Intelligence
        #collapse-two.collapse(aria-labelledby='heading-two' data-parent='#accordion-two')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are “black boxes” which we are not able to understand. Relying on these models has a multifaceted impact and raises significant concerns about their transparency. Applications in sensitive and critical domains are a strong motivational factor in trying to understand the behavior of black boxes. We propose to address this issue by providing an interpretable layer on top of black box models by aggregating “local” explanations. We present GLocalX, a “local-first” model agnostic explanation method. Starting from local explanations expressed in form of local decision rules, GLocalX iteratively generalizes them into global explanations by hierarchically aggregating them. Our goal is to learn accurate yet simple interpretable models to emulate the given black box, and, if possible, replace it entirely. We validate GLocalX in a set of experiments in standard and constrained settings with limited or no access to either data or local explanations. Experiments show that GLocalX is able to accurately emulate several models with simple and small models, reaching state-of-the-art performance against natively global solutions. Our findings show how it is often possible to achieve a high level of both accuracy and comprehensibility of classification models, even in complex domains with high-dimensional data, without necessarily trading one property for the other. This is a key requirement for a trustworthy AI, necessary for adoption in high-stakes decision making applications.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-two' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1016/j.artint.2021.103457', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1▪4]
    #GMR2018a.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 4.
        small [GMR2018a]
    .col-lg-8.bg-yellow.p-3
        #accordion-three.accordion
        | #[strong Local Rule-Based Explanations of Black Box Decision Systems]
        br
        | #[em Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore , Pedreschi Dino, Turini Franco , Giannotti Fosca] (2018) - Arxive preprint
        #collapse-three.collapse(aria-labelledby='heading-three' data-parent='#accordion-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of achine learning components in socially sensitive and safety-critical contexts. Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-three' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://arxiv.org/abs/1805.10820', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #SGM2019.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 34.
        small [SGM2019]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirty-three.accordion
        | #[strong Global Explanations with Local Scoring]
        br
        | #[em Setzu Mattia, Guidotti Riccardo, Monreale Anna, Turini Franco] (2021) - Machine Learning and Knowledge Discovery in Databases. In ECML PKDD 2019: Machine Learning and Knowledge Discovery in Databases
        #collapse-thirty-three.collapse(aria-labelledby='heading-thirty-three' data-parent='#accordion-thirty-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Artificial Intelligence systems often adopt machine learning models encoding complex algorithms with potentially unknown behavior. As the application of these “black box” models grows, it is our responsibility to understand their inner working and formulate them in human-understandable explanations. To this end, we propose a rule-based model-agnostic explanation method that follows a local-to-global schema: it generalizes a global explanation summarizing the decision logic of a black box starting from the local explanations of single predicted instances. We define a scoring system based on a rule relevance score to extract global explanations from a set of local explanations in the form of decision rules. Experiments on several datasets and black boxes show the stability, and low complexity of the global explanations provided by the proposed solution in comparison with baselines and state-of-the-art global explainers.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirty-three' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1007/978-3-030-43823-4_14', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #PGG2019.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 37.
        small [PGG2019]
    .col-lg-8.bg-yellow.p-3
        #accordion-thirty-six.accordion
        | #[strong Meaningful Explanations of Black Box AI Decision Systems]
        br
        | #[em Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Ruggieri Salvatore, Turini Franco] (2021) - Proceedings of the AAAI Conference on Artificial Intelligence. In Proceedings of the AAAI Conference on Artificial Intelligence
        #collapse-thirty-six.collapse(aria-labelledby='heading-thirty-six' data-parent='#accordion-thirty-six')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Black box AI systems for automated decision making, often based on machine learning over (big) data, map a user’s features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases inherited by the algorithms from human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We focus on the urgent open challenge of how to construct meaningful explanations of opaque AI/ML systems, introducing the local-toglobal framework for black box explanation, articulated along three lines: (i) the language for expressing explanations in terms of logic rules, with statistical and causal interpretation; (ii) the inference of local explanations for revealing the decision rationale for a specific case, by auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of many local explanations into simple global ones, with algorithms that optimize for quality and comprehensibility. We argue that the local-first approach opens the door to a wide variety of alternative solutions along different dimensions: a variety of data sources (relational, text, images, etc.), a variety of learning problems (multi-label classification, regression, scoring, ranking), a variety of languages for expressing meaningful explanations, a variety of means to audit a black box.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-thirty-six' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://dx.doi.org/10.1609/aaai.v33i01.33019780', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #RGG2020.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 44.
        small [RGG2020]
    .col-lg-8.bg-yellow.p-3
        #accordion-forty-three.accordion
        | #[strong Opening the black box: a primer for anti-discrimination]
        br
        | #[em Ruggieri Salvatore, Giannotti Fosca, Guidotti Riccardo, Monreale Anna, Pedreschi Dino, Turini Franco] (2020). In ANNUARIO DI DIRITTO COMPARATO E DI STUDI LEGISLATIVI
        #collapse-forty-three.collapse(aria-labelledby='heading-forty-three' data-parent='#accordion-forty-three')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small The pervasive adoption of Artificial Intelligence (AI) models in the modern information society, requires counterbalancing the growing decision power demanded to AI models with risk assessment methodologies. In this paper, we consider the risk of discriminatory decisions and review approaches for discovering discrimination and for designing fair AI models. We highlight the tight relations between discrimination discovery and explainable AI, with the latter being a more general approach for understanding the behavior of black boxes.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-forty-three' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='http://hdl.handle.net/11568/1088440', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    #PGG2018.row.mt-5.justify-content-center
    .col-lg-1.text-right
        h4 50.
        small [PGG2018]
    .col-lg-8.bg-yellow.p-3
        #accordion-forty-nine.accordion
        | #[strong Open the Black Box Data-Driven Explanation of Black Box Decision Systems]
        br
        | #[em Pedreschi Dino, Giannotti Fosca, Guidotti Riccardo, Monreale Anna , Pappalardo Luca , Ruggieri Salvatore , Turini Franco ] (2018) - Arxive preprint
        #collapse-forty-nine.collapse(aria-labelledby='heading-forty-nine' data-parent='#accordion-forty-nine')
            div.bg-yellow
                hr
                p.small #[strong Abstract]
                p.small Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations.
    .col-lg-2.pl-3
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='#' data-toggle='collapse' data-target='#collapse-forty-nine' aria-expanded='true' aria-controls='collapseAbs') More information
        p.my-1
                a.btn-mini.px-2.btn-secondary.small(href='https://arxiv.org/abs/1806.09936', target="_blank") External Link
        p.my-1
                    small Research Line #[strong 1]
    